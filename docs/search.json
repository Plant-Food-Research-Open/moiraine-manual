[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The moiraine R package user manual",
    "section": "",
    "text": "Preface\nQuick blurb around multi-omics integration. There are many tools available to perform multi-omics integration, and a lot are implemented as R packages. These tools differ conceptually (in terms of required data input, assumptions, questions they answer) but also at a practical level in terms of input data format, parameters, etc and output format. That makes it time-consuming to apply different tools to a same multi-omics dataset, and to compare the results."
  },
  {
    "objectID": "index.html#the-moiraine-package",
    "href": "index.html#the-moiraine-package",
    "title": "The moiraine R package user manual",
    "section": "The moiraine package",
    "text": "The moiraine package\nThe moiraine package aims at alleviating this by providing a framework to easily and consistently apply different integration tools to a same dataset. It also facilitates the comparison of results with consistent formatting of integration output and visualisations.\nIn addition, in an effort to make these computations reproducible, moiraine heavily relies on targets for the creating of reproducible pipelines."
  },
  {
    "objectID": "index.html#about-this-manual",
    "href": "index.html#about-this-manual",
    "title": "The moiraine R package user manual",
    "section": "About this manual",
    "text": "About this manual\nIn this manual, we are showcasing the functionalities of the moiraine package by presenting an in-depth walk-through example of multi-omics integration analysis. This will not only talk about the how in terms of R functions etc, but also talk about the integration methods and how to use them.\nHere, say that we heavily recommend to be familiar with targets (teaching targets it out of the scope of this manual, and we refer to the excellent targets manual).\n\nlibrary(targets)\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n#> [1] 2\n\nA targets chunck:\n\ntar_target(a, 2+2)\n\n\n\n\"something\""
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "omics dataset + what are samples and features metadata\nwhat is multi-omics integration\nsupervised vs unsupervised\nthe analysis workflow from moiraine (use the graph I use in presentations)\n\n\n1.0.1 Other things to add in intro\nNote: Throughout the package and vignettes, we refer to the different biological entities measured in a given dataset (e.g. genes, transcripts, metabolic compounds, etc) as features.\nTo improve upon the default ggplot2 colours, each vignette is run with the following options:\n\noptions(\n  ggplot2.continuous.colour = \"viridis\",\n  ggplot2.continuous.fill = \"viridis\",\n  ggplot2.discrete.colour = function() {\n    ggplot2::scale_colour_brewer(\n      palette = \"Paired\", \n      na.value = \"grey\"\n    )\n  } ,\n  ggplot2.discrete.fill = function() {\n    ggplot2::scale_fill_brewer(\n      palette = \"Paired\",\n      na.value = \"grey\"\n    )\n  } \n)"
  },
  {
    "objectID": "example_dataset.html",
    "href": "example_dataset.html",
    "title": "2  The example dataset",
    "section": "",
    "text": "The dataset that is used as example in this manual comes from Li et al. (2022). In this paper, the authors investigate the molecular mechanisms of bovine respiratory disease (BRD) in beef cattle, using multi-omics data. They collected genomics, transcriptomics and metabolomics measurements on blood samples obtained from both healthy and infected animals. They performed a genome-wide association study (GWAS) to identify genomic variants associated with BRD incidence, as well as a differential expression (DE) analysis on both the transcriptomics and metabolomics datasets to identify genes and metabolites whose expression or abundance differed between the two animal groups. They also performed an expression quantitative trait loci (eQTL) analysis to highlight associations between genomic variants and differentially expressed genes. The datasets analysed in this article are publicly available. In this chapter, we detail the content of each dataset, and how they were obtained and processed to use for this manual.\n\n\n\n\nLi, Jiyuan, Robert Mukiibi, Janelle Jiminez, Zhiquan Wang, Everestus C. Akanno, Edouard Timsit, and Graham S. Plastow. 2022. “Applying Multi-Omics Data to Study the Genetic Background of Bovine Respiratory Disease Infection in Feedlot Crossbred Cattle.” Frontiers in Genetics 13. https://www.frontiersin.org/articles/10.3389/fgene.2022.1046192."
  },
  {
    "objectID": "data_import.html#the-example-dataset-files",
    "href": "data_import.html#the-example-dataset-files",
    "title": "3  Importing data",
    "section": "\n3.1 The example dataset files",
    "text": "3.1 The example dataset files\nThe dataset analysed this manual is presented in Chapter 2. The associated files that we will use here are:\n\n\nGenomics data:\n\ngenomics_dataset.csv: contains the genomic variants’ dosage, with genomic variants as rows and samples as columns.\ngenomics_features_info.csv: contains information about the genomic variants (chromosome, genomic position, etc, as well as the results of a GWAS analysis).\n\n\n\nTranscriptomics data:\n\ntranscriptomics_dataset.csv: contains the raw read counts for the measured genes – rows correspond to transcripts, and columns to samples.\nbos_taurus_gene_model.gff3: the genome annotation file used to map the transcriptomics reads to gene models.\ntranscriptomics_de_results.csv: the results of a differential expression analysis run on the transcriptomics dataset to compare healthy and diseased animals.\ntranscriptomics_go_annotation.csv: contains the correspondence between genes and GO terms in a long format (one row per gene/GO term pair).\n\n\n\nMetabolomics data:\n\nmetabolomics_dataset.csv: contains the area peak values – rows correspond to samples, and columns to compounds.\nmetabolomics_features_info.csv: contains information about the compounds (such as mass, retention time, and formula and name if the compounds has been identified) as well as the results of a differential expression analysis run on the metabolomics dataset to compare healthy and diseased animals.\n\n\nSamples information: stored in the samples_info.csv file, in which each row corresponds to a sample.\n\nEach of these files is available through the moiraine package, and can be retrieved via system.file(\"extdata/genomics_dataset.csv\", package = \"moiraine\")."
  },
  {
    "objectID": "data_import.html#importing-the-datasets",
    "href": "data_import.html#importing-the-datasets",
    "title": "3  Importing data",
    "section": "\n3.2 Importing the datasets",
    "text": "3.2 Importing the datasets\nWe will show how to import the datasets, first manually, and then in an automated way (using a target factory function).\n\n3.2.1 Manually\nWe can start by creating targets that track the different data files. This ensures that when a data file changes, the target is considered outdated and any analysis relying on this data file will be re-run (see here for more information). For example, for the genomics dataset, we write:\n\ntar_target(\n  dataset_file_geno,\n  system.file(\"extdata/genomics_dataset.csv\", package = \"moiraine\"),\n  format = \"file\"\n)\n\nThe created target, called dataset_file_geno, takes as value the path to the file:\n\ntar_read(dataset_file_geno)\n#> [1] \"/powerplant/workspace/hrpoab/RENV_CACHE/v5/R-4.2/x86_64-pc-linux-gnu/moiraine/0.0.0.9000/c5fcba1231fe0ad084ba7ffc17cfc3e0/moiraine/extdata/genomics_dataset.csv\"\n\nThe next step is to import this dataset in R. We use the import_dataset_csv() function for that, rather than the readr::read_csv() or similar functions, as it ensures that the data is imported with the correct format for further use with the moiraine package. When importing a dataset, we need to specify the path to the file, as well as the name of the column in the csv file that contains the row names (through the col_id argument). In addition, we need to specify whether the features are represented in rows in the csv file, or in columns. This is done through the argument features_as_rows. For example, we can load the genomics dataset through:\n\ntar_target(\n  data_geno,\n  import_dataset_csv(\n    dataset_file_geno, \n    col_id = \"marker\", \n    features_as_rows = TRUE)\n)\n\nThe function returns a matrix in which the rows correspond to the features measured, and the columns correspond to the samples:\n\ntar_read(data_geno) |> dim()\n#> [1] 23036   139\ntar_read(data_geno)[1:5, 1:3]\n#>                             R21 Y3660 Y3243\n#> 1_41768691                    1     0     2\n#> 10-27008241-A-C-rs42918694    2     2     2\n#> 10-37505419-T-C-rs136559242   0     1     0\n#> 10-49904259-G-A-rs471723345   1     2     2\n#> 1-109550832-G-A-rs209732846   2     2     1\n\nNote that import_dataset_csv() uses readr::read_csv() to read in the data. It accepts arguments that will be passed on to read_csv(), which can be useful to control how the data file must be read, e.g. by specifying the columns’ type, or which characters must be considered as missing values.\n\n3.2.2 Using a target factory function\nCreating a target to track the raw file and using the import_dataset_csv() function to read it can be a bit cumbersome if we want to import several datasets. Luckily, this process can be automated with the import_dataset_csv_factory() function. It takes as an input a vector of files path, and for each file creates:\n\na target named dataset_file_XX (XX explained below), which tracks the raw data file;\na target named data_XX, which corresponds to the data matrix that has been imported through the import_dataset_csv function.\n\nFor each file, we need to specify the name of the column giving the row names (argument col_ids), and whether the features are stored as rows or as columns (argument features_as_rowss). Note that these arguments are the same as in the primary function import_dataset_csv(), except that they have an additional ‘s’ at the end of their name. This will be the case for most of the target factory functions from the package.\nIn addition, we have to provide a unique suffix which will be appended to the name of the targets created (i.e. the XX mentioned above) through the target_name_suffixes argument. This allows us to track which target corresponds to which dataset.\nSo the following code (note that it is not within a tar_target() call):\n\nimport_dataset_csv_factory(\n  files = c(\n    system.file(\"extdata/genomics_dataset.csv\", package = \"moiraine\"),\n    system.file(\"extdata/transcriptomics_dataset.csv\", package = \"moiraine\"),\n    system.file(\"extdata/metabolomics_dataset.csv\", package = \"moiraine\")\n  ),\n  col_ids = c(\"marker\", \"gene_id\", \"sample_id\"),\n  features_as_rowss = c(TRUE, TRUE, FALSE),\n  target_name_suffixes = c(\"geno\", \"transcripto\", \"metabo\")\n)\n\nwill create the following targets:\n\ndataset_file_geno, dataset_file_transcripto, dataset_file_metabo\ndata_geno, data_metabo, data_transcripto\n\n\ntar_read(data_geno) |> dim()\n#> [1] 23036   139\ntar_read(data_transcripto) |> dim()\n#> [1] 20335   143\ntar_read(data_metabo) |> dim()\n#> [1]  55 139\n\nWith this factory function, it is not possible to pass arguments to read_csv(). If you want to control how the files are read, please use the import_dataset_csv() function directly instead, as shown in Section 3.2.1."
  },
  {
    "objectID": "data_import.html#importing-the-features-metadata",
    "href": "data_import.html#importing-the-features-metadata",
    "title": "3  Importing data",
    "section": "\n3.3 Importing the features metadata",
    "text": "3.3 Importing the features metadata\nSimilarly to how we imported the datasets, there are two ways of importing features metadata: either manually, or using a target factory function. The two options are illustrated below.\n\n3.3.1 Manually\nAs shown in the previous section, we can start by creating a target that tracks the raw features metadata file, then read the file into R using the import_fmetadata_csv() function. It has the similar arguments as the import_dataset_csv() function, but returns a data-frame (rather than a matrix); and does not have the options to read a csv where the features are columns (they must be in rows):\n\nlist(\n  tar_target(\n    fmetadata_file_geno,\n    system.file(\"extdata/genomics_features_info.csv\", package = \"moiraine\"),\n    format = \"file\"\n  ),\n  \n  tar_target(\n    fmetadata_geno,\n    import_fmetadata_csv(\n      fmetadata_file_geno,\n      col_id = \"marker\",\n      col_types = c(\"chromosome\" = \"c\")\n    )\n  )\n)\n\nNotice that in the import_fmetadata_csv() call, we’ve added an argument (col_types) which will be passed on to read_csv(). This is to ensure that the chromosome column will be read as character (even though the chromosomes are denoted with integers).\n\ntar_read(fmetadata_geno) |> head()\n#>                                                feature_id chromosome  position\n#> 1_41768691                                     1_41768691          1  42139849\n#> 10-27008241-A-C-rs42918694     10-27008241-A-C-rs42918694         10  26971270\n#> 10-37505419-T-C-rs136559242   10-37505419-T-C-rs136559242         10  37388728\n#> 10-49904259-G-A-rs471723345   10-49904259-G-A-rs471723345          0         0\n#> 1-109550832-G-A-rs209732846   1-109550832-G-A-rs209732846          1 108696486\n#> 11-104555023-A-G-rs109353933 11-104555023-A-G-rs109353933         11 104498929\n#>                              gen_train_score ref alt ilmn_strand\n#> 1_41768691                            0.6786   T   G         BOT\n#> 10-27008241-A-C-rs42918694            0.8050   A   C         TOP\n#> 10-37505419-T-C-rs136559242           0.7890   A   G         TOP\n#> 10-49904259-G-A-rs471723345           0.7970   A   G         TOP\n#> 1-109550832-G-A-rs209732846           0.8909   T   C         BOT\n#> 11-104555023-A-G-rs109353933          0.8673   T   C         BOT\n#>                              customer_strand norm_id qtl_type qtl_effect\n#> 1_41768691                               BOT       2     <NA>         NA\n#> 10-27008241-A-C-rs42918694               TOP       1     <NA>         NA\n#> 10-37505419-T-C-rs136559242              BOT       1     <NA>         NA\n#> 10-49904259-G-A-rs471723345              TOP       2     <NA>         NA\n#> 1-109550832-G-A-rs209732846              TOP       3     <NA>         NA\n#> 11-104555023-A-G-rs109353933             TOP       1     <NA>         NA\n#>                              p_value fdr\n#> 1_41768691                        NA  NA\n#> 10-27008241-A-C-rs42918694        NA  NA\n#> 10-37505419-T-C-rs136559242       NA  NA\n#> 10-49904259-G-A-rs471723345       NA  NA\n#> 1-109550832-G-A-rs209732846       NA  NA\n#> 11-104555023-A-G-rs109353933      NA  NA\n\nYou can see that in the data-frame of features metadata, the feature IDs are present both as row names and in the feature_id column. This makes it easier to subset the datasets later on.\n\n3.3.2 Using a target factory function\nAlternatively, we can use a target factory function that automates the process when we have to read in several features metadata files. In our case, we have to do it for the genomics and metabolomics datasets only, as the transcriptomics dataset has a different features metadata format. However because we need to specify the column types for the genomics dataset, we will use the targets factory function to read in the metabolomics features metadata only. The arguments are almost the same as for import_dataset_csv_factory() (except for features_as_rowss):\n\nimport_fmetadata_csv_factory(\n  files = c(\n    system.file(\"extdata/metabolomics_features_info.csv\", package = \"moiraine\")\n  ),\n  col_ids = c(\"feature_id\"),\n  target_name_suffixes = c(\"metabo\")\n)\n\nThe targets created are:\n\nfmetadata_file_metabo\nfmetadata_metabo\n\n\ntar_read(fmetadata_metabo) |> head()\n#>           feature_id     hmdb_id                  name chemical_formula\n#> HMDB00001  HMDB00001 HMDB0000001     1-Methylhistidine        C7H11N3O2\n#> HMDB00008  HMDB00008 HMDB0000008 2-Hydroxybutyric acid           C4H8O3\n#> HMDB00357  HMDB00357 HMDB0000011 3-Hydroxybutyric acid           C4H8O3\n#> HMDB00042  HMDB00042 HMDB0000042           Acetic acid           C2H4O2\n#> HMDB00043  HMDB00043 HMDB0000043               Betaine         C5H12NO2\n#> HMDB00060  HMDB00060 HMDB0000060      Acetoacetic acid           C4H6O3\n#>           monisotopic_molecular_weight cas_registry_number\n#> HMDB00001                    169.08513            332-80-9\n#> HMDB00008                    104.04734           3347-90-8\n#> HMDB00357                    104.04734            625-72-9\n#> HMDB00042                     60.02113             64-19-7\n#> HMDB00043                    118.08680           6915-17-9\n#> HMDB00060                    102.03169            541-50-4\n#>                                smiles                    inchikey kegg_id\n#> HMDB00001 CN1C=NC(C[C@H](N)C(O)=O)=C1 BRMWTNUJHUMWMS-LURJTMIESA-N  C01152\n#> HMDB00008            CC[C@H](O)C(O)=O AFENDNXGAFYKQO-VKHMYHEASA-N  C05984\n#> HMDB00357           C[C@@H](O)CC(O)=O WHBMMWSBFZVSSR-GSVOUGTGSA-N  C01089\n#> HMDB00042                     CC(O)=O QTBSBXVTEAMEQO-UHFFFAOYSA-N  C00033\n#> HMDB00043          C[N+](C)(C)CC(O)=O KWIUHFFTVRNATP-UHFFFAOYSA-O    <NA>\n#> HMDB00060               CC(=O)CC(O)=O WDJHALXBUFZDSR-UHFFFAOYSA-N  C00164\n#>                                    direct_parent                   super_class\n#> HMDB00001              Histidine and derivatives Organic acids and derivatives\n#> HMDB00008    Alpha hydroxy acids and derivatives Organic acids and derivatives\n#> HMDB00357     Beta hydroxy acids and derivatives Organic acids and derivatives\n#> HMDB00042                       Carboxylic acids Organic acids and derivatives\n#> HMDB00043                      Alpha amino acids Organic acids and derivatives\n#> HMDB00060 Short-chain keto acids and derivatives Organic acids and derivatives\n#>               t_value      p_value         padj de_signif     de_status\n#> HMDB00001  -0.5557020 5.797635e-01 6.784466e-01    Not DE        Not DE\n#> HMDB00008   0.2181562 8.276321e-01 8.925444e-01    Not DE        Not DE\n#> HMDB00357  -9.7388879 2.353250e-17 2.157146e-16        DE downregulated\n#> HMDB00042 -12.5323491 1.753101e-24 4.821028e-23        DE downregulated\n#> HMDB00043  -7.9073179 7.827088e-13 3.913544e-12        DE downregulated\n#> HMDB00060  -0.4369834 6.628164e-01 7.439776e-01    Not DE        Not DE\n\nAgain, the targets factory function does not allow to pass arguments to read_csv() (if you need them, please use import_fmetadata_csv() directly as we have done in Section 3.3.1).\n\n3.3.3 Importing features metadata from a GTF/GFF file\nThe moiraine package can also extract features metadata from a genome annotation file (.gtf or .gff). We’ll demonstrate that for the transcriptomics dataset, for which information about the position and name of the transcripts can be found in the genome annotation used to map the reads. The function is called import_fmetadata_gff() (it is also the function you would use to read in information from a .gtf file). The type of information to extract from the annotation file is specified through the feature_type argument, which can be either 'genes' or 'transcripts'. In addition, if the function does not extract certain fields from the annotation file, these can be explicitly called using the add_fields parameter.\nIn this example, we want to extract information about the genes from the gtf file. We also want to make sure that the Name and descriptionfield are imported, as they give the name and description of the genes. To read in this information “manually”, we create the following targets:\n\nlist(\n  tar_target(\n    fmetadata_file_transcripto,\n    system.file(\"extdata/bos_taurus_gene_model.gff3\", package = \"moiraine\"),\n    format = \"file\"\n  ),\n  \n  tar_target(\n    fmetadata_transcripto,\n    import_fmetadata_gff(\n      fmetadata_file_transcripto,\n      feature_type = \"genes\",\n      add_fields = c(\"Name\", \"description\")\n    )\n  )\n)\n\nAs for the other import functions, there exists a more succinct target factory version, called import_fmetadata_gff_factory():\n\nimport_fmetadata_gff_factory(\n  files = system.file(\"extdata/bos_taurus_gene_model.gff3\", package = \"moiraine\"),\n  feature_types = \"genes\",\n  add_fieldss = c(\"Name\", \"description\"),\n  target_name_suffixes = \"transcripto\"\n)\n\nThis will create two targets: fmetadata_file_transcripto and fmetadata_transcripto.\nAs with import_fmetadata, the function returns a data-frame of features information:\n\ntar_read(fmetadata_transcripto) |> head()\n#>                            feature_id chromosome    start      end  width\n#> ENSBTAG00000000005 ENSBTAG00000000005         17 65389743 65505336 115594\n#> ENSBTAG00000000008 ENSBTAG00000000008         29 32214439 32244810  30372\n#> ENSBTAG00000000009 ENSBTAG00000000009         18 12338037 12342272   4236\n#> ENSBTAG00000000010 ENSBTAG00000000010         21 34209956 34223394  13439\n#> ENSBTAG00000000011 ENSBTAG00000000011          8  7950815  7971600  20786\n#> ENSBTAG00000000012 ENSBTAG00000000012         20 33708626 33732944  24319\n#>                    strand  Name\n#> ENSBTAG00000000005      +  GRK3\n#> ENSBTAG00000000008      - KCNJ1\n#> ENSBTAG00000000009      + FOXF1\n#> ENSBTAG00000000010      +  UBL7\n#> ENSBTAG00000000011      -   TDH\n#> ENSBTAG00000000012      + TTC33\n#>                                                                                                       description\n#> ENSBTAG00000000005                        G protein-coupled receptor kinase 3 [Source:VGNC Symbol;Acc:VGNC:53904]\n#> ENSBTAG00000000008 potassium inwardly rectifying channel subfamily J member 1 [Source:VGNC Symbol;Acc:VGNC:30453]\n#> ENSBTAG00000000009                                            forkhead box F1 [Source:VGNC Symbol;Acc:VGNC:29084]\n#> ENSBTAG00000000010                                           ubiquitin like 7 [Source:VGNC Symbol;Acc:VGNC:50128]\n#> ENSBTAG00000000011                                 L-threonine dehydrogenase [Source:VGNC Symbol;Acc:VGNC:108945]\n#> ENSBTAG00000000012                         tetratricopeptide repeat domain 33 [Source:VGNC Symbol;Acc:VGNC:36471]"
  },
  {
    "objectID": "data_import.html#importing-the-samples-metadata",
    "href": "data_import.html#importing-the-samples-metadata",
    "title": "3  Importing data",
    "section": "\n3.4 Importing the samples metadata",
    "text": "3.4 Importing the samples metadata\nAs for importing datasets or features metadata, the import_smetadata_csv() function reads in a csv file that contains information about the samples measured. Similarly to import_fmetadata_csv(), this function assumes that the csv file contains samples as rows. In this example, we have one samples information file for all of our omics datasets, but it is possible to have one separate samples metadata csv file for each omics dataset (if there are some omics-specific information such as batch, technology specifications, etc).\nWe can do this by manually creating the following targets:\n\nlist(\n  tar_target(\n    smetadata_file_all,\n    system.file(\"extdata/samples_info.csv\", package = \"moiraine\"),\n    format = \"file\"\n  ),\n\n  tar_target(\n    smetadata_all,\n    import_smetadata_csv(\n      smetadata_file_all,\n      col_id = \"animal_id\"\n    )\n  )\n)\n\nwhich is equivalent to the (more succinct) command:\n\nimport_smetadata_csv_factory(\n  files = system.file(\"extdata/samples_info.csv\", package = \"moiraine\"),\n  col_ids = \"animal_id\",\n  target_name_suffixes = \"all\"\n)\n\nThe latter command creates the targets smetadata_file_all and smetadata_all. smetadata_all stores the samples metadata imported as a data-frame:\n\ntar_read(smetadata_all) |> head()\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster\n#> R21      0.820691    0.171456                K3\n#> Y3660    0.093585    0.054195                K2\n#> Y3243    0.190262    0.765567                K1\n#> R5764    0.676299    0.110607                K3\n#> P4669    0.503471    0.107136                K3\n#> R5452    0.733992    0.000010                K3\n\nNote that in the samples metadata data-frame, the sample IDs are present both as row names and in the id column. This makes it easier to subset the datasets later on.\nAs for the other import functions, import_smetadata_csv() accepts arguments that will be passed to read_csv() in order to specify how the file should be read. The targets factory version does not have this option."
  },
  {
    "objectID": "data_import.html#creating-the-omics-sets",
    "href": "data_import.html#creating-the-omics-sets",
    "title": "3  Importing data",
    "section": "\n3.5 Creating the omics sets",
    "text": "3.5 Creating the omics sets\nOnce each dataset and associated features and samples metadata have been imported, we need to combine them into omics sets. In practice, this means that for each omics dataset, we will create an R object that stores the actual dataset alongside its relevant metadata. moiraine relies on the Biobase containers derived from Biobase::eSet to store the different omics datasets; for example, Biobase::ExpressionSet objects are used to store transcriptomics measurements. Currently, moiraine support four types of omics containers:\n\ngenomics containers, which are Biobase::SnpSet objects. The particularity of this data type is that the features metadata data-frame must contain a column named chromosome and a column named position, which store the chromosome and genomic position within the chromosome (in base pairs) of a given genomic marker or variant.\ntranscriptomics containers, which are Biobase::ExpressionSet objects. The particularity of this data type is that the features metadata data-frame must contain the following columns: chromosome, start, end, giving the chromosome, start and end positions (in base pairs) of the genes or transcripts. Moreover, the values in start and end must be integers, and for each row the value in end must be higher than the value in start.\nmetabolomics containers, which are MetabolomeSet objects (implemented within moiraine). There are no restrictions on the features metadata table for this type of containers.\nphenotype containers, which are PhenotypeSet objects (implemented within moiraine). There are no restrictions on the features metadata table for this type of containers.\n\nIn practice, the nuances between these different containers are not very important, and the type of container used to store a particular dataset will have no impact on the downstream analysis apart from the name that will be given to the omics dataset. So in order to create a container for a transcriptomics dataset in the absence of features metadata, we have to create a dummy data-frame with the columns chromosome, start and end containing the values ch1, 1, and 10 (for example) and use that as features metadata. Alternately, or for other omics data (e.g. proteomics), it is possible to use a PhenotypeSet object instead.\n\n3.5.1 Creating a single omics set\nThe function create_omics_set() provides a convenient wrapper to create such container objects from the imported datasets and metadata. It has two mandatory arguments: the dataset, which should be in the form of a matrix where the rows correspond to features and the columns to samples; and the type of omics data that the dataset represents ('genomics', 'transcriptomics', 'metabolomics' or 'phenomics'). The latter determines which type of container will be generated. Optionally, a features metadata and/or a samples metadata data-frame can be passed on via the features_metadata and samples_metadata arguments, respectively. For example, let’s create a set for the genomics data:\n\ntar_target(\n  set_geno,\n  create_omics_set(\n    data_geno,\n    omics_type = \"genomics\",\n    features_metadata = fmetadata_geno,\n    samples_metadata = smetadata_all\n  )\n)\n\nIf executed, this command will return the following warning:\n\n#> Warning: 5 samples in samples metadata not in dataset, will be removed from\n#> metadata.\n\nThis is because, when providing features and samples metadata information, the function makes sure that the feature or sample IDs present in the metadata tables match those used in the dataset. In our case, 5 sample IDs from the metadata data-frame are not present in the dataset. We can confirm that by comparing the column names of the genomics dataset to the row names of the samples metadata:\n\nsetdiff(\n  tar_read(smetadata_all) |> rownames(),\n  tar_read(data_geno) |> colnames()\n)\n#> [1] \"P4744\" \"P4772\" \"R8953\" \"U5416\" \"R9909\"\n\nRather than throwing an error, the function will add a row for each missing sample ID to the metadata data-frame, with a NA in every column, and will remove from the metadata data-frame any sample not present in the dataset. The same applies for features metadata.\nThe resulting object is a SnpSet:\n\ntar_read(set_geno)\n#> SnpSet (storageMode: lockedEnvironment)\n#> assayData: 23036 features, 139 samples \n#>   element names: call, callProbability \n#> protocolData: none\n#> phenoData\n#>   rowNames: R21 Y3660 ... O5108 (139 total)\n#>   varLabels: id feedlot ... geno_comp_cluster (10 total)\n#>   varMetadata: labelDescription\n#> featureData\n#>   featureNames: 1_41768691 10-27008241-A-C-rs42918694 ... STAT5_13516_2\n#>     (23036 total)\n#>   fvarLabels: feature_id chromosome ... fdr (13 total)\n#>   fvarMetadata: labelDescription\n#> experimentData: use 'experimentData(object)'\n#> Annotation:\n\nwhich can be queried using specific methods from the Biobase package, e.g.:\n\ntar_load(set_geno)\n\ndim(set_geno)\n#> Features  Samples \n#>    23036      139\n\nfeatureNames(set_geno) |> head()\n#> [1] \"1_41768691\"                   \"10-27008241-A-C-rs42918694\"  \n#> [3] \"10-37505419-T-C-rs136559242\"  \"10-49904259-G-A-rs471723345\" \n#> [5] \"1-109550832-G-A-rs209732846\"  \"11-104555023-A-G-rs109353933\"\n\nsampleNames(set_geno) |> head()\n#> [1] \"R21\"   \"Y3660\" \"Y3243\" \"R5764\" \"P4669\" \"R5452\"\n\nfData(set_geno) |> head() ## extracts features metadata\n#>                                                feature_id chromosome  position\n#> 1_41768691                                     1_41768691          1  42139849\n#> 10-27008241-A-C-rs42918694     10-27008241-A-C-rs42918694         10  26971270\n#> 10-37505419-T-C-rs136559242   10-37505419-T-C-rs136559242         10  37388728\n#> 10-49904259-G-A-rs471723345   10-49904259-G-A-rs471723345          0         0\n#> 1-109550832-G-A-rs209732846   1-109550832-G-A-rs209732846          1 108696486\n#> 11-104555023-A-G-rs109353933 11-104555023-A-G-rs109353933         11 104498929\n#>                              gen_train_score ref alt ilmn_strand\n#> 1_41768691                            0.6786   T   G         BOT\n#> 10-27008241-A-C-rs42918694            0.8050   A   C         TOP\n#> 10-37505419-T-C-rs136559242           0.7890   A   G         TOP\n#> 10-49904259-G-A-rs471723345           0.7970   A   G         TOP\n#> 1-109550832-G-A-rs209732846           0.8909   T   C         BOT\n#> 11-104555023-A-G-rs109353933          0.8673   T   C         BOT\n#>                              customer_strand norm_id qtl_type qtl_effect\n#> 1_41768691                               BOT       2     <NA>         NA\n#> 10-27008241-A-C-rs42918694               TOP       1     <NA>         NA\n#> 10-37505419-T-C-rs136559242              BOT       1     <NA>         NA\n#> 10-49904259-G-A-rs471723345              TOP       2     <NA>         NA\n#> 1-109550832-G-A-rs209732846              TOP       3     <NA>         NA\n#> 11-104555023-A-G-rs109353933             TOP       1     <NA>         NA\n#>                              p_value fdr\n#> 1_41768691                        NA  NA\n#> 10-27008241-A-C-rs42918694        NA  NA\n#> 10-37505419-T-C-rs136559242       NA  NA\n#> 10-49904259-G-A-rs471723345       NA  NA\n#> 1-109550832-G-A-rs209732846       NA  NA\n#> 11-104555023-A-G-rs109353933      NA  NA\n\npData(set_geno) |> head() ## extracts samples metadata\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster\n#> R21      0.820691    0.171456                K3\n#> Y3660    0.093585    0.054195                K2\n#> Y3243    0.190262    0.765567                K1\n#> R5764    0.676299    0.110607                K3\n#> P4669    0.503471    0.107136                K3\n#> R5452    0.733992    0.000010                K3\n\nNote that these methods can also be applied to the other types of containers.\n\n3.5.2 Using a target factory for creating omics sets\nThe function create_omics_set_factory() allows us to create several omics sets at once. It returns a list of targets, each storing one of the created omics set container. It takes as input arguments vectors that give for each omics set the arguments required by create_omics_set().\n\ncreate_omics_set_factory(\n  datasets = c(data_geno, data_transcripto, data_metabo),\n  omics_types = c(\"genomics\", \"transcriptomics\", \"metabolomics\"),\n  features_metadatas = c(fmetadata_geno, fmetadata_transcripto, fmetadata_metabo),\n  samples_metadatas = c(smetadata_all, smetadata_all, smetadata_all)\n)\n\nAgain, the warnings raised by the function originate from discrepancies between the datasets and associated metadata. It is always good practice to double-check manually to make sure that it is not due to a typo in the IDs or similar error.\nIf one of the datasets has no associated features or samples metadata, use NULL in the corresponding input arguments, e.g.:\n\ncreate_omics_set_factory(\n  datasets = c(data_geno, data_transcripto, data_metabo),\n  omics_types = c(\"genomics\", \"transcriptomics\", \"metabolomics\"),\n  features_metadatas = c(NULL, fmetadata_transcripto, fmetadata_metabo),\n  samples_metadatas = c(smetadata_all, NULL, smetadata_all)\n)\n\nThe create_omics_set_factory() function has a target_name_suffixes argument to customise the name of the created targets. However, if this argument is not provided, the function will attempt to read the suffixes to use from the name of the dataset targets. So in this case, it knows that the suffixes to use are 'geno', 'transcripto' and 'metabo'. Consequently, the function creates the following targets: set_geno, set_transcripto, set_metabo.\n\ntar_read(set_geno)\n#> SnpSet (storageMode: lockedEnvironment)\n#> assayData: 23036 features, 139 samples \n#>   element names: call, callProbability \n#> protocolData: none\n#> phenoData\n#>   rowNames: R21 Y3660 ... O5108 (139 total)\n#>   varLabels: id feedlot ... geno_comp_cluster (10 total)\n#>   varMetadata: labelDescription\n#> featureData\n#>   featureNames: 1_41768691 10-27008241-A-C-rs42918694 ... STAT5_13516_2\n#>     (23036 total)\n#>   fvarLabels: feature_id chromosome ... fdr (13 total)\n#>   fvarMetadata: labelDescription\n#> experimentData: use 'experimentData(object)'\n#> Annotation:\n\n\ntar_read(set_transcripto)\n#> ExpressionSet (storageMode: lockedEnvironment)\n#> assayData: 20335 features, 143 samples \n#>   element names: exprs \n#> protocolData: none\n#> phenoData\n#>   rowNames: R9497 R5969 ... Y9816 (143 total)\n#>   varLabels: id feedlot ... geno_comp_cluster (10 total)\n#>   varMetadata: labelDescription\n#> featureData\n#>   featureNames: ENSBTAG00000000005 ENSBTAG00000000008 ...\n#>     ENSBTAG00000055314 (20335 total)\n#>   fvarLabels: feature_id chromosome ... description (8 total)\n#>   fvarMetadata: labelDescription\n#> experimentData: use 'experimentData(object)'\n#> Annotation:\n\n\ntar_read(set_metabo)\n#> MetabolomeSet (storageMode: lockedEnvironment)\n#> assayData: 55 features, 139 samples \n#>   element names: call \n#> protocolData: none\n#> phenoData\n#>   rowNames: R21 Y3660 ... U5416 (139 total)\n#>   varLabels: id feedlot ... geno_comp_cluster (10 total)\n#>   varMetadata: labelDescription\n#> featureData\n#>   featureNames: HMDB00001 HMDB00008 ... HMDB01881 (55 total)\n#>   fvarLabels: feature_id hmdb_id ... de_status (16 total)\n#>   fvarMetadata: labelDescription\n#> experimentData: use 'experimentData(object)'\n#> Annotation:"
  },
  {
    "objectID": "data_import.html#creating-the-multi-omics-set",
    "href": "data_import.html#creating-the-multi-omics-set",
    "title": "3  Importing data",
    "section": "\n3.6 Creating the multi-omics set",
    "text": "3.6 Creating the multi-omics set\nFinally, we can combine the different omics sets into one multi-omics set object. moiraine makes use of the MultiDataSet package for that. MultiDataSet (Hernandez-Ferrer et al. (2017)) implements a multi-omics data container that collects, in one R object, several omics datasets alongside their associated features and samples metadata. One of the main advantages of using a MultiDataSet container is that we can pass all of the information associated with a set of related omics datasets with only one R object. In addition, the MultiDataSet package implements a number of very useful functions. For example, it is possible to assess the samples that are common to several omics sets. This is particularly useful for data integration, as the moiraine package can automatically discard samples missing from one or more datasets prior to the integration step if needed. Note that sample matching between the different omics datasets is based on sample IDs, so they must be consistent between the different datasets.\nWe will create the multi-omics set with the create_multiomics_set() function. It requires a list of the omics sets (that we created via either create_omics_set() or create_omics_set_factory()) to include, and returns a MultiDataSet::MultiDataSet-class object.\n\ntar_target(\n  mo_set,\n  create_multiomics_set(\n    list(set_geno,\n         set_transcripto,\n         set_metabo)\n  )\n)\n\n\ntar_read(mo_set)\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 23036 features, 139 samples \n#>     . rnaseq: 20335 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 23036 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 20335 rows, 8 cols (feature_id, ..., Name)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nWithin the MultiDataSet object, each omics set is assigned a name. The name depends first on the omics container type: a SnpSet set will be named snps, an ExpressionSet set will be named rnaseq, a MetabolomeSet will be named metabolome and a PhenotypeSet will be called phenotypes. If several sets of the same type are provided, they will be assigned unique names, e.g. snps+1 and snps+2 (the + symbol used as separator is set in the MultiDataSet package and cannot be changed). Alternatively, we can provide custom names for the datasets, using the datasets_names argument. These will be added to the type name (e.g. snps+customname). For example:\n\ntar_target(\n  mo_set_with_names,\n  create_multiomics_set(\n    list(set_geno,\n         set_transcripto,\n         set_metabo),\n    datasets_names = c(\"CaptureSeq\", \"RNAseq\", \"LCMS\")\n  )\n)\n\nreturns:\n\ntar_read(mo_set_with_names)\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps+CaptureSeq: 23036 features, 139 samples \n#>     . rnaseq+RNAseq: 20335 features, 143 samples \n#>     . metabolome+LCMS: 55 features, 139 samples \n#>  . featureData:\n#>     . snps+CaptureSeq: 23036 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq+RNAseq: 20335 rows, 8 cols (feature_id, ..., Name)\n#>     . metabolome+LCMS: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps+CaptureSeq: YES\n#>     . rnaseq+RNAseq: YES\n#>     . metabolome+LCMS: NO\n#>  . phenoData:\n#>     . snps+CaptureSeq: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq+RNAseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome+LCMS: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nImportantly, the create_multiomics_set() function makes sure that samples metadata is consistent across the datasets for common samples. That is, if the same column (i.e. with the same name) is present in the samples metadata of several omics datasets, the values in this column must match for each sample present in all datasets. Otherwise, the function returns an error.\nIn the following chapter on Inspecting the MultiDataSet object, we will see how to handle the MultiDataSet object we just created. Alternatively, the MultiDataSet package vignette provides examples of constructing, querying and subsetting MultiDataSet objects."
  },
  {
    "objectID": "data_import.html#recap-targets-list",
    "href": "data_import.html#recap-targets-list",
    "title": "3  Importing data",
    "section": "\n3.7 Recap – targets list",
    "text": "3.7 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for data import\n\nlist(\n  ## Data import using a target factory\n  import_dataset_csv_factory(\n    files = c(\n      system.file(\"extdata/genomics_dataset.csv\", package = \"moiraine\"),\n      system.file(\"extdata/transcriptomics_dataset.csv\", package = \"moiraine\"),\n      system.file(\"extdata/metabolomics_dataset.csv\", package = \"moiraine\")\n    ),\n    col_ids = c(\"marker\", \"gene_id\", \"sample_id\"),\n    features_as_rowss = c(TRUE, TRUE, FALSE),\n    target_name_suffixes = c(\"geno\", \"transcripto\", \"metabo\")\n  ),\n  \n  ## Genomics features metadata file\n  tar_target(\n    fmetadata_file_geno,\n    system.file(\"extdata/genomics_features_info.csv\", package = \"moiraine\"),\n    format = \"file\"\n  ),\n  \n  ## Genomics features metadata import\n  tar_target(\n    fmetadata_geno,\n    import_fmetadata_csv(\n      fmetadata_file_geno,\n      col_id = \"marker\",\n      col_types = c(\"chromosome\" = \"c\")\n    )\n  ),\n  \n  \n  ## Metabolomics features metadata import\n  import_fmetadata_csv_factory(\n    files = c(\n      system.file(\"extdata/metabolomics_features_info.csv\", package = \"moiraine\")\n    ),\n    col_ids = c(\"feature_id\"),\n    target_name_suffixes = c(\"metabo\")\n  ),\n  \n  ## Transcriptomics features metadata import\n  import_fmetadata_gff_factory(\n    files = system.file(\"extdata/bos_taurus_gene_model.gff3\", package = \"moiraine\"),\n    feature_types = \"genes\",\n    add_fieldss = c(\"Name\", \"description\"),\n    target_name_suffixes = \"transcripto\"\n  ),\n  \n  ## Samples metadata import\n  import_smetadata_csv_factory(\n    files = system.file(\"extdata/samples_info.csv\", package = \"moiraine\"),\n    col_ids = \"animal_id\",\n    target_name_suffixes = \"all\"\n  ),\n  \n  ## Creating omics sets for each dataset\n  create_omics_set_factory(\n    datasets = c(data_geno, data_transcripto, data_metabo),\n    omics_types = c(\"genomics\", \"transcriptomics\", \"metabolomics\"),\n    features_metadatas = c(fmetadata_geno, fmetadata_transcripto, fmetadata_metabo),\n    samples_metadatas = c(smetadata_all, smetadata_all, smetadata_all)\n  ),\n  \n  ## Creating the MultiDataSet object\n  tar_target(\n    mo_set,\n    create_multiomics_set(\n      list(set_geno,\n           set_transcripto,\n           set_metabo)\n    )\n  )\n)\n\n\n\n\n\nHernandez-Ferrer, Carles, Carlos Ruiz-Arenas, Alba Beltran-Gomila, and Juan R. González. 2017. “MultiDataSet: An r Package for Encapsulating Multiple Data Sets with Application to Omic Data Integration.” BMC Bioinformatics 18 (1): 36. https://doi.org/10.1186/s12859-016-1455-1."
  },
  {
    "objectID": "inspecting_multidataset.html#querying-datasets-names-and-dimensions",
    "href": "inspecting_multidataset.html#querying-datasets-names-and-dimensions",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.1 Querying datasets names and dimensions",
    "text": "4.1 Querying datasets names and dimensions\nThe names of the omics datasets stored in a MultiDataSet object can be obtained with:\n\nnames(mo_set)\n#> [1] \"snps\"       \"rnaseq\"     \"metabolome\"\n\nIt is also possible to query the number of features and samples in each dataset via n_features() and n_samples(). Both functions return a named integer vector:\n\nn_features(mo_set)\n#>       snps     rnaseq metabolome \n#>      23036      20335         55\n\n\nn_samples(mo_set)\n#>       snps     rnaseq metabolome \n#>        139        143        139\n\nThe feature and sample IDs for each dataset can be extracted with the get_features() and get_samples() functions. Both functions return a named list of features or samples ID for each omics dataset:\n\nget_features(mo_set) |> str()\n#> List of 3\n#>  $ snps      : chr [1:23036] \"1_41768691\" \"10-27008241-A-C-rs42918694\" \"10-37505419-T-C-rs136559242\" \"10-49904259-G-A-rs471723345\" ...\n#>  $ rnaseq    : chr [1:20335] \"ENSBTAG00000000005\" \"ENSBTAG00000000008\" \"ENSBTAG00000000009\" \"ENSBTAG00000000010\" ...\n#>  $ metabolome: chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n\n\nget_samples(mo_set) |> str()\n#> List of 3\n#>  $ snps      : chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>  $ rnaseq    : chr [1:143] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>  $ metabolome: chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ..."
  },
  {
    "objectID": "inspecting_multidataset.html#extracting-datasets-and-metadata",
    "href": "inspecting_multidataset.html#extracting-datasets-and-metadata",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.2 Extracting datasets and metadata",
    "text": "4.2 Extracting datasets and metadata\nWe can extract the dataset matrices from a MultiDataSet object with the get_datasets() function, which returns a named list of matrices, each with features as rows and samples as columns:\n\nget_datasets(mo_set) |> str()\n#> List of 3\n#>  $ snps      : num [1:23036, 1:139] 1 2 0 1 2 0 1 2 1 1 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:23036] \"1_41768691\" \"10-27008241-A-C-rs42918694\" \"10-37505419-T-C-rs136559242\" \"10-49904259-G-A-rs471723345\" ...\n#>   .. ..$ : chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>  $ rnaseq    : num [1:20335, 1:143] 733 6 0 2693 0 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:20335] \"ENSBTAG00000000005\" \"ENSBTAG00000000008\" \"ENSBTAG00000000009\" \"ENSBTAG00000000010\" ...\n#>   .. ..$ : chr [1:143] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>  $ metabolome: num [1:55, 1:139] 9.1 58.2 403 172.6 0.7 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n#>   .. ..$ : chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n\n\nget_datasets(mo_set)[[\"snps\"]][1:5, 1:5]\n#>                             R21 Y3660 Y3243 R5764 P4669\n#> 1_41768691                    1     0     2     2     1\n#> 10-27008241-A-C-rs42918694    2     2     2     1     2\n#> 10-37505419-T-C-rs136559242   0     1     0     2     0\n#> 10-49904259-G-A-rs471723345   1     2     2     2     2\n#> 1-109550832-G-A-rs209732846   2     2     1     2     2\n\nTo obtain the matrix for a single dataset from the MultiDataSet object, the get_dataset_matrix() function can be used instead:\n\nget_dataset_matrix(mo_set, \"snps\")[1:5, 1:5]\n#>                             R21 Y3660 Y3243 R5764 P4669\n#> 1_41768691                    1     0     2     2     1\n#> 10-27008241-A-C-rs42918694    2     2     2     1     2\n#> 10-37505419-T-C-rs136559242   0     1     0     2     0\n#> 10-49904259-G-A-rs471723345   1     2     2     2     2\n#> 1-109550832-G-A-rs209732846   2     2     1     2     2\n\nSimilarly, the functions get_features_metadata() and get_samples_metadata() each return a named list of feature or sample metadata data-frames, one per omics dataset:\n\nget_features_metadata(mo_set) |> str()\n#> List of 3\n#>  $ snps      :'data.frame':  23036 obs. of  13 variables:\n#>   ..$ feature_id     : chr [1:23036] \"1_41768691\" \"10-27008241-A-C-rs42918694\" \"10-37505419-T-C-rs136559242\" \"10-49904259-G-A-rs471723345\" ...\n#>   ..$ chromosome     : chr [1:23036] \"1\" \"10\" \"10\" \"0\" ...\n#>   ..$ position       : num [1:23036] 4.21e+07 2.70e+07 3.74e+07 0.00 1.09e+08 ...\n#>   ..$ gen_train_score: num [1:23036] 0.679 0.805 0.789 0.797 0.891 ...\n#>   ..$ ref            : chr [1:23036] \"T\" \"A\" \"A\" \"A\" ...\n#>   ..$ alt            : chr [1:23036] \"G\" \"C\" \"G\" \"G\" ...\n#>   ..$ ilmn_strand    : chr [1:23036] \"BOT\" \"TOP\" \"TOP\" \"TOP\" ...\n#>   ..$ customer_strand: chr [1:23036] \"BOT\" \"TOP\" \"BOT\" \"TOP\" ...\n#>   ..$ norm_id        : num [1:23036] 2 1 1 2 3 1 3 3 0 0 ...\n#>   ..$ qtl_type       : chr [1:23036] NA NA NA NA ...\n#>   ..$ qtl_effect     : num [1:23036] NA NA NA NA NA NA NA NA NA NA ...\n#>   ..$ p_value        : num [1:23036] NA NA NA NA NA NA NA NA NA NA ...\n#>   ..$ fdr            : num [1:23036] NA NA NA NA NA NA NA NA NA NA ...\n#>  $ rnaseq    :'data.frame':  20335 obs. of  8 variables:\n#>   ..$ feature_id : chr [1:20335] \"ENSBTAG00000000005\" \"ENSBTAG00000000008\" \"ENSBTAG00000000009\" \"ENSBTAG00000000010\" ...\n#>   ..$ chromosome : Factor w/ 178 levels \"1\",\"2\",\"3\",\"4\",..: 17 29 18 21 8 20 20 5 5 23 ...\n#>   ..$ start      : int [1:20335] 65389743 32214439 12338037 34209956 7950815 33708626 33674806 74883636 74905031 27720176 ...\n#>   ..$ end        : int [1:20335] 65505336 32244810 12342272 34223394 7971600 33732944 33703223 74893752 74919112 27721739 ...\n#>   ..$ width      : int [1:20335] 115594 30372 4236 13439 20786 24319 28418 10117 14082 1564 ...\n#>   ..$ strand     : Factor w/ 3 levels \"+\",\"-\",\"*\": 1 2 1 1 2 1 1 2 2 2 ...\n#>   ..$ Name       : chr [1:20335] \"GRK3\" \"KCNJ1\" \"FOXF1\" \"UBL7\" ...\n#>   ..$ description: chr [1:20335] \"G protein-coupled receptor kinase 3 [Source:VGNC Symbol;Acc:VGNC:53904]\" \"potassium inwardly rectifying channel subfamily J member 1 [Source:VGNC Symbol;Acc:VGNC:30453]\" \"forkhead box F1 [Source:VGNC Symbol;Acc:VGNC:29084]\" \"ubiquitin like 7 [Source:VGNC Symbol;Acc:VGNC:50128]\" ...\n#>  $ metabolome:'data.frame':  55 obs. of  16 variables:\n#>   ..$ feature_id                  : chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n#>   ..$ hmdb_id                     : chr [1:55] \"HMDB0000001\" \"HMDB0000008\" \"HMDB0000042\" \"HMDB0000043\" ...\n#>   ..$ name                        : chr [1:55] \"1-Methylhistidine\" \"2-Hydroxybutyric acid\" \"Acetic acid\" \"Betaine\" ...\n#>   ..$ chemical_formula            : chr [1:55] \"C7H11N3O2\" \"C4H8O3\" \"C2H4O2\" \"C5H12NO2\" ...\n#>   ..$ monisotopic_molecular_weight: num [1:55] 169 104 60 118 102 ...\n#>   ..$ cas_registry_number         : chr [1:55] \"332-80-9\" \"3347-90-8\" \"64-19-7\" \"6915-17-9\" ...\n#>   ..$ smiles                      : chr [1:55] \"CN1C=NC(C[C@H](N)C(O)=O)=C1\" \"CC[C@H](O)C(O)=O\" \"CC(O)=O\" \"C[N+](C)(C)CC(O)=O\" ...\n#>   ..$ inchikey                    : chr [1:55] \"BRMWTNUJHUMWMS-LURJTMIESA-N\" \"AFENDNXGAFYKQO-VKHMYHEASA-N\" \"QTBSBXVTEAMEQO-UHFFFAOYSA-N\" \"KWIUHFFTVRNATP-UHFFFAOYSA-O\" ...\n#>   ..$ kegg_id                     : chr [1:55] \"C01152\" \"C05984\" \"C00033\" NA ...\n#>   ..$ direct_parent               : chr [1:55] \"Histidine and derivatives\" \"Alpha hydroxy acids and derivatives\" \"Carboxylic acids\" \"Alpha amino acids\" ...\n#>   ..$ super_class                 : chr [1:55] \"Organic acids and derivatives\" \"Organic acids and derivatives\" \"Organic acids and derivatives\" \"Organic acids and derivatives\" ...\n#>   ..$ t_value                     : num [1:55] -0.556 0.218 -12.532 -7.907 -0.437 ...\n#>   ..$ p_value                     : num [1:55] 5.80e-01 8.28e-01 1.75e-24 7.83e-13 6.63e-01 ...\n#>   ..$ padj                        : num [1:55] 6.78e-01 8.93e-01 4.82e-23 3.91e-12 7.44e-01 ...\n#>   ..$ de_signif                   : chr [1:55] \"Not DE\" \"Not DE\" \"DE\" \"DE\" ...\n#>   ..$ de_status                   : chr [1:55] \"Not DE\" \"Not DE\" \"downregulated\" \"downregulated\" ...\n\n\nget_samples_metadata(mo_set) |> str()\n#> List of 3\n#>  $ snps      :'data.frame':  139 obs. of  10 variables:\n#>   ..$ id               : chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>   ..$ feedlot          : chr [1:139] \"F1\" \"F1\" \"F1\" \"F2\" ...\n#>   ..$ gender           : chr [1:139] \"female\" \"male\" \"male\" \"male\" ...\n#>   ..$ status           : chr [1:139] \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>   ..$ day_on_feed      : num [1:139] 31 19 16 46 35 49 21 16 37 37 ...\n#>   ..$ rnaseq_batch     : chr [1:139] \"B2\" \"B2\" \"B2\" \"B1\" ...\n#>   ..$ geno_comp_1      : num [1:139] 0.00785 0.85222 0.04417 0.21309 0.38939 ...\n#>   ..$ geno_comp_2      : num [1:139] 0.8207 0.0936 0.1903 0.6763 0.5035 ...\n#>   ..$ geno_comp_3      : num [1:139] 0.1715 0.0542 0.7656 0.1106 0.1071 ...\n#>   ..$ geno_comp_cluster: chr [1:139] \"K3\" \"K2\" \"K1\" \"K3\" ...\n#>  $ rnaseq    :'data.frame':  143 obs. of  10 variables:\n#>   ..$ id               : chr [1:143] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>   ..$ feedlot          : chr [1:143] \"F2\" \"F2\" \"F2\" \"F2\" ...\n#>   ..$ gender           : chr [1:143] \"male\" \"male\" \"male\" \"male\" ...\n#>   ..$ status           : chr [1:143] \"BRD\" \"BRD\" \"BRD\" \"BRD\" ...\n#>   ..$ day_on_feed      : num [1:143] 35 24 38 30 31 24 26 18 13 32 ...\n#>   ..$ rnaseq_batch     : chr [1:143] \"B1\" \"B1\" \"B1\" \"B1\" ...\n#>   ..$ geno_comp_1      : num [1:143] 0.15372 0.11066 0.14073 0.28673 0.00001 ...\n#>   ..$ geno_comp_2      : num [1:143] 0.745 0.626 0.809 0.108 0.999 ...\n#>   ..$ geno_comp_3      : num [1:143] 0.10178 0.26351 0.04987 0.60547 0.00108 ...\n#>   ..$ geno_comp_cluster: chr [1:143] \"K3\" \"K3\" \"K3\" \"K1\" ...\n#>  $ metabolome:'data.frame':  139 obs. of  10 variables:\n#>   ..$ id               : chr [1:139] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>   ..$ feedlot          : chr [1:139] \"F1\" \"F1\" \"F1\" \"F2\" ...\n#>   ..$ gender           : chr [1:139] \"female\" \"male\" \"male\" \"male\" ...\n#>   ..$ status           : chr [1:139] \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>   ..$ day_on_feed      : num [1:139] 31 19 16 46 35 49 21 16 37 37 ...\n#>   ..$ rnaseq_batch     : chr [1:139] \"B2\" \"B2\" \"B2\" \"B1\" ...\n#>   ..$ geno_comp_1      : num [1:139] 0.00785 0.85222 0.04417 0.21309 0.38939 ...\n#>   ..$ geno_comp_2      : num [1:139] 0.8207 0.0936 0.1903 0.6763 0.5035 ...\n#>   ..$ geno_comp_3      : num [1:139] 0.1715 0.0542 0.7656 0.1106 0.1071 ...\n#>   ..$ geno_comp_cluster: chr [1:139] \"K3\" \"K2\" \"K1\" \"K3\" ...\n\nFor the samples metadata, it is possible to extract a single data-frame that combines the metadata from the different datasets with the function get_samples_metadata_combined(). The only_common_cols argument controls whether only the columns that are common to the samples metadata of the different omics datasets should be returned. For this example, as the samples metadata is identical across the datasets, it makes no difference:\n\nget_samples_metadata_combined(mo_set) |> head()\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster\n#> R21      0.820691    0.171456                K3\n#> Y3660    0.093585    0.054195                K2\n#> Y3243    0.190262    0.765567                K1\n#> R5764    0.676299    0.110607                K3\n#> P4669    0.503471    0.107136                K3\n#> R5452    0.733992    0.000010                K3"
  },
  {
    "objectID": "inspecting_multidataset.html#sec-inspecting-multidataset-summary-plots",
    "href": "inspecting_multidataset.html#sec-inspecting-multidataset-summary-plots",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.3 Summary plots",
    "text": "4.3 Summary plots\nA number of plotting functions have been implemented to obtain a quick overview of the omics datasets in a MultiDataSet object.\n\n4.3.1 Samples upset plot\nFirst, the plot_samples_upset() function displays the number of common and unique samples across the datasets with an UpSet plot:\n\nplot_samples_upset(mo_set)\n\n\n\n\nAs can be seen in the upset plot above, 135 samples have measurements across all three omics datasets. In addition, 4 samples have both transcriptomics and metabolomics measurements, but no transcriptomics information; 3 samples are present in the genomics and metabolomics datasets but not the transcriptomics dataset, and the genomics and transcriptomics datasets each have a unique sample not present in the other omics datasets.\n\n4.3.2 Datasets density plots\nNext, we can show the density plot of each omics dataset with the plot_density_data() function. By default, all datasets are plotted onto the same axes, which is not very useful if they have very different scales. We can change that by setting the combined argument to FALSE, which splits the plot into one facet per dataset, and by setting scales to 'free' in order to give its own scale to each dataset:\n\nplot_density_data(mo_set, combined = FALSE, scales = \"free\")\n\n\n\n\nBy default, all datasets are represented in the density plot, but it is possible to focus on one or a subset of them via the datasets argument. This is useful here as the plots for the transcriptomics and metabolomics could benefit from a log10 transformation for the x-axis:\n\nplot_density_data(\n  mo_set,\n  datasets = c(\"rnaseq\", \"metabolome\"),\n  combined = FALSE,\n  scales = \"free\"\n) +\n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in continuous x-axis\n#> Warning: Removed 338138 rows containing non-finite values (`stat_density()`).\n\n\n\n\nNote that as the plot_density_data() function returns a ggplot, it can be further customised with other ggplot2 functions as shown above.\n\n4.3.3 Datasets mean-sd plots\nIt is also possible to assess for each dataset whether there exists a relationship between the features mean and standard deviation, with the plot_meansd_data() function. The presence of such relationship indicates that the dataset should be transformed, via a log or variance-stabilising transformation. The function requires the hexbin package to be installed:\n\nplot_meansd_data(mo_set)\n\n\n\n\nIn our case, we can see a very strong relationship between features mean and standard deviation in both the transcriptomics and metabolomics datasets, which suggest that a log or variance-stabilising transformation will be necessary in both cases (datasets transformation are covered in Chapter 6).\nNote that the hexplots are only drawn for datasets with at least 30 features, and the trend curve (in pink) is only drawn for datasets with at least 10 features."
  },
  {
    "objectID": "inspecting_multidataset.html#assessing-missing-values",
    "href": "inspecting_multidataset.html#assessing-missing-values",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.4 Assessing missing values",
    "text": "4.4 Assessing missing values\nFinally, one very important aspect to check is the presence of missing values in the datasets. The function check_missing_values() provide a summary of the number of missing values in each dataset:\n\ncheck_missing_values(mo_set)\n#> 9615 (0.3%) missing values in snps dataset, across 4093 features and 139 samples.\n#> No missing values in rnaseq dataset.\n#> 588 (7.69%) missing values in metabolome dataset, across 15 features and 45 samples.\n\nThe function returns an invisible character vector containing the messages printed above, which is useful for automatic reporting.\nIn Chapter 6, we will see how to impute missing values."
  },
  {
    "objectID": "inspecting_multidataset.html#visualising-the-datasets",
    "href": "inspecting_multidataset.html#visualising-the-datasets",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.5 Visualising the datasets",
    "text": "4.5 Visualising the datasets\nOnce the omics datasets are stored in a MultiDataSet object, we can easily visualise the measurements for a set of features of interest. As an example, we will randomly select three features from each of the omics datasets:\n\nset.seed(32)\nrandom_features <- get_features(mo_set) |>\n  map(\\(x) sample(x, size = 3, replace = FALSE)) |>\n  unlist() |>\n  unname()\n\nrandom_features\n#> [1] \"ARS-BFGL-NGS-102169_dup\" \"BovineHD0300020059\"     \n#> [3] \"BTB-01546164\"            \"ENSBTAG00000038316\"     \n#> [5] \"ENSBTAG00000016902\"      \"ENSBTAG00000048333\"     \n#> [7] \"HMDB00214\"               \"HMDB00407\"              \n#> [9] \"HMDB00182\"\n\n\n4.5.1 As a heatmap\nThe function plot_data_heatmap() allows us to view the data for these features as a heatmap. It relies on the ComplexHeatmap::Heatmap() function, and can be customised by passing arguments to this function (for example to remove the column labels):\n\nplot_data_heatmap(\n  mo_set,\n  random_features,\n  center = TRUE,\n  scale = TRUE,\n  show_column_names = FALSE\n)\n#> Warning: Not enough data to calculate distance between samples, disabling\n#> clustering of columns.\n\n\n\n\nNote that we specified that the data should be centred and scaled before plotting, to represent features from different datasets on a similar scale.\nBy default, all samples all represented, including those that are only present in some of the omics datasets (hence the warning about columns clustering). We can instead restrict the plot to only samples that are present across all datasets (only_common_samples argument), or to specific samples by passing a list of samples ID to the samples argument:\n\nplot_data_heatmap(\n  mo_set,\n  random_features,\n  center = TRUE,\n  scale = TRUE,\n  show_column_names = FALSE,\n  samples = c(\"O4713\", \"Y3660\", \"R5979\")\n)\n\n\n\n\nWe can also add samples and/or features information to the sides of the heatmap through the samples_info and features_info arguments. These two arguments take a vector of column names from the samples or features metadata table, respectively. The ComplexHeatmap::Heatmap() picks random colours for these annotations, but we can set specific colour palettes by passing a list of colour palettes through the argument colours_list. For continuous annotations, the colour palette must be generated with circlize::colorRamp2().\n\nplot_data_heatmap(\n  mo_set,\n  random_features,\n  center = TRUE,\n  scale = TRUE,\n  show_column_names = FALSE,\n  only_common_samples = TRUE,\n  samples_info = c(\"status\", \"day_on_feed\"),\n  features_info = c(\"chromosome\"),\n  colours_list = list(\n    \"status\" = c(\"Control\" = \"gold\", \"BRD\" = \"lightblue\"),\n    \"day_on_feed\" = colorRamp2(c(5, 70), c(\"white\", \"pink3\"))\n  )\n)\n\n\n\n\nWe can also use information from the features metadata tables to give a more meaningful label to the features. For example, we can use the column Name from the transcriptomics features metadata and the column name from the metabolomics features metadata to label the features. This is done by passing a named list through the label_cols argument, where each element is the name of the column to use and the name of the element gives the name of the dataset in the MultiDataSet object. If these labels are too long, we can truncate them through the truncate argument (see the function help).\n\nplot_data_heatmap(\n  mo_set,\n  random_features,\n  center = TRUE,\n  scale = TRUE,\n  show_column_names = FALSE,\n  only_common_samples = TRUE,\n  samples_info = c(\"status\", \"day_on_feed\"),\n  features_info = c(\"chromosome\"),\n  colours_list = list(\n    \"status\" = c(\"Control\" = \"gold\", \"BRD\" = \"lightblue\"),\n    \"day_on_feed\" = colorRamp2(c(5, 70), c(\"white\", \"pink3\"))\n  ),\n  label_cols = list(\n    \"rnaseq\" = \"Name\",\n    \"metabolome\" = \"name\"\n  ),\n  truncate = 20\n)\n\n\n\n\nNote that because we didn’t include the snps dataset in the list passed through label_cols, the ID of the features are used as labels.\n\n4.5.2 Against samples covariates\nAlternatively, we can display the features’ measurements against some samples covariate, with the plot_data_covariate() function. As for the plot_data_heatmap() function, the plot shows data from all samples, unless otherwise specified (through either the common_samples_only or samples arguments). The covariate is specified as a column name from the samples metadata (can be from any dataset’s samples metadata). If the covariate is categorical, the function generates violin plots. For example, we can represent the feature’s measurements against the animal disease status:\n\nplot_data_covariate(\n  mo_set,\n  \"status\",\n  random_features,\n  only_common_samples = TRUE\n)\n\n\n\n\nWe can use other columns from the samples metadata to customise the points colour and shape. For the colour, the constructed plot will depend on whether the corresponding in categorical or numeric:\n\nplot_data_covariate(\n  mo_set,\n  \"status\",\n  random_features,\n  only_common_samples = TRUE,\n  colour_by = \"feedlot\",\n  shape_by = \"feedlot\"\n)\n\n\n\n\nplot_data_covariate(\n  mo_set,\n  \"status\",\n  random_features,\n  only_common_samples = TRUE,\n  colour_by = \"day_on_feed\",\n  shape_by = \"feedlot\"\n)\n\n\n\n\nIf instead the covariate is numerical, the function produces scatterplots with a loess curve for each feature:\n\nplot_data_covariate(\n  mo_set,\n  \"day_on_feed\",\n  random_features,\n  only_common_samples = TRUE\n)\n\n\n\n\nAgain, we can use other samples information to specify the colour or shapes of the samples. Note that if the covariate used for points colour is discrete, a loess curve will be fitted for each category. If the covariate is continuous, or if changing the shape of the points, only one loess curve will be fitted for all data points.\n\nplot_data_covariate(\n  mo_set,\n  \"day_on_feed\",\n  random_features,\n  only_common_samples = TRUE,\n  colour_by = \"status\",\n  shape_by = \"status\"\n)\n\n\n\n\nplot_data_covariate(\n  mo_set,\n  \"day_on_feed\",\n  random_features,\n  only_common_samples = TRUE,\n  colour_by = \"day_on_feed\",\n  shape_by = \"feedlot\"\n)\n\n\n\n\nThe features can be renamed using features metadata through the label_cols argument in the same way that with the plot_data_heatmap() function:\n\nplot_data_covariate(\n  mo_set,\n  \"day_on_feed\",\n  random_features,\n  only_common_samples = TRUE,\n  label_cols = list(\n    \"rnaseq\" = \"Name\",\n    \"metabolome\" = \"name\"\n  )\n)"
  },
  {
    "objectID": "inspecting_multidataset.html#recap-targets-list",
    "href": "inspecting_multidataset.html#recap-targets-list",
    "title": "4  Inspecting the MultiDataSet object",
    "section": "\n4.6 Recap – targets list",
    "text": "4.6 Recap – targets list\nAlthough we didn’t create any new target in this section, we can turn some plots into targets.\n\nTargets list for inspecting a MultiDataSet object\n\nlist(\n  ## Creating a density plot for each dataset\n  tar_target(\n    density_plots,\n    plot_density_data(\n      mo_set,\n      combined = FALSE,\n      scales = \"free\"\n    )\n  ),\n\n  ## Plotting the relationship between features mean and standard deviation\n  ## for each dataset\n  tar_target(\n    mean_sd_plots,\n    plot_meansd_data(mo_set)\n  ),\n  \n  ## Assessing missing values\n  tar_target(\n    n_missing_values,\n    check_missing_values(mo_set)\n  )\n)"
  },
  {
    "objectID": "modifying_multidataset.html#modifying-a-dataset-matrix",
    "href": "modifying_multidataset.html#modifying-a-dataset-matrix",
    "title": "5  Modifying the MultiDataSet object",
    "section": "\n5.1 Modifying a dataset matrix",
    "text": "5.1 Modifying a dataset matrix\nLet us imagine that, prior to performing the data transformation step (which we will see in Chapter 6), we want to replace all zero values in the transcriptomics dataset with a small value, to avoid issues during the log-transformation process (note that this is only to illustrate this functionality, for the actual analysis we will use a different transformation that handles zero values). We will pick this small value as half of the non-null minimum value in the dataset (which will be 0.5, since we are working with count data). We can compute the new matrix of RNAseq counts:\n\nrnaseq_mat <- get_dataset_matrix(mo_set, \"rnaseq\")\nrnaseq_mat[1:5, 1:5]\n#>                    R9497 R5969 R5327 R5979 R9504\n#> ENSBTAG00000000005   733  1407  2919   872   740\n#> ENSBTAG00000000008     6     7     3    10    20\n#> ENSBTAG00000000009     0     1     9     0     0\n#> ENSBTAG00000000010  2693  2212  2937  2000  2345\n#> ENSBTAG00000000011     0     1     1     1     0\n\nsmall_val <- (1/2) * min(rnaseq_mat[rnaseq_mat != 0])\nsmall_val\n#> [1] 0.5\n\nnew_mat <- rnaseq_mat\nnew_mat[new_mat == 0] <- small_val\n\nnew_mat[1:5, 1:5]\n#>                     R9497 R5969 R5327  R5979  R9504\n#> ENSBTAG00000000005  733.0  1407  2919  872.0  740.0\n#> ENSBTAG00000000008    6.0     7     3   10.0   20.0\n#> ENSBTAG00000000009    0.5     1     9    0.5    0.5\n#> ENSBTAG00000000010 2693.0  2212  2937 2000.0 2345.0\n#> ENSBTAG00000000011    0.5     1     1    1.0    0.5\n\nIn order to replace the rnaseq dataset stored in the MultiDataSet object with this new matrix, we pass both the object and the new matrix to the replace_dataset() function, along with the name of the omics dataset whose matrix should be replaced:\n\nmo_set_modif <- replace_dataset(mo_set, \"rnaseq\", new_mat)\n\n## Checking that the replacement has been done\nget_dataset_matrix(mo_set_modif, \"rnaseq\")[1:5, 1:5]\n#>                     R9497 R5969 R5327  R5979  R9504\n#> ENSBTAG00000000005  733.0  1407  2919  872.0  740.0\n#> ENSBTAG00000000008    6.0     7     3   10.0   20.0\n#> ENSBTAG00000000009    0.5     1     9    0.5    0.5\n#> ENSBTAG00000000010 2693.0  2212  2937 2000.0 2345.0\n#> ENSBTAG00000000011    0.5     1     1    1.0    0.5\n\nNote that this only works for modifying the values within an omics dataset, and not for filtering, since both features and samples number and IDs in the new dataset matrix should match the ones in the original matrix:\n\nmo_set_modif <- replace_dataset(mo_set, \"rnaseq\", new_mat[1:10, 1:10])\n#> Error in replace_dataset(mo_set, \"rnaseq\", new_mat[1:10, 1:10]): 'new_data' argument has incorrect dimensions. Should have 20335 rows (features) and 143 columns (samples).\n\n\nClick here to see a targets version of the code.\n\nlist(\n  ## Replacing zero values in RNAseq dataset\n  ## (note that it is more tidy to write a function for that and call it here)\n  tar_target(\n    rnaseq_mat_nozero,\n    {\n      rnaseq_mat <- get_dataset_matrix(mo_set, \"rnaseq\")\n      small_val <- (1/2) * min(rnaseq_mat[rnaseq_mat != 0])\n      new_mat <- rnaseq_mat\n      new_mat[new_mat == 0] <- small_val\n\n      new_mat\n    }\n  ),\n\n  ## Replacing RNAseq dataset in MultiDataSet object\n  tar_target(\n    mo_set_rnaseq_nozero,\n    replace_dataset(mo_set, \"rnaseq\", rnaseq_mat_nozero)\n  )\n)"
  },
  {
    "objectID": "modifying_multidataset.html#adding-information-to-features-metadata",
    "href": "modifying_multidataset.html#adding-information-to-features-metadata",
    "title": "5  Modifying the MultiDataSet object",
    "section": "\n5.2 Adding information to features metadata",
    "text": "5.2 Adding information to features metadata\nIn the case of the transcriptomics dataset, we extracted the features metadata directly from a GFF file, which provides information about the genome annotation used. However, we might want to add information about the genes from a different source. We could add this information to the data-frame generated with import_fmetadata_gff() (see Section 3.3.3) before creating theMultiDataSet object, but we will demonstrate here how to add information once we’ve already created the object. Note that we will use targets for this example, as we will incorporate these changes in our analysis pipeline.\nLet’s start by reading in the differential expression results:\n\nlist(\n  tar_target(\n    rnaseq_de_res_file,\n    system.file(\n      \"extdata/transcriptomics_de_results.csv\",\n      package = \"moiraine\"\n    ),\n    format = \"file\"\n  ),\n\n  tar_target(\n    rnaseq_de_res_df,\n    read_csv(rnaseq_de_res_file) |>\n      rename(feature_id = gene_id) |>\n      mutate(dataset = \"rnaseq\")\n  )\n)\n\nNotice that in the results file, the gene IDs are stored in the gene_id column. Here, we rename this column as feature_id, which is required for adding it to the features metadata. In addition, we create a dataset column which contains the name of the dataset in the MultiDataSet object to which the features belong. This is also necessary.\nThe differential results look like this:\n\ntar_read(rnaseq_de_res_df) |>\n  head()\n#> # A tibble: 6 × 9\n#>   feature_id  log_fc log_cpm     f  p_value      fdr de_signif de_status dataset\n#>   <chr>        <dbl>   <dbl> <dbl>    <dbl>    <dbl> <chr>     <chr>     <chr>  \n#> 1 ENSBTAG000…   4.61    4.19  358. 5.32e-40 5.93e-36 DE        upregula… rnaseq \n#> 2 ENSBTAG000…   3.80    6.83  356. 6.33e-40 5.93e-36 DE        upregula… rnaseq \n#> 3 ENSBTAG000…   5.41    2.24  347. 2.41e-39 1.50e-35 DE        upregula… rnaseq \n#> 4 ENSBTAG000…   4.34    3.52  344. 3.40e-39 1.59e-35 DE        upregula… rnaseq \n#> 5 ENSBTAG000…   2.18    6.74  327. 4.05e-38 1.52e-34 DE        upregula… rnaseq \n#> 6 ENSBTAG000…  -1.31    2.64  316. 2.39e-37 7.48e-34 Not DE    Not DE    rnaseq\n\nWe can now use the add_features_metadata() function to add this table to the features metadata of the transcriptomics dataset. The new MultiDataSet object that includes information about the differential expression results will be saved in the mo_set_de target:\n\ntar_target(\n  mo_set_de,\n  add_features_metadata(mo_set, rnaseq_de_res_df)\n)\n\nThe new information has been added to the features metadata of the transcriptomics dataset.\n\ntar_read(mo_set_de) |>\n  get_features_metadata() |>\n  pluck(\"rnaseq\") |>\n  head()\n#>                            feature_id chromosome    start      end  width\n#> ENSBTAG00000000005 ENSBTAG00000000005         17 65389743 65505336 115594\n#> ENSBTAG00000000008 ENSBTAG00000000008         29 32214439 32244810  30372\n#> ENSBTAG00000000009 ENSBTAG00000000009         18 12338037 12342272   4236\n#> ENSBTAG00000000010 ENSBTAG00000000010         21 34209956 34223394  13439\n#> ENSBTAG00000000011 ENSBTAG00000000011          8  7950815  7971600  20786\n#> ENSBTAG00000000012 ENSBTAG00000000012         20 33708626 33732944  24319\n#>                    strand  Name\n#> ENSBTAG00000000005      +  GRK3\n#> ENSBTAG00000000008      - KCNJ1\n#> ENSBTAG00000000009      + FOXF1\n#> ENSBTAG00000000010      +  UBL7\n#> ENSBTAG00000000011      -   TDH\n#> ENSBTAG00000000012      + TTC33\n#>                                                                                                       description\n#> ENSBTAG00000000005                        G protein-coupled receptor kinase 3 [Source:VGNC Symbol;Acc:VGNC:53904]\n#> ENSBTAG00000000008 potassium inwardly rectifying channel subfamily J member 1 [Source:VGNC Symbol;Acc:VGNC:30453]\n#> ENSBTAG00000000009                                            forkhead box F1 [Source:VGNC Symbol;Acc:VGNC:29084]\n#> ENSBTAG00000000010                                           ubiquitin like 7 [Source:VGNC Symbol;Acc:VGNC:50128]\n#> ENSBTAG00000000011                                 L-threonine dehydrogenase [Source:VGNC Symbol;Acc:VGNC:108945]\n#> ENSBTAG00000000012                         tetratricopeptide repeat domain 33 [Source:VGNC Symbol;Acc:VGNC:36471]\n#>                         log_fc    log_cpm          f      p_value          fdr\n#> ENSBTAG00000000005  0.13603041  5.9051177  4.1626465 4.324808e-02 7.170137e-02\n#> ENSBTAG00000000008 -0.12965356 -0.7437052  1.0676845 3.032916e-01 3.938643e-01\n#> ENSBTAG00000000009  1.27141158 -2.5627934 23.9562951 2.731328e-06 9.665375e-06\n#> ENSBTAG00000000010  0.39567729  6.2563594 60.5303416 1.581955e-12 1.521160e-11\n#> ENSBTAG00000000011  0.07766873 -2.7608361  0.1418457 7.070363e-01 7.773874e-01\n#> ENSBTAG00000000012  0.15756169  3.6628775 11.0448775 1.141125e-03 2.623062e-03\n#>                    de_signif de_status\n#> ENSBTAG00000000005    Not DE    Not DE\n#> ENSBTAG00000000008    Not DE    Not DE\n#> ENSBTAG00000000009    Not DE    Not DE\n#> ENSBTAG00000000010    Not DE    Not DE\n#> ENSBTAG00000000011    Not DE    Not DE\n#> ENSBTAG00000000012    Not DE    Not DE\n\nNote that with this function, we can add information about features from different datasets at once, which is why there needs to be a dataset column in the data-frame to add, indicating the dataset to which each feature belongs. Also, not all features from a given dataset need to be present in this new data-frame; it is possible to add information for only a subset of them. In that case, the function will throw a warning giving the number of features from the corresponding dataset missing from the new data-frame, and the new columns in the corresponding features metadata will be filled with NA for features not present. However, it is only possible to add columns that do not already exist in the corresponding features metadata."
  },
  {
    "objectID": "modifying_multidataset.html#adding-information-to-samples-metadata",
    "href": "modifying_multidataset.html#adding-information-to-samples-metadata",
    "title": "5  Modifying the MultiDataSet object",
    "section": "\n5.3 Adding information to samples metadata",
    "text": "5.3 Adding information to samples metadata\nSimilarly, we can add a data-frame of information to the samples metadata. Here, we will create a table that contains “new” simulated information about the samples that we want to incorporate in our MultiDataSet object.\n\n## Getting the list of samples ID across the datasets\nsamples_list <- get_samples(mo_set) |>\n  unlist() |>\n  unname() |>\n  unique()\n\n## Simulating new information table, with new samples grouping\nnew_samples_df <- tibble(id = samples_list) |>\n  mutate(new_group = sample(letters[1:3], n(), replace = TRUE))\n\nhead(new_samples_df)\n#> # A tibble: 6 × 2\n#>   id    new_group\n#>   <chr> <chr>    \n#> 1 R21   b        \n#> 2 Y3660 c        \n#> 3 Y3243 c        \n#> 4 R5764 c        \n#> 5 P4669 a        \n#> 6 R5452 a\n\nNote that the sample IDs must be stored in a column named id. We will use the add_samples_metadata() function to add this new data-frame to the samples metadata in our MultiDataSet object. There are several options for which samples metadata tables should be modified; this is controlled through the datasets argument of the function. By default, the information is added to the samples metadata table of all omics datasets (case when datasets = NULL):\n\nmo_set_new_samples_info <- add_samples_metadata(mo_set, new_samples_df)\n#> Warning: snps dataset: 5 sample IDs not in 'mo_data', will be removed from\n#> samples metadata.\n#> Warning: rnaseq dataset: 1 sample IDs not in 'mo_data', will be removed from\n#> samples metadata.\n#> Warning: metabolome dataset: 5 sample IDs not in 'mo_data', will be removed\n#> from samples metadata.\n\nmo_set_new_samples_info |>\n  get_samples_metadata() |>\n  map(head)\n#> $snps\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster new_group\n#> R21      0.820691    0.171456                K3         b\n#> Y3660    0.093585    0.054195                K2         c\n#> Y3243    0.190262    0.765567                K1         c\n#> R5764    0.676299    0.110607                K3         c\n#> P4669    0.503471    0.107136                K3         a\n#> R5452    0.733992    0.000010                K3         a\n#> \n#> $rnaseq\n#>          id feedlot gender status day_on_feed rnaseq_batch geno_comp_1\n#> R9497 R9497      F2   male    BRD          35           B1    0.153716\n#> R5969 R5969      F2   male    BRD          24           B1    0.110663\n#> R5327 R5327      F2   male    BRD          38           B1    0.140730\n#> R5979 R5979      F2   male    BRD          30           B1    0.286733\n#> R9504 R9504      F2   male    BRD          31           B1    0.000010\n#> R5994 R5994      F2   male    BRD          24           B1    0.129271\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster new_group\n#> R9497    0.744505    0.101779                K3         b\n#> R5969    0.625823    0.263514                K3         b\n#> R5327    0.809396    0.049874                K3         c\n#> R5979    0.107794    0.605473                K1         b\n#> R9504    0.998913    0.001077                K3         c\n#> R5994    0.034351    0.836377                K1         c\n#> \n#> $metabolome\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster new_group\n#> R21      0.820691    0.171456                K3         b\n#> Y3660    0.093585    0.054195                K2         c\n#> Y3243    0.190262    0.765567                K1         c\n#> R5764    0.676299    0.110607                K3         c\n#> P4669    0.503471    0.107136                K3         a\n#> R5452    0.733992    0.000010                K3         a\n\nHowever, it is also possible to specify for which dataset(s) the changes should be made, by passing their name to the datasets argument.\n\nmo_set_new_samples_info <- add_samples_metadata(\n  mo_set, \n  new_samples_df, \n  datasets = c(\"rnaseq\", \"metabolome\")\n)\n#> Warning: rnaseq dataset: 1 sample IDs not in 'mo_data', will be removed from\n#> samples metadata.\n#> Warning: metabolome dataset: 5 sample IDs not in 'mo_data', will be removed\n#> from samples metadata.\n\nmo_set_new_samples_info |>\n  get_samples_metadata() |>\n  map(head)\n#> $snps\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster\n#> R21      0.820691    0.171456                K3\n#> Y3660    0.093585    0.054195                K2\n#> Y3243    0.190262    0.765567                K1\n#> R5764    0.676299    0.110607                K3\n#> P4669    0.503471    0.107136                K3\n#> R5452    0.733992    0.000010                K3\n#> \n#> $rnaseq\n#>          id feedlot gender status day_on_feed rnaseq_batch geno_comp_1\n#> R9497 R9497      F2   male    BRD          35           B1    0.153716\n#> R5969 R5969      F2   male    BRD          24           B1    0.110663\n#> R5327 R5327      F2   male    BRD          38           B1    0.140730\n#> R5979 R5979      F2   male    BRD          30           B1    0.286733\n#> R9504 R9504      F2   male    BRD          31           B1    0.000010\n#> R5994 R5994      F2   male    BRD          24           B1    0.129271\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster new_group\n#> R9497    0.744505    0.101779                K3         b\n#> R5969    0.625823    0.263514                K3         b\n#> R5327    0.809396    0.049874                K3         c\n#> R5979    0.107794    0.605473                K1         b\n#> R9504    0.998913    0.001077                K3         c\n#> R5994    0.034351    0.836377                K1         c\n#> \n#> $metabolome\n#>          id feedlot gender  status day_on_feed rnaseq_batch geno_comp_1\n#> R21     R21      F1 female Control          31           B2    0.007853\n#> Y3660 Y3660      F1   male Control          19           B2    0.852220\n#> Y3243 Y3243      F1   male Control          16           B2    0.044171\n#> R5764 R5764      F2   male Control          46           B1    0.213094\n#> P4669 P4669      F3   male     BRD          35           B2    0.389393\n#> R5452 R5452      F2   male Control          49           B1    0.265998\n#>       geno_comp_2 geno_comp_3 geno_comp_cluster new_group\n#> R21      0.820691    0.171456                K3         b\n#> Y3660    0.093585    0.054195                K2         c\n#> Y3243    0.190262    0.765567                K1         c\n#> R5764    0.676299    0.110607                K3         c\n#> P4669    0.503471    0.107136                K3         a\n#> R5452    0.733992    0.000010                K3         a\n\nIn both cases, the function throws some warnings to alert about samples missing from this new table, or samples that are not present in the original samples metadata table. These warnings should be checked to avoid issues due to typos, etc.\nAs with the add_features_metadata() function, it is possible to add information about only a subset of the samples; however the columns in the new data-frame must not already be present in the features metadata tables to which it will be added."
  },
  {
    "objectID": "modifying_multidataset.html#recap-targets-list",
    "href": "modifying_multidataset.html#recap-targets-list",
    "title": "5  Modifying the MultiDataSet object",
    "section": "\n5.4 Recap – targets list",
    "text": "5.4 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for modifying a MultiDataSet object\n\nlist(\n  ## RNAseq differential expression results file\n  tar_target(\n    rnaseq_de_res_file,\n    system.file(\n      \"extdata/transcriptomics_de_results.csv\",\n      package = \"moiraine\"\n    ),\n    format = \"file\"\n  ),\n\n  ## Reading the RNAseq differential expression results\n  tar_target(\n    rnaseq_de_res_df,\n    read_csv(rnaseq_de_res_file) |>\n      rename(feature_id = gene_id) |>\n      mutate(dataset = \"rnaseq\")\n  ),\n\n  ## Adding the differential expression results to the MultiDataSet object\n  tar_target(\n    mo_set_de,\n    add_features_metadata(mo_set, rnaseq_de_res_df)\n  )\n)"
  },
  {
    "objectID": "preprocessing.html#datasets-transformations",
    "href": "preprocessing.html#datasets-transformations",
    "title": "6  Data pre-processing",
    "section": "\n6.1 Datasets transformations",
    "text": "6.1 Datasets transformations\nAfter inspection of the density plots for the different datasets (see Section 4.3), it might be necessary to normalise or transform some or all datasets. This is necessary to mitigate the mean-variance trend that occurs in RNAseq data, for example, or simply to bring the different features to a comparable scale. Transformation here refers to applying a function to each feature (i.e. each row) within a dataset that will transform the measurement values for the feature.\nmoiraine implements several options to transform an omics dataset:\n\nVariance Stabilising Normalisation (VSN) through the vsn package – recommended for metabolomics datasets or other continuous datasets with a strong mean-variance trend;\nVariance Stabilising Transformation (VST) through the DESeq2 package – recommended for RNAseq data or any raw read count-type data;\nAutomatic selection of the best normalisation method for each feature through the bestNormalize package – recommended for phenotype data, and when the number of features is small (note that the selection of the normalisation method is done independently for each feature, so the same transformation might not be applied to all features);\nA selection of common normalisation methods through the bestNormalize package, including center/scale, log, exponential, square-root, arcsinh, Box Cox, Yeo-Johnson and ordered quantile transformations (see details in the bestNormalize vignette) – recommended when applying the same transformation to all features, e.g. log2 transformation or centering.\n\n\n6.1.1 Transforming a single dataset\nThe transformation of one dataset is done through the transform_dataset() function, which takes as input a MultiDataSet object, the name of the dataset to transform, and the name of the transformation to be applied, which should be one of vsn, vst-deseq2, best-normalize-auto or best-normalize-manual. For the latter, the name of the normalisation method from the BestNormalize package to use must also be supplied through the method argument.\nThe return_multidataset argument determines whether a MultiDataSet object with the corresponding dataset transformed should be returned. If it is set to FALSE, the function will instead return a list with the transformed dataset as a matrix as well as other useful information returned by the transformation function applied. It is possible to only return the transformed matrix, by setting return_matrix_only to TRUE. This can be useful to quickly assess the effects of the transformation outside of the analysis pipeline.\nFor example, we can apply the Variance Stabilising Transformation to the transcriptomics dataset:\n\ntar_load(mo_set_de)\n\nrnaseq_vst <- transform_dataset(\n  mo_set_de,\n  \"rnaseq\",\n  \"vst-deseq2\",\n  return_multidataset = FALSE\n)\n#> Applying Variance Stabilising Transformation (DESeq2) to rnaseq dataset.\n#> converting counts to integer mode\n\nThe function returns a list, with the transformed dataset as matrix in the transformed_data element. Information generated during the transformation by the DESeq2 package is stored in the info_transformation element. The name of the transformation applied is stored in the transformation element:\n\nnames(rnaseq_vst)\n#> [1] \"transformed_data\"    \"info_transformation\" \"transformation\"\n\nrnaseq_vst$transformed_data[1:5, 1:5]\n#>                        R9497     R5969     R5327     R5979     R9504\n#> ENSBTAG00000000005 10.357314 11.289047 12.070536 10.201227 10.251942\n#> ENSBTAG00000000008  4.836495  4.937948  4.375764  4.986154  5.710581\n#> ENSBTAG00000000009  3.486141  4.054636  4.983434  3.486141  3.486141\n#> ENSBTAG00000000010 12.216690 11.937084 12.079359 11.383360 11.897780\n#> ENSBTAG00000000011  3.486141  4.054636  4.005139  3.979732  3.486141\n\nrnaseq_vst$info_transformation\n#> class: DESeqTransform \n#> dim: 20335 143 \n#> metadata(1): version\n#> assays(1): ''\n#> rownames(20335): ENSBTAG00000000005 ENSBTAG00000000008 ...\n#>   ENSBTAG00000055312 ENSBTAG00000055314\n#> rowData names(6): baseMean baseVar ... dispGeneIter dispFit\n#> colnames(143): R9497 R5969 ... Y9747 Y9816\n#> colData names(1): sizeFactor\n\nrnaseq_vst$transformation\n#> [1] \"vst-deseq2\"\n\nIf we instead want to apply a log2 transformation to the dataset, we will use the best-normalize-manual transformation option instead, and we have to specify the transformation to use through the method argument; in our case, we will use the log_x method.\nThe bestNormalize::log_x() function uses by default a log base 10, but this can be changed by passing the b argument (the log base to use) to 2. We will also set a (the offset to use before log-transformation) to 0.5 – see the bestNormalize::log_x() function help for information about the parameters. By default, all bestNormalize functions standardise the transformed datasets, which we can prevent by setting standardize = FALSE:\n\nrnaseq_log2 <- transform_dataset(\n  mo_set_de,\n  \"rnaseq\",\n  \"best-normalize-manual\",\n  method = \"log_x\",\n  return_multidataset = TRUE, \n  ## Arguments passed to BestNormalize::log_x():\n  a = 0.5,\n  b = 2,\n  standardize = FALSE\n)\n#> Applying log_x transformation (bestNormalize) to rnaseq dataset.\n\nIn that case, we asked the function to return a MultiDataSet object, in which the rnaseq dataset has been transformed:\n\nrnaseq_log2\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 23036 features, 139 samples \n#>     . rnaseq: 20335 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 23036 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 20335 rows, 15 cols (feature_id, ..., de_signif)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nget_dataset_matrix(rnaseq_log2, \"rnaseq\")[1:5, 1:5]\n#>                        R9497      R5969      R5327      R5979     R9504\n#> ENSBTAG00000000005  9.518653 10.4589192 11.5115056  9.7690113  9.532356\n#> ENSBTAG00000000008  2.700440  2.9068906  1.8073549  3.3923174  4.357552\n#> ENSBTAG00000000009 -1.000000  0.5849625  3.2479275 -1.0000000 -1.000000\n#> ENSBTAG00000000010 11.395266 11.1114617 11.5203731 10.9661449 11.195680\n#> ENSBTAG00000000011 -1.000000  0.5849625  0.5849625  0.5849625 -1.000000\n\n\n6.1.2 Transformation target factory\nThe target factory function transformation_datasets_factory() provides a wrapper to apply (potentially different) transformations to several datasets at once. The function takes as input the MultiDataSet object as well as a named character vector, in which each element corresponds to a transformation that should be applied to a specific dataset. If a dataset is not present in the transformation vector, it will not be transformed (but it will still be present in the resulting MultiDataSet object).\nHere, we would like to apply Variance Stabilising Transformation to the transcriptomics dataset, and a log2 transformation to the metabolomics dataset. Note that the VST and VSN transformations are very close to the log2 transformation, especially for features with high means.\n\ntransformation_datasets_factory(\n  mo_set_de,\n  c(\"rnaseq\" = \"vst-deseq2\",\n    \"metabolome\" = \"best-normalize-manual\"),\n  methods = c(\"metabolome\" = \"log_x\"),\n  a = 0.01,\n  b = 2,\n  standardize = FALSE,\n  transformed_data_name = \"mo_set_transformed\"\n)\n\nThe transformation_datasets_factory() function works as follows:\n\nIt creates a grouped tibble listing the transformation to apply to each dataset, stored in the transformations_spec target;\n\n\ntar_read(transformations_spec)\n#> # A tibble: 2 × 4\n#>   dsn        transf                meth  tar_group\n#>   <chr>      <chr>                 <chr>     <int>\n#> 1 rnaseq     vst-deseq2            <NA>          2\n#> 2 metabolome best-normalize-manual log_x         1\n\n\nIt performs the required transformation on each dataset via dynamic branching. This is done through a call to the transform_dataset() function. The transformed datasets are stored in a list, in the transformations_runs_list target. Note that by default the function will store all details of the transformations, which can be useful for later inspection, but can be memory-intensive. It is possible to only store the transformed datasets instead, by setting the return_matrix_only argument to TRUE in the transformation_datasets_factory() call.\n\n\ntar_load(transformations_runs_list)\n\nnames(transformations_runs_list)\n#> [1] \"transformations_runs_list_7a466037\" \"transformations_runs_list_a1c8db41\"\n\nmap_chr(transformations_runs_list, attr, \"dataset_name\")\n#> transformations_runs_list_7a466037 transformations_runs_list_a1c8db41 \n#>                       \"metabolome\"                           \"rnaseq\"\n\ntransformations_runs_list[[\"transformations_runs_list_a1c8db41\"]] |> names()\n#> [1] \"transformed_data\"    \"info_transformation\" \"transformation\"\n\n\nIt creates a new MultiDataSet object, with the transformed version of the datasets. By default, this new MultiDataSet object is stored in a target called transformed_set, but a different name can be specified via the transformed_data_name argument (here we called it mo_set_transformed).\n\n\ntar_load(mo_set_transformed)\n\nmo_set_transformed\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 23036 features, 139 samples \n#>     . rnaseq: 20335 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 23036 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 20335 rows, 15 cols (feature_id, ..., de_signif)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nget_dataset_matrix(mo_set_de, \"metabolome\")[1:5, 1:3]\n#>             R21 Y3660 Y3243\n#> HMDB00001   9.1   9.7   9.3\n#> HMDB00008  58.2  22.8   9.1\n#> HMDB00042 403.0 392.0 606.0\n#> HMDB00043 172.6 163.1 165.2\n#> HMDB00060   0.7   1.5   1.4\n\nget_dataset_matrix(mo_set_de, \"metabolome\")[1:5, 1:3]\n#>             R21 Y3660 Y3243\n#> HMDB00001   9.1   9.7   9.3\n#> HMDB00008  58.2  22.8   9.1\n#> HMDB00042 403.0 392.0 606.0\n#> HMDB00043 172.6 163.1 165.2\n#> HMDB00060   0.7   1.5   1.4\n\nWe can assess the effect of the transformations by generating density and mean-sd plots for the transformed datasets:\n\nplot_density_data(\n  mo_set_transformed,\n  combined = FALSE,\n  scales = \"free\"\n)\n\n\n\n\nNote how the relationship between features mean and standard deviation has been reduced in both transformed datasets:\n\nplot_meansd_data(mo_set_transformed)\n\n\n\n\nFinally, it can be useful to summarise which transformations have been applied to the datasets, for example when creating a report. The function get_table_transformation() is here for that. It takes as an input the transformations_runs_list target generated by transformation_datasets_factory(), and returns a tibble indicating the transformation applied to each dataset:\n\nget_table_transformations(transformations_runs_list)\n#> # A tibble: 2 × 2\n#>   Dataset    Transformation                                                 \n#>   <chr>      <chr>                                                          \n#> 1 metabolome Non-Standardized Log_2(x + 0.01) transformation (bestNormalize)\n#> 2 rnaseq     Variance Stabilising Transformation (DESeq2)"
  },
  {
    "objectID": "preprocessing.html#running-a-pca-on-each-dataset",
    "href": "preprocessing.html#running-a-pca-on-each-dataset",
    "title": "6  Data pre-processing",
    "section": "\n6.2 Running a PCA on each dataset",
    "text": "6.2 Running a PCA on each dataset\nIt is always best practice to run some exploratory analysis on a dataset prior to running analyses. This is largely outside the scope of this package, and we assume that any input dataset has been properly assessed before turning to the integration pipeline. However, running a Principal Component Analysis (PCA) on each of the omics datasets within the integration pipeline serves two purposes:\n\nas a last check to ensure that there are no obvious batch effects or problematic samples that should be addressed,\nas a missing data imputation method.\n\nThe moiraine package relies on the Bioconductor pcaMethods package to perform the PCA. In particular, the pcaMethods package implements a NIPALS (non-linear iterative partial least squares) method for PCA, which allows for missing values in the input dataset, and imputes missing values based on the results of the PCA.\n\n6.2.1 Running the PCAs\nThe pca_complete_data_factory() function uses dynamic branching to perform a PCA on each omics dataset within a MultiDataSet object. It takes as input the MultiDataSet object (in our case, mo_set_transformed), and, optionally, the names of the datasets on which a PCA should be run. This is useful if one dataset is very large and has no missing values, and we want to avoid running a PCA on it. It then proceeds as follows:\n\nIt creates a target called dataset_names_pca, which stores a vector of dataset names on which a PCA should be applied;\nFor each value in dataset_names_pca, it extracts the omics dataset as a matrix with features as rows and samples as columns, using the get_dataset_matrix() function. This is done via dynamic branching, and the results are stored as a list in the pca_mats_list target. Note that the names of this list are not meaningful; to check which element of the list corresponds to which dataset, you can run map_chr(pca_mats_list, attr, \"dataset_name\");\nFor each matrix in pca_mats_list, it applies the run_pca_matrix() function to the corresponding dataset. This is done via dynamic branching; it results in a list where each element is the PCA result (i.e. a pcaMethods::pcaRes object) for a given dataset. This list is stored in the pca_runs_list target. Note that the names of this list are not meaningful; to check which element of the list corresponds to which dataset, you can run map_chr(pca_runs_list, attr, \"dataset_name\");\nIt extracts from the result of each PCA the complete dataset, i.e. with missing values imputed, and uses this information to construct a new MultiDataSet object, in which the datasets are complete (i.e. no missing value). This is done by calling the get_complete_data() function. If no PCA was run on a dataset, the dataset will still be present in the new MultiDataSet object, but its missing values will not be imputed. The resulting complete MultiDataSet object is stored by default in a target called complete_set; this name can be changed via the complete_data_name argument.\n\nLet’s apply this to our multi-omics dataset:\n\npca_complete_data_factory(\n  mo_set_transformed,\n  complete_data_name = \"mo_set_complete\"\n)\n\nWe can have a look at the different targets constructed. By default, a PCA was run on all datasets:\n\ntar_read(dataset_names_pca)\n#> [1] \"snps\"       \"rnaseq\"     \"metabolome\"\n\n\ntar_load(pca_mats_list)\n\nmap_chr(pca_mats_list, attr, \"dataset_name\")\n#> pca_mats_list_302d7473 pca_mats_list_84d36937 pca_mats_list_64d37e6c \n#>                 \"snps\"               \"rnaseq\"           \"metabolome\"\nmap(pca_mats_list, ~.x[1:5, 1:5])\n#> $pca_mats_list_302d7473\n#>                             R21 Y3660 Y3243 R5764 P4669\n#> 1_41768691                    1     0     2     2     1\n#> 10-27008241-A-C-rs42918694    2     2     2     1     2\n#> 10-37505419-T-C-rs136559242   0     1     0     2     0\n#> 10-49904259-G-A-rs471723345   1     2     2     2     2\n#> 1-109550832-G-A-rs209732846   2     2     1     2     2\n#> \n#> $pca_mats_list_84d36937\n#>                        R9497     R5969     R5327     R5979     R9504\n#> ENSBTAG00000000005 10.357314 11.289047 12.070536 10.201227 10.251942\n#> ENSBTAG00000000008  4.836495  4.937948  4.375764  4.986154  5.710581\n#> ENSBTAG00000000009  3.486141  4.054636  4.983434  3.486141  3.486141\n#> ENSBTAG00000000010 12.216690 11.937084 12.079359 11.383360 11.897780\n#> ENSBTAG00000000011  3.486141  4.054636  4.005139  3.979732  3.486141\n#> \n#> $pca_mats_list_64d37e6c\n#>                  R21     Y3660     Y3243    R5764     P4669\n#> HMDB00001  3.1874511 3.2794713 3.2187812       NA 3.6334312\n#> HMDB00008  5.8631951 4.5115945 3.1874511 2.657640 4.1626934\n#> HMDB00042  8.6546718 8.6147466 9.2431978 8.655388 7.9009272\n#> HMDB00043  7.4313722 7.3497014 7.3681572 7.707428 6.4027563\n#> HMDB00060 -0.4941091 0.5945485 0.4956952 2.908813 0.6870607\n\n\ntar_load(pca_runs_list)\n\nnames(pca_runs_list)\n#> [1] \"pca_runs_list_74d71ae8\" \"pca_runs_list_71fd94b4\" \"pca_runs_list_559272f0\"\n\nmap_chr(pca_runs_list, attr, \"dataset_name\")\n#> pca_runs_list_74d71ae8 pca_runs_list_71fd94b4 pca_runs_list_559272f0 \n#>                 \"snps\"               \"rnaseq\"           \"metabolome\"\n\nThe result of the PCA run on the genomics dataset looks like this:\n\ntar_read(pca_runs_list_74d71ae8)\n#> nipals calculated PCA\n#> Importance of component(s):\n#>                   PC1     PC2     PC3     PC4     PC5     PC6      PC7      PC8\n#> R2            0.05578 0.02881 0.01305 0.01196 0.01168 0.01005 0.009848 0.009565\n#> Cumulative R2 0.05578 0.08459 0.09765 0.10960 0.12128 0.13133 0.141175 0.150739\n#>                    PC9     PC10\n#> R2            0.009286 0.008915\n#> Cumulative R2 0.160026 0.168941\n#> 23036    Variables\n#> 139  Samples\n#> 9615     NAs ( 0.3 %)\n#> 10   Calculated component(s)\n#> Data was mean centered before running PCA \n#> Data was NOT scaled before running PCA \n#> Scores structure:\n#> [1] 139  10\n#> Loadings structure:\n#> [1] 23036    10\n\nYou can notice that there is some information about the number of principal components computed, and whether the dataset was centred and scaled before applying the PCA. This is handled by the default arguments of run_pca_matrix(), but can be specified by passing the corresponding arguments to pca_complete_data_factory(). For example, to scale the datasets before performing a PCA, we could use:\n\npca_complete_data_factory(\n  mo_set_transformed,\n  complete_data_name = \"mo_set_complete\",\n  scale = TRUE\n)\n\nFor convenience, the run_pca() function can be used to run a PCA on one of the omics datasets directly from a MultiDataSet object. It is a wrapper around the run_pca_matrix() function, and takes as input a MultiDataSet object as well as the name of the omics dataset on which a PCA should be run, e.g.:\n\nrun_pca(mo_set_de, \"rnaseq\")\n\n\n6.2.2 Visualising the PCA results\nIt is possible to get an overview of the results of each PCA. First, the function plot_screeplot_pca() displays the percentage of variance explained by the principal components computed for each dataset. It takes as input the pca_runs_list target constructed in the previous step. Note that by default, 10 components are computed for each dataset.\n\nplot_screeplot_pca(pca_runs_list)\n\n\n\n\nIn addition, the plot_samples_coordinates_pca allows us to display the samples in the reduced principal components space (the common PCA sample plot). The function returns a list of plots (one plot per dataset). By default, it shows all principal components computed for each dataset, but for clarity we will only look at the first three:\n\nplot_samples_coordinates_pca(\n  pca_runs_list,\n  pcs = 1:3\n)\n#> $snps\n\n\n\n#> \n#> $rnaseq\n\n\n\n#> \n#> $metabolome\n\n\n\n\nNote that it is possible to look at a different set of principal components for each dataset. For that, the index of the principal components should be passed to the pcs argument as a named list (where the name of each element corresponds to a dataset name), e.g.:\n\nplot_samples_coordinates_pca(\n  pca_runs_list,\n  pcs = list(\n    \"snps\" = 1:4,\n    \"rnaseq\" = 1:2,\n    \"metabolome\" = 1:3\n  )\n)\n\nBy default, the points in the sample plots are not coloured. It is however possible to colour the samples according to the information contained in the sample metadata tables available through the MultiDataset object. We can set different colours and shapes for the upper and lower plots in the scatterplot matrix, see the plot_samples_score() function for more information. For example, we can assess whether the first three principal components show any clustering of the samples according to their cluster computed from genomics similarity, disease status or feedlot (we’ll only show the results for the SNPs dataset here):\n\nplot_samples_coordinates_pca(\n  pca_runs_list,\n  datasets = \"snps\",\n  pcs = 1:3,\n  mo_data = mo_set_de,\n  colour_upper = \"geno_comp_cluster\",\n  shape_upper = \"status\",\n  colour_lower = \"feedlot\"\n) +\n  theme(legend.box = \"vertical\")\n\n\n\n\n\n6.2.3 Missing values imputation\nWe can check that the complete multi-omics set constructed has no more missing values:\n\ntar_load(mo_set_complete)\n\nmo_set_complete\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 23036 features, 139 samples \n#>     . rnaseq: 20335 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 23036 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 20335 rows, 15 cols (feature_id, ..., de_signif)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\n\ncheck_missing_values(mo_set_complete)\n#> No missing values in snps dataset.\n#> No missing values in rnaseq dataset.\n#> No missing values in metabolome dataset."
  },
  {
    "objectID": "preprocessing.html#recap-targets-list",
    "href": "preprocessing.html#recap-targets-list",
    "title": "6  Data pre-processing",
    "section": "\n6.3 Recap – targets list",
    "text": "6.3 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for datasets preprocessing\n\nlist(\n  ## Applying transformations to the datasets\n  transformation_datasets_factory(\n    mo_set_de,\n    c(\"rnaseq\" = \"vst-deseq2\",\n      \"metabolome\" = \"best-normalize-manual\"),\n    methods = c(\"metabolome\" = \"log_x\"),\n    a = 0.01,\n    b = 2,\n    standardize = FALSE,\n    transformed_data_name = \"mo_set_transformed\"\n  ),\n  \n  ## Density plot for each transformed dataset\n  tar_target(\n    density_plots_transformed,\n    plot_density_data(\n      mo_set_transformed,\n      combined = FALSE,\n      scales = \"free\"\n    )\n  ),\n  \n  ## Plotting the mean-SD trend for transformed each dataset\n  tar_target(\n    mean_sd_plots_transformed,\n    plot_meansd_data(mo_set_transformed)\n  ),\n  \n  ## Summary table of the transformations applied\n  tar_target(\n    transformation_summary,\n    get_table_transformations(transformations_runs_list)\n  ),\n  \n  ## Running a PCA on each dataset\n  pca_complete_data_factory(\n    mo_set_transformed,\n    complete_data_name = \"mo_set_complete\"\n  ),\n  \n  ## PCA screeplots\n  tar_target(\n    pca_screeplots,\n    plot_screeplot_pca(pca_runs_list)\n  ),\n  \n  ## PCA sample plots\n  tar_target(\n    pca_sample_plots,\n    plot_samples_coordinates_pca(\n      pca_runs_list,\n      datasets = \"snps\",\n      pcs = 1:3,\n      mo_data = mo_set_de,\n      colour_upper = \"geno_comp_cluster\",\n      shape_upper = \"status\",\n      colour_lower = \"feedlot\"\n    )\n  )\n)"
  },
  {
    "objectID": "prefiltering.html#subsetting-samples-of-interest",
    "href": "prefiltering.html#subsetting-samples-of-interest",
    "title": "7  Dataset pre-filtering",
    "section": "\n7.1 Subsetting samples of interest",
    "text": "7.1 Subsetting samples of interest\nIt is possible that for the integration analysis, we might want to retain only a subset of samples of interest; for example to focus on a specific phenotypic group. In this section, we will see a number of ways to subset samples of interest from a MultiDataSet object.\n\n7.1.1 Based on sample IDs\nThe MultiDataSet package allows to subset a MultiDataSet object based on a vector of sample IDs. For example, here we generate a list of 10 samples to which we would like to restrict the MultiDataSet object:\n\n## Randomly selecting 10 samples\nset.seed(47)\nsamples_list <- get_samples(mo_set_complete) |>\n  unlist() |>\n  unname() |>\n  unique() |>\n  sample(10, replace = FALSE)\n\nhead(samples_list)\n#> [1] \"U5523\" \"R8953\" \"R21\"   \"G3594\" \"R107\"  \"O5245\"\n\nWe can restrict the MultiDataSet object to only these samples as follows:\n\nmo_samples_filtered <- mo_set_complete[samples_list, ]\n\nn_samples(mo_samples_filtered)\n#>       snps     rnaseq metabolome \n#>          9         10         10\n\nThe MultiDataSet object returned only contains the 10 samples of interest. One of the selected samples was not present in the genomics dataset, which is why this dataset has only 9 samples.\n\n7.1.2 Based on metadata\nAlternatively, we might want to select samples based on some information contained in the samples metadata associated with the omics datasets. Again, this option is implemented in the MultiDataSet package through the subset() function (see their vignette for more information). For this example, we want to retain only animals from feedlot 1. This information is encoded in the feedlot column from the samples metadata of the datasets:\n\nget_samples_metadata_combined(mo_set_complete) |> str()\n#> 'data.frame':    144 obs. of  10 variables:\n#>  $ id               : chr  \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>  $ feedlot          : chr  \"F1\" \"F1\" \"F1\" \"F2\" ...\n#>  $ gender           : chr  \"female\" \"male\" \"male\" \"male\" ...\n#>  $ status           : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>  $ day_on_feed      : num  31 19 16 46 35 49 21 16 37 37 ...\n#>  $ rnaseq_batch     : chr  \"B2\" \"B2\" \"B2\" \"B1\" ...\n#>  $ geno_comp_1      : num  0.00785 0.85222 0.04417 0.21309 0.38939 ...\n#>  $ geno_comp_2      : num  0.8207 0.0936 0.1903 0.6763 0.5035 ...\n#>  $ geno_comp_3      : num  0.1715 0.0542 0.7656 0.1106 0.1071 ...\n#>  $ geno_comp_cluster: chr  \"K3\" \"K2\" \"K1\" \"K3\" ...\n\nIn the subset function, the first argument is the MultiDataSet object to subset, the second argument slot is for subsetting features based on their metadata (which we will see in the next section), and the third slot is for samples subsetting. We perform the subsetting by passing an expression, similar to what we would use with the dplyr::filter() function, i.e. we treat the column name on which to perform the subsetting as a variable name.\n\nmo_samples_filtered <- subset(mo_set_complete, , feedlot == \"F1\")\n\nn_samples(mo_samples_filtered)\n#>       snps     rnaseq metabolome \n#>         22         23         23\n\nThe MultiDataSet object returned contains only samples corresponding to animals from feedlot 1.\n\n7.1.3 Retaining common samples\nMost data integration tools only accept samples that are present in all omics datasets. When that is the case, the moiraine package will automatically remove samples that are absent from some datasets when preparing the input data for the corresponding integration tool. However, for convenience, we show here how to restrict a MultiDataSet object to only samples that are common to all datasets.\nThis is done through the commonSamples() function from the MultiDataSet package, which returns a filtered MultiDataSet object:\n\nmo_samples_filtered <- commonSamples(mo_set_complete)\n\nn_samples(mo_samples_filtered)\n#>       snps     rnaseq metabolome \n#>        135        135        135\n\nThe returned MultiDataSet object contains 135 samples which are present in all three omics datasets (which we can confirm with the Upset plot generated in Section 4.3)."
  },
  {
    "objectID": "prefiltering.html#subsetting-features-of-interest",
    "href": "prefiltering.html#subsetting-features-of-interest",
    "title": "7  Dataset pre-filtering",
    "section": "\n7.2 Subsetting features of interest",
    "text": "7.2 Subsetting features of interest\n\n7.2.1 Based on feature IDs\nAs with samples, we might want to filter a MultiDataSet object to only specific features of interest. We will randomly select feature IDs from each omics dataset:\n\nset.seed(36)\nfeatures_list <- get_features(mo_set_complete) |>\n  map(\\(x) sample(x, size = 5, replace = FALSE))\n\nstr(features_list)\n#> List of 3\n#>  $ snps      : chr [1:5] \"BovineHD0600012019\" \"BovineHD1200015003\" \"BTB-01046082\" \"BovineHD2600010539\" ...\n#>  $ rnaseq    : chr [1:5] \"ENSBTAG00000002154\" \"ENSBTAG00000009915\" \"ENSBTAG00000052111\" \"ENSBTAG00000012274\" ...\n#>  $ metabolome: chr [1:5] \"HMDB00201\" \"HMDB00641\" \"HMDB00123\" \"HMDB00294\" ...\n\nThe subset() method implemented in the MultiDataSet package can be used to restrict the omics datasets to specific features based on a list of IDs. However, this only works by directly passing the features ID in the command, as follows:\n\nmo_features_filtered <- subset(\n  mo_set_complete, \n  feature_id %in% c(\"BovineHD0600012019\", \"ENSBTAG00000002154\")\n)\n\nn_features(mo_features_filtered)\n#>       snps     rnaseq metabolome \n#>          1          1          0\n\nWhile passing a vector of feature IDs doesn’t work:\n\nfeatures_vec <- c(\"BovineHD0600012019\", \"ENSBTAG00000002154\")\nsubset(mo_set_complete, feature_id %in% features_vec)\n#> Error in .local(x, ...): feat expression could not be applied to any of the sets.\n\nThis type of subsetting is made possible with the subset_features() function in moiraine:\n\nmo_features_filtered <- subset_features(mo_set_complete, features_vec)\n\nn_features(mo_features_filtered)\n#>       snps     rnaseq metabolome \n#>          1          1          0\n\nThe subset_features() function accepts the features ID either as a vector, or as a list of vectors (typically one per dataset):\n\nmo_features_filtered <- subset_features(mo_set_complete, features_list)\nn_features(mo_features_filtered)\n#>       snps     rnaseq metabolome \n#>          5          5          5\n\n## Getting the selected IDs as a vector\nfeatures_vec <- features_list |>\n  unlist() |>\n  unname()\n\nhead(features_vec)\n#> [1] \"BovineHD0600012019\" \"BovineHD1200015003\" \"BTB-01046082\"      \n#> [4] \"BovineHD2600010539\" \"BovineHD2600008282\" \"ENSBTAG00000002154\"\n\nmo_features_filtered <- subset_features(mo_set_complete, features_vec)\nn_features(mo_features_filtered)\n#>       snps     rnaseq metabolome \n#>          5          5          5\n\n\n7.2.2 Based on metadata\nIt is also possible to subset features based on their metadata. For that, we can use the subset() function from the MultiDataSet package, as we did for samples subsetting. For example, for the transcriptomics and metabolomics dataset, we have in the features metadata a column (de_signif) that recaps the results of a differential abundance analysis on the corresponding dataset. We could decide to select only the differentially abundant compounds from this dataset. Note that it only performs the filtering for datasets that have this column in their features metadata.\n\nmo_features_filtered <- subset(mo_set_complete, de_signif == \"DE\")\n#> Warning in .local(x, ...): The following sets could not be filtered by feature\n#> id: snps\n\nn_features(mo_features_filtered)\n#>       snps     rnaseq metabolome \n#>      23036        111         30"
  },
  {
    "objectID": "prefiltering.html#features-preselection",
    "href": "prefiltering.html#features-preselection",
    "title": "7  Dataset pre-filtering",
    "section": "\n7.3 Features preselection",
    "text": "7.3 Features preselection\nIn the previous section, we saw how to restrict the MultiDataSet object to a set of features of interest. However, in a typical integration workflow, we instead want to reduce the dimensions of the omics datasets through a data-centric method that discards features least relevant to the biological problem of interest. Here, we present two approaches for features preselection: an unsupervised approach, which relies only on the omics measurements, and a supervised approach, which accounts for information we have about the samples. The choice between these two approaches will depend on the research question being investigated. Note that this preselection step is distinct from the data cleaning process that should be applied to each omics dataset, in which features with low expression or high missing values are removed. This ideally should be done before the multi-omics integration workflow constructed with moiraine, although it can be integrated in the analysis pipeline.\n\n7.3.1 Unsupervised features preselection\nIn order to reduce the number of features in the omics datasets, one option is to only retain the most variable features from each dataset. We refer to this approach as unsupervised preselection, as it only relies on the omics measurements to discard irrelevant features. In the package, two metrics of feature variability are implemented: the coefficient of variation (COV), and the Median Absolute Deviation (MAD). Careful consideration is required when determining which of these metrics should be used to select the most variable features, as each has some drawbacks. In particular:\n\nFiltering based on COV will retain features that are only present in very few samples. This might be problematic for noisy datasets in which some features are technical artefacts, or if we are looking for biomarkers that are expressed across all observations.\nFiltering based on MAD will discard any feature that is absent in more than half of the observations. This might be problematic if for example we are comparing two groups with unbalanced size, and we are looking for group-specific biomarkers.\n\nTherefore, a first step of data cleaning to remove artefact features, as well as consideration of the biological research question, is needed before proceeding.\nThe feature_preselection_cov_factory() and feature_preselection_mad_factory() functions allow us to perform unsupervised COV- or MAD-based preselection for some or all datasets within a MultiDataSet object. It provides two options to set the desired size of the filtered datasets: we can either specify the number of features to retain in each dataset (via the to_keep_ns argument), or the proportion of features that should be kept in each dataset (via the to_keep_props argument). For example, let’s say that we want to retain 1,000 features with the highest MAD score in both the genomics and transcriptomics datasets (as the metabolomics dataset contains only 55 compounds, no preselection will be applied to it):\n\nfeature_preselection_mad_factory(\n  mo_samples_complete,\n  to_keep_ns = c(\"snps\" = 1000, \"rnaseq\" = 1000),\n  with_ties = TRUE,\n  filtered_set_target_name = \"mo_presel_unsupervised\"\n)\n\nThe feature_preselection_mad_factory works as follows:\n\nit creates a grouped tibble in which each row is one of the datasets to be filtered, with the number or proportion of features to retain. It is stored in the mad_spec target:\n\n\ntar_read(mad_spec)\n#> # A tibble: 2 × 4\n#>   dsn      tkn wt    tar_group\n#>   <chr>  <dbl> <lgl>     <int>\n#> 1 snps    1000 TRUE          2\n#> 2 rnaseq  1000 TRUE          1\n\n\nit uses dynamic branching over the grouped tibble to extract each omics dataset as a matrix via the get_dataset_matrix() function. The result of this target, called mad_mat, is a list where each element is a matrix of omics measurements. The names of this list are specific to the dynamic branching, but the name of the omics dataset to which each matrix belongs is stored in their 'dataset_name' attribute:\n\n\ntar_load(mad_mat)\n\nmap_chr(mad_mat, attr, \"dataset_name\")\n#> mad_mat_e428ef62 mad_mat_d5a0e3cc \n#>         \"rnaseq\"           \"snps\"\nmap(mad_mat, \\(x) x[1:5, 1:5])\n#> $mad_mat_e428ef62\n#>                        R9497     R5969     R5327     R5979     R9504\n#> ENSBTAG00000000005 10.357314 11.289047 12.070536 10.201227 10.251942\n#> ENSBTAG00000000008  4.836495  4.937948  4.375764  4.986154  5.710581\n#> ENSBTAG00000000009  3.486141  4.054636  4.983434  3.486141  3.486141\n#> ENSBTAG00000000010 12.216690 11.937084 12.079359 11.383360 11.897780\n#> ENSBTAG00000000011  3.486141  4.054636  4.005139  3.979732  3.486141\n#> \n#> $mad_mat_d5a0e3cc\n#>                             R21 Y3660 Y3243 R5764 P4669\n#> 1_41768691                    1     0     2     2     1\n#> 10-27008241-A-C-rs42918694    2     2     2     1     2\n#> 10-37505419-T-C-rs136559242   0     1     0     2     0\n#> 10-49904259-G-A-rs471723345   1     2     2     2     2\n#> 1-109550832-G-A-rs209732846   2     2     1     2     2\n\n\nit uses dynamic branching over the list of matrices to perform the prefiltering for each dataset, by calling the select_features_mad_matrix() function. The function computes the MAD coefficient of each feature, then selects the features with the highest absolute MAD values. The with_ties argument determines whether more features than requested by to_keep_ns or to_keep_props should be kept if several features at the limit of selection have identical MAD values. The select_features_mad_matrix() function returns a tibble with the MAD coefficient of each feature in the dataset, as well as an indicator of whether the feature was retained or not. This is useful to produce some diagnostic plots, for example with the plot_feature_preselection_mad() function. The results of the prefiltering are stored as a list in the target called individual_mad_values.\n\n\ntar_load(individual_mad_values)\n\nmap_chr(individual_mad_values, attr, \"dataset_name\")\n#> individual_mad_values_0bdea42a individual_mad_values_19c0eba7 \n#>                       \"rnaseq\"                         \"snps\"\n\nmap(individual_mad_values, head, 3)\n#> $individual_mad_values_0bdea42a\n#> # A tibble: 3 × 3\n#>   feature_id           mad selected\n#>   <chr>              <dbl> <lgl>   \n#> 1 ENSBTAG00000049569  3.55 TRUE    \n#> 2 ENSBTAG00000048835  3.53 TRUE    \n#> 3 ENSBTAG00000051412  3.48 TRUE    \n#> \n#> $individual_mad_values_19c0eba7\n#> # A tibble: 3 × 3\n#>   feature_id                    mad selected\n#>   <chr>                       <dbl> <lgl>   \n#> 1 1_41768691                   1.48 TRUE    \n#> 2 10-27008241-A-C-rs42918694   1.48 TRUE    \n#> 3 10-37505419-T-C-rs136559242  1.48 TRUE\n\n\nIt creates a new MultiDataSet object in which the relevant datasets have been filtered to only contain the selected features, via get_filtered_dataset_variability(). By default, the target used to store this object is called filtered_set_mad, but this can be changed via the filtered_set_target_name argument (here we called it mo_presel_unsupervised instead).\n\n\ntar_read(mo_presel_unsupervised)\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 12618 features, 139 samples \n#>     . rnaseq: 1000 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 12618 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 1000 rows, 15 cols (feature_id, ..., de_signif)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nThe plot_feature_preselection_mad() function can be used to visualise the distribution of MAD values across each (non-filtered) dataset and the minimum MAD value retained in the filtered datasets:\n\nplot_feature_preselection_mad(individual_mad_values)\n\n\n\n\nIf we instead wanted to retain 50% of all features both the genomics and transcriptomics datasets, we would write:\n\nfeature_preselection_mad_factory(\n  mo_samples_complete,\n  to_keep_props = c(\"rnaseq\" = 0.5, \"metabolome\" = 0.5),\n  with_ties = TRUE,\n  filtered_set_target_name = \"mo_presel_unsupervised\"\n)\n\nNote that the feature_preselection_cov_factory() function works in exactly the same way, but calls the select_features_cov_matrix() function, and the results can be visualised with the plot_feature_preselection_cov() function.\nFor convenience, the select_features_mad() and select_features_cov() functions can be used to perform a MAD- or COV-based prefiltering on one of the omics datasets directly from a MultiDataSet object. These are wrappers around the select_features_mad_matrix() and select_features_cov_matrix() functions, respectively, and take as input a MultiDataSet object as well as the name of the omics dataset on which the preselection should be run, as well as either the number or proportion of features to retain, e.g.:\n\nselect_features_mad(mo_set_complete, \"rnaseq\", to_keep_n = 1000)\nselect_features_mad(mo_set_complete, \"rnaseq\", to_keep_prop = 0.5)\n\n\n7.3.2 Supervised features preselection\nAnother approach to features preselection can be preferred when we are trying to assess the features most relevant to an outcome of interest or to differences between sample groups. In this scenario, prior to integrating the datasets, it could be useful to reduce the size of the datasets by filtering out the features that are least associated with the outcome of interest. In this case, we can use some single-omics feature selection method to perform a first “crude” prefiltering.\nmoiraine relies on the sPLS-DA algorithm implemented in the mixOmics package for this. sparse Partial Least-Squares Discriminant Analysis (or sPLS-DA for short) is a feature selection method that aims to detect, in a multivariate dataset, the variables or features that best discriminate a categorical outcome of interest in the observations. The advantages of sPLS-DA is that it can handle datasets in which there are more features than samples, which is typically the case in omics datasets. More information can be found in Lê Cao, Boitard, and Besse (2011) or in the mixOmics vignette. By running an sPLS-DA analysis on each dataset separately, we can remove the features that are least informative with respect to the trait or outcome of interest. We refer to this approach as supervised preselection, as it relies on information about the samples to select the features of interest.\nThe feature_preselection_splsda_factory() function allows us to perform this supervised preselection for some or all datasets within a MultiDataSet object. It provides the option to set either the number or proportion of features to retain in each dataset, via the to_keep_ns and to_keep_props arguments. One additional argument that needs to be passed to the function is group, which gives the name of the column in the samples metadata information to be used as categorical outcome for the sPLS-DA run. This column must be present in the sample metadata of at least one of the datasets to be filtered. For this example, we will retain in each dataset 1,000 features that best discriminate the control and diseased animals. Warning: this function can take several minutes to run.\n\nfeature_preselection_splsda_factory(\n  mo_samples_complete,\n  group = \"status\",\n  to_keep_ns = c(\"snps\" = 1000, \"rnaseq\" = 1000),\n  filtered_set_target_name = \"mo_presel_supervised\"\n)\n\nThe function works as follows:\n\nit creates a grouped tibble in which each row is one of the datasets to be filtered, with the number or proportion of features to retain. It is stored in the splda_spec target:\n\n\ntar_read(splsda_spec)\n#> # A tibble: 2 × 3\n#>   dsn      tkn tar_group\n#>   <chr>  <dbl>     <int>\n#> 1 snps    1000         2\n#> 2 rnaseq  1000         1\n\n\nit uses dynamic branching over the grouped tibble to generate for each omics dataset the necessary input for the mixOmics package, via the get_input_splsda() function. The result, stored in the individual_splsda_input target, is a list of sPLS-DA inputs, i.e. a list itself containing the omics dataset as a matrix (with samples as rows and features as columns) and a vector indicating the grouping of the samples:\n\n\ntar_load(individual_splsda_input)\n\nmap(individual_splsda_input, names)\n#> $individual_splsda_input_a82e0f4d\n#> [1] \"rnaseq\" \"Y\"     \n#> \n#> $individual_splsda_input_de775b91\n#> [1] \"snps\" \"Y\"\nmap(individual_splsda_input, \\(x) head(x[[\"Y\"]]))\n#> $individual_splsda_input_a82e0f4d\n#> R9497 R5969 R5327 R5979 R9504 R5994 \n#>   BRD   BRD   BRD   BRD   BRD   BRD \n#> Levels: BRD Control\n#> \n#> $individual_splsda_input_de775b91\n#>     R21   Y3660   Y3243   R5764   P4669   R5452 \n#> Control Control Control Control     BRD Control \n#> Levels: BRD Control\n\n\nit uses dynamic branching to run a performance cross-validation analysis on each dataset, via the function perf_splsda() (which is essentially a wrapper for the mixOmics::perf() function). This cross-validation step selects the optimal number of components to compute for each dataset during the sPLS-DA analysis. The results of this cross-validation step are saved in a list, stored in the individual_splsda_perf target. It can take a bit of time (for this example, around 6 minutes per dataset).\n\n\ntar_load(individual_splsda_perf)\n\nmap_chr(individual_splsda_perf, attr, \"dataset_name\")\n#> individual_splsda_perf_68d22d74 individual_splsda_perf_746586a5 \n#>                        \"rnaseq\"                          \"snps\"\n\nThe cross-validation results be visualised with the plot_feature_preselection_splsda() function, in which the chosen value for the number of components to compute for each dataset is highlighted in grey:\n\nplot_feature_preselection_splsda(individual_splsda_perf)\n\n\n\n\n\nit uses dynamic branching to run sPLS-DA on each dataset, via the run_splsda() function. The results are stored as a list in the individual_splsda_run target.\n\n\nmap_chr(tar_read(individual_splsda_run), attr, \"dataset_name\")\n#> individual_splsda_run_fe6f99fe individual_splsda_run_4a7ecf19 \n#>                       \"rnaseq\"                         \"snps\"\n\n\nit creates a new MultiDataSet object in which the relevant datasets have been filtered to only contain the selected features. By default, the target used to store this object is called filtered_set_mad, but this can be changed via the filtered_set_target_name argument.\n\n\ntar_read(mo_presel_supervised)\n#> Object of class 'MultiDataSet'\n#>  . assayData: 3 elements\n#>     . snps: 1000 features, 139 samples \n#>     . rnaseq: 994 features, 143 samples \n#>     . metabolome: 55 features, 139 samples \n#>  . featureData:\n#>     . snps: 1000 rows, 13 cols (feature_id, ..., p_value)\n#>     . rnaseq: 994 rows, 15 cols (feature_id, ..., de_signif)\n#>     . metabolome: 55 rows, 16 cols (feature_id, ..., de_signif)\n#>  . rowRanges:\n#>     . snps: YES\n#>     . rnaseq: YES\n#>     . metabolome: NO\n#>  . phenoData:\n#>     . snps: 139 samples, 10 cols (id, ..., geno_comp_3)\n#>     . rnaseq: 143 samples, 10 cols (id, ..., geno_comp_3)\n#>     . metabolome: 139 samples, 10 cols (id, ..., geno_comp_3)\n\nYou can notice that with this approach, we are not guaranteed to retain exactly 1,000 features per dataset. This is because, in an sPLS-DA run, a same feature can be selected for more than one latent component, and so the number of features retained will be slightly smaller than the one requested."
  },
  {
    "objectID": "prefiltering.html#recap-targets-list",
    "href": "prefiltering.html#recap-targets-list",
    "title": "7  Dataset pre-filtering",
    "section": "\n7.4 Recap – targets list",
    "text": "7.4 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for datasets prefiltering\n\nlist(\n  ## Unsupervised feature selection based on MAD score\n  feature_preselection_mad_factory(\n    mo_set_complete,\n    to_keep_ns = c(\"snps\" = 1000, \"rnaseq\" = 1000),\n    with_ties = TRUE,\n    filtered_set_target_name = \"mo_presel_unsupervised\"\n  ),\n  \n  ## Diagnostic plot for MAD-based feature selection\n  tar_target(\n    preselection_mad_plot,\n    plot_feature_preselection_mad(individual_mad_values)\n  ),\n  \n  ## Supervised feature selection based on bruising groups\n  feature_preselection_splsda_factory(\n    mo_set_complete,\n    group = \"status\",\n    to_keep_ns = c(\"snps\" = 1000, \"rnaseq\" = 1000),\n    filtered_set_target_name = \"mo_presel_supervised\"\n  ),\n  \n  ## Diagnostic plot for sPLS-DA based feature selection\n  tar_target(\n    preselection_splsda_plot,\n    plot_feature_preselection_splsda(individual_splsda_perf)\n  )\n)\n\n\n\n\n\nLê Cao, Kim-Anh, Simon Boitard, and Philippe Besse. 2011. “Sparse PLS Discriminant Analysis: Biologically Relevant Feature Selection and Graphical Displays for Multiclass Problems.” BMC Bioinformatics 12 (1): 253. https://doi.org/10.1186/1471-2105-12-253."
  },
  {
    "objectID": "spls.html#what-is-spls",
    "href": "spls.html#what-is-spls",
    "title": "8  Integration with sPLS",
    "section": "\n8.1 What is sPLS?",
    "text": "8.1 What is sPLS?\nsPLS (for sparse Partial Least Squares) is a method for the unsupervised integration of two omics datasets. Given two omics datasets for which measurements are taken on the same samples, sPLS aims at investigating the shared variation between the datasets, and the selection of correlated features driving this shared variation.\nsPLS works by constructing linear combinations of the omics features, called latent components. These latent components are constructed such that the covariance between each omics dataset’ component is maximised. They are constructed iteratively, and different matrix deflation modes can be used after the construction of each component. The selected mode defines the role that the two datasets play in the analysis, i.e. whether there is a predictor and a response dataset, or whether the two datasets play symmetrical roles. In addition, \\(L1\\)-regularisation (also called LASSO) is applied to the latent components in order to select the most important features from each dataset. Note that within the mixOmics package, the (s)PLS algorithm has two types: the univariate type denoted PLS1, which refers to the case where one of the datasets contains a single feature or variable, and the multivariate type, PLS2, which is for the integration of two datasets each with multiple features. This chapter is focused on the PLS2 type.\nsPLS requires as input matrices of omics measurements obtained on the same samples. While the omics datasets are automatically centred and scaled by the algorithm, proper preprocessing and normalisation is assumed to be carried out by the user. Although the sPLS algorithm can handle the presence of missing features, it will prohibit the use of cross-validation, so it is recommended to perform data imputation prior to running an sPLS analysis."
  },
  {
    "objectID": "spls.html#creating-the-spls-input",
    "href": "spls.html#creating-the-spls-input",
    "title": "8  Integration with sPLS",
    "section": "\n8.2 Creating the sPLS input",
    "text": "8.2 Creating the sPLS input\nThe first step is to transform the MultiDataSet object into a suitable format for the mixOmics package. This can be done through the get_input_spls() function. The function restricts the datasets to only common samples, and returns a named list, where each element is an omics dataset matrix with samples as rows and features as columns. The names of the two datasets to analyse are passed on through the datasets argument. In addition, the analysis mode must be specified through the mode argument, and optionally the multilevel option to handle repeated measurements can be set through the multilevel argument. Both concepts are explained below.\n\n8.2.1 The four sPLS modes\nThere are four possible modes that can be used for an sPLS analysis. These modes define the role that each of the datasets play, and differ by the type of matrix deflation used in the analysis. These four modes are:\n\nThe regression mode (mode = \"regression\"): the first dataset (first name in datasets argument) is considered as the ‘predictor’ dataset, and the second as the ‘response’ dataset. With this mode, the order in which the datasets appear in the datasets argument matters.\nThe canonical mode (mode = \"canonical\"): the two datasets play a symmetrical role, and the order in which they appear in the datasets argument does not matter. This mode is used when there is no a priori relationship between the two datasets. This is similar to a canonical correlation analysis (CCA).\nThe invariant mode (mode = \"invariant\"): similar to the regression mode with the first dataset being considered as the predictor dataset, and the second as the response dataset. However no matrix deflation is performed, which allows to perform a redundancy analysis. Again, the order in which the datasets appear in the datasets argument matters.\nThe classic mode (mode = \"classic\"): similar to the regression mode with the first dataset being considered as the predictor dataset, and the second as the response dataset; however this mode uses a different normalisation method for the loading vectors in accordance to another version of the sPLS method. Again, the order in which the datasets appear in the datasets argument matters.\n\nThe difference between these four modes lies in the type of matrix deflation used after computing each latent component. Therefore, the first latent components constructed will be identical between modes, but the subsequent ones will differ.\n\n8.2.2 The multilevel option\nThe mixOmics package includes an option to deal with repeated measurements (see the mixOmics multilevel article). This corresponds to the case where several samples in the omics datasets come from the same biological individual (patient, animal, plant) that has been measured several times (e.g. before and after a treatment, or across several body sites). In such case, it is possible that the measurements obtained are more similar within an individual than within a treatment group, which reduces our ability to detect differences between the treatment groups.\nTo mitigate this effect, the mixOmics package can decompose the datasets to extract their within-individual variation part, which is then used as input for the sPLS run (Liquet et al. (2012) ). As noted by the authors of the package, this methodology is applicable to balanced as well as unbalanced designs. Before selecting this option, is it recommended to explore the datasets with some unsupervised dimension reduction method (e.g. a PCA), in order to assess whether the between-individual variation should be removed to enable the detection of any between-treatment effect.\nThe multilevel option can accommodate one- or two-factor decomposition, which refers to the number of treatment covariates whose effect we are studying. For example, if we have repeated measurements on the same subjects for three time-points after the application of a treatment, we will use a one-factor decomposition with time being the factor of interest. If instead we have repeated measurements on the same subjects for three time-points after the application of two different treatments (so 6 measurements for a same subject), we will use a two-factor decomposition with time and treatment as the factors of interest.\nWith the get_input_spls() function, in order to use the multilevel option with one factor, the multilevel argument should be populated with the name of the column in the samples metadata of the omics datasets containing the ID of the biological individuals. For example, if we have measured different plants once after the application of two different treatments, the samples metadata table might look like this:\n\n\n\n\n\nid\nplant_id\ntreatment\n\n\n\nsample_1\nsample_1\nplant_1\nA\n\n\nsample_2\nsample_2\nplant_1\nB\n\n\nsample_3\nsample_3\nplant_2\nA\n\n\nsample_4\nsample_4\nplant_2\nB\n\n\nsample_5\nsample_5\nplant_3\nA\n\n\nsample_6\nsample_6\nplant_3\nB\n\n\nsample_7\nsample_7\nplant_4\nA\n\n\nsample_8\nsample_8\nplant_4\nB\n\n\nsample_9\nsample_9\nplant_5\nA\n\n\nsample_10\nsample_10\nplant_5\nB\n\n\n\n\n\nwhere the id column contains the unique ID for each observation (these are the IDs present in the omics datasets), plant_id records the ID of the plant measured (our biological individuals) on which repeated measurements have been taken, and treatment contains the treatment that has been applied before each observation. In that case, we will set multilevel = \"plant_id\" to use the multilevel option:\n\nspls_input_multilevel1 <- get_input_spls(\n  mo_set_multilevel1, \n  mode = \"canonical\",\n  datasets = c(\"rnaseq\", \"metabolome\"),\n  multilevel = \"plant_id\"\n)\n\nThe resulting design table that will be used by the mixOmics package is stored in the multilevel argument of the object returned by the get_input_spls() function. It consists of a data-frame with observations as rows, and an integer ID for the corresponding biological individual in the first column:\n\nattr(spls_input_multilevel1, \"multilevel\")\n\n\n#>           plant_id\n#> sample_1         1\n#> sample_2         1\n#> sample_3         2\n#> sample_4         2\n#> sample_5         3\n#> sample_6         3\n#> sample_7         4\n#> sample_8         4\n#> sample_9         5\n#> sample_10        5\n\nIn order to use the multilevel option with two factors, the multilevel argument should instead be populated with a vector of three column names from the samples metadata table. The first value, similarly to a one-factor decomposition, should be the name of the column containing the ID of the biological individuals. The second and third values should be the name of the columns containing the levels of the two factors of interest. Continuing on the previous example, supposing that in a similar study, we take measurements on the plants at three different time-points after the application of each treatment. The samples metadata table might look like this:\n\n\n\n\n\nid\nplant_id\ntreatment\ntime\n\n\n\nsample_1\nsample_1\nplant_1\nA\n1\n\n\nsample_2\nsample_2\nplant_1\nA\n2\n\n\nsample_3\nsample_3\nplant_1\nA\n3\n\n\nsample_4\nsample_4\nplant_1\nB\n1\n\n\nsample_5\nsample_5\nplant_1\nB\n2\n\n\nsample_6\nsample_6\nplant_1\nB\n3\n\n\nsample_7\nsample_7\nplant_2\nA\n1\n\n\nsample_8\nsample_8\nplant_2\nA\n2\n\n\nsample_9\nsample_9\nplant_2\nA\n3\n\n\nsample_10\nsample_10\nplant_2\nB\n1\n\n\nsample_11\nsample_11\nplant_2\nB\n2\n\n\nsample_12\nsample_12\nplant_2\nB\n3\n\n\n\n\n\nwith similar columns to the previous example, and an additional time column indicating the time-point at which the measurement was taken. In that case, we should set multilevel = c(\"plant_id\", \"treatment\", \"time\") to use the multilevel option, as follows:\n\nspls_input_multilevel2 <- get_input_spls(\n  mo_set_multilevel2, \n  mode = \"canonical\",\n  datasets = c(\"rnaseq\", \"metabolome\"),\n  multilevel = c(\"plant_id\", \"treatment\", \"time\")\n)\n\nAs for the one-factor example, the resulting design table that will be used by the mixOmics package is stored in the multilevel argument of the object returned by the get_input_spls() function. It consists of a data-frame with observations as rows, and an integer ID for the corresponding biological individual in the first column, and the two factors with their original levels in the following columns:\n\nattr(spls_input_multilevel2, \"multilevel\")\n\n\n#>           plant_id treatment time\n#> sample_1         1         A    1\n#> sample_2         1         A    2\n#> sample_3         1         A    3\n#> sample_4         1         B    1\n#> sample_5         1         B    2\n#> sample_6         1         B    3\n#> sample_7         2         A    1\n#> sample_8         2         A    2\n#> sample_9         2         A    3\n#> sample_10        2         B    1\n#> sample_11        2         B    2\n#> sample_12        2         B    3\n\n\n8.2.3 The sPLS input object\nFor our example dataset, we will integrate the transcriptomics and metabolomics datasets using the canonical mode (since we don’t have any a priori on the relationship between the two datasets). In this case we do not have a repeated measurements design.\n\ntar_target(\n  spls_input,\n  get_input_spls(\n    mo_presel_supervised,\n    mode = \"canonical\",\n    datasets = c(\"rnaseq\", \"metabolome\")\n  )\n)\n\nThe result of the function is a named list of length 2, where each element is a matrix of omics measurements, which have been restricted to samples that are common to the two datasets. The sPLS mode to be used is stored in the mode attribute of the object.\n\ntar_read(spls_input) |> str()\n#> List of 2\n#>  $ rnaseq    : num [1:139, 1:994] 3.49 3.49 4.01 3.49 4.25 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:139] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>   .. ..$ : chr [1:994] \"ENSBTAG00000000020\" \"ENSBTAG00000000046\" \"ENSBTAG00000000056\" \"ENSBTAG00000000061\" ...\n#>   ..- attr(*, \"datatype\")= chr \"ExpressionSet\"\n#>   .. ..- attr(*, \"package\")= chr \"Biobase\"\n#>  $ metabolome: num [1:139, 1:55] 3.29 3.39 3.64 3.32 3.63 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:139] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>   .. ..$ : chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n#>   ..- attr(*, \"datatype\")= chr \"MetabolomeSet\"\n#>   .. ..- attr(*, \"package\")= chr \"moiraine\"\n#>  - attr(*, \"mode\")= chr \"canonical\""
  },
  {
    "objectID": "spls.html#choosing-the-number-of-latent-components",
    "href": "spls.html#choosing-the-number-of-latent-components",
    "title": "8  Integration with sPLS",
    "section": "\n8.3 Choosing the number of latent components",
    "text": "8.3 Choosing the number of latent components\nThe first step when running an sPLS analysis is to determine the number of latent components to construct, which can be chosen using cross-validation. This is done by running an initial sPLS analysis on the datasets with no feature selection and a large number of latent components; for example we want to test up to four latent components. We use for this the spls_run() function, which is a wrapper for the mixOmics::spls() function.\n\ntar_target(\n  spls_novarsel,\n  spls_run(\n    spls_input,\n    ncomp = 4\n  )\n)\n\nWe then apply the mixOmics::perf() function on the result of this initial run. There are several parameters to set:\n\nvalidation: the type of cross-validation to perform, M-fold (\"Mfold\") or leave-one-out (\"loo\"). We recommend to use M-fold validation, except when the number of samples is very small.\nfolds: for M-fold cross-validation, the number of folds to construct, i.e. the number of groups in which to split the samples. Each group in turn will be considered as test set while the remaining groups are used to make the training set. The value to use depends on the number of samples in the datasets. By default, 10 is a reasonable number. For leave-one-out cross-validation, this parameter is set to the number of samples (that is the principle of leave-one-out cross-validation).\nnrepeat: the number of times the cross-validation will be repeated. This is important for M-fold cross-validation, as the way the samples are split into groups affects the results. Therefore, by repeating the cross-validation scheme we are averaging the results over different splits, thus reducing the impact of samples splitting. We recommend at least 10 repeats. Irrelevant for leave-one-out cross-validation, so can be left to 1.\ncpus: number of CPUs to use for the computation. Useful if folds \\(\\times\\) repeats is large, as this can be computationally intensive.\n\nHere we’ll perform a 10-fold cross validation with 10 repeats.\n\ntar_target(\n  spls_perf_res,\n  mixOmics::perf(\n    spls_novarsel,\n    validation = \"Mfold\",\n    folds = 10,\n    nrepeat = 10,\n    cpus = 3\n  )\n)\n\nWe can visualise the results of the cross-validation with the mixOmics::plot.tune.spls() function, setting the criterion to Q2.total, which corresponds to the \\(Q^2\\) criterion reflecting the predictive power of the model (other possible values include cor.tpred, cor.upred, RSS.tpred, RSS.upred):\n\nplot(tar_read(spls_perf_res), criterion = \"Q2.total\")\n\n\n\n\nThe plot generated displays the average \\(Q^2_{total}\\) criterion across the different cross-validation folds obtained with different number of latent components. The authors of the mixOmics package recommend to select the number of latent components above which the Q2 criterion becomes smaller than 0.0975 (depicted as a black horizontal line in the plot). However, there are cases in which the Q2 values instead increases with the number of latent components. In these cases, it is recommended to select the number of latent components above which the Q2 values exceed the threshold of 0.0975. To facilitate the selection, the spls_get_optim_ncomp() function can automatically select the optimal number of latent components to use based on the cross-validation results. However, its results should always be checked against the graphical representation of the \\(Q^2_{total}\\) criterion. In our case, the function selects the optimal number of latent components to construct to be 1:\n\nspls_get_optim_ncomp(tar_read(spls_perf_res))\n#> [1] 1\n\nNote that with this function, it is possible to set a minimum number of latent components to construct. This is useful if we want to be able to generate sample plots, even though technically only one latent component is necessary. This minimum number can be set through the min_ncomp argument. For the example, we will set it to two, to be able to showcase some of the plotting functionalities. We will save the chosen value in a target, for reuse in the rest of the pipeline:\n\ntar_target(\n  spls_optim_ncomp,\n  spls_get_optim_ncomp(spls_perf_res, min_ncomp = 2)\n)\n\n\ntar_read(spls_optim_ncomp)\n#> [1] 2"
  },
  {
    "objectID": "spls.html#choosing-the-number-of-features-to-retain",
    "href": "spls.html#choosing-the-number-of-features-to-retain",
    "title": "8  Integration with sPLS",
    "section": "\n8.4 Choosing the number of features to retain",
    "text": "8.4 Choosing the number of features to retain\nThe next parameter to set is the number of features to retain from each of the omics datasets, for each latent component. Again, we can use cross-validation to select optimal values amongst a grid of values to test. The range of values to test depends on the type of question we are trying to answer: selecting a larger number of features might lead to a higher degree of agreement between the latent components of the two datasets, but will be hard to manually inspect for further characterisation.\nThe function spls_tune() provides a wrapper around the mixOmics::tune() function that performs this cross-validation. Some of the arguments are similar to the mixOmics::perf() function, e.g. validation, folds, nrepeats or cpus. In addition, we recommend setting the measure argument, which corresponds to the metric used for performance assessment, to \"cor\" (i.e. the correlation between the latent components constructed). Alternatively, it is possible to set the measure argument to \"RSS\".\nThe keepX and keepY arguments control the grid of values to be tested as possible number of features to retain from the first and second dataset, respectively. Both arguments should be in the form of an integer vector of values to test. If no value is provided for either of these arguments, six values ranging from 5 to 30 (by increments of 5) are tested for the corresponding dataset. Here we will increase the maximum number of features that can be retained from each dataset to 100. Note that the metabolomics dataset has only 55 features, so the spls_tune function will remove all values to test above this number.\n\ntar_target(\n  spls_tune_res,\n  spls_tune(\n    spls_input,\n    ncomp = spls_optim_ncomp,\n    keepX = seq(10, 100, 10),\n    keepY = seq(10, 100, 10),\n    validation = \"Mfold\",\n    folds = 10,\n    nrepeat = 5,\n    measure = \"cor\",\n    cpus = 3\n  )\n)\n\nThe cross-validation results can be inspected with the spls_plot_tune() function:\n\ntar_load(spls_tune_res)\nspls_plot_tune(spls_tune_res)\n\n\n\n\nIn the plot, each point corresponds to a pair of values (keepX, keepY) tested. The colour of each point indicates the average correlation obtained across the cross-validation folds between the latent components constructed for the first dataset (left panel) or second dataset (right panel). The grey shadow around each point indicates the Coefficient of Variation (i.e. standard deviation divided by mean) of this correlation across the cross-validation folds (so a larger shadow indicates a larger relative variation across the folds). The point corresponding to the optimal pair of values for keepX and keepY is highlighted in red. It corresponds to the pair of values that maximises this average correlation for both datasets (according to a t-test of significance).\nThe optimal number of features to retain from each dataset for the different latent components is stored in the cross-validation results object, and can be accessed with:\n\nspls_tune_res$choice.keepX\n#> comp1 comp2 \n#>    40    10\nspls_tune_res$choice.keepY\n#> comp1 comp2 \n#>    30    10\n\nNote that for the example, we will keep the selected values, however given the diagnostic plot shown above, we could probably set the number of features to retain for the second latent component to 20 for the transcriptomics dataset and 30 for the metabolomics dataset."
  },
  {
    "objectID": "spls.html#final-spls-run",
    "href": "spls.html#final-spls-run",
    "title": "8  Integration with sPLS",
    "section": "\n8.5 Final sPLS run",
    "text": "8.5 Final sPLS run\nOnce a value has been selected for all parameters, we can perform the final sPLS run.\n\ntar_target(\n  spls_final_run,\n  spls_run(\n    spls_input,\n    ncomp = spls_optim_ncomp,\n    keepX = spls_tune_res$choice.keepX,\n    keepY = spls_tune_res$choice.keepY\n  )\n)\n\nWe will load the results of this final run for interpretation.\n\ntar_load(spls_final_run)\n\nTo facilitate reporting, the spls_get_params() function extracts from the sPLS result the parameters used (i.e. number of latent components computed and number of features retained from each dataset for each latent component):\n\nspls_get_params(spls_final_run)\n#> # A tibble: 3 × 3\n#>   Parameter Description                                                    Value\n#>   <chr>     <chr>                                                          <chr>\n#> 1 ncomp     Number of latent component                                     2    \n#> 2 keepX     Number of features retained in dataset rnaseq for each latent… 40, …\n#> 3 keepY     Number of features retained in dataset metabolome for each la… 30, …"
  },
  {
    "objectID": "spls.html#results-interpretation",
    "href": "spls.html#results-interpretation",
    "title": "8  Integration with sPLS",
    "section": "\n8.6 Results interpretation",
    "text": "8.6 Results interpretation\nIn Chapter 12, we show the different functionalities implemented in the moiraine package that facilitate the interpretation of the results from an integration tool. In this section, we show some of the sPLS-specific plots that can be generated to help interpret the results of an sPLS run.\n\n8.6.1 Samples plots\nWe can represent the samples in the subspace spanned by the latent components for each dataset, using the mixOmics::plotIndiv() function.\n\nplotIndiv(\n  spls_final_run,\n  ind.names = TRUE,\n  overlap = FALSE,\n  comp = 1:2\n)\n\n\n\n\nHere, ‘Block X’ refers to the first dataset (i.e. the transcriptomics dataset in our case), and ‘Block Y’ to the second dataset (the metabolomics dataset). The agreement between the latent components computed for each dataset can be more easily visualised using an arrow plot, using the mixOmics::plotArrow() function:\n\nplotArrow(\n  spls_final_run,\n  ind.names = TRUE,\n  comp = 1:2\n)\n\n\n\n\nIn the arrow plot, each sample is represented with two points, which are linked by an arrow. The point at the start of the arrow corresponds to the sample in the projection space of the first dataset (i.e. transcriptomics dataset), while the point at the end of the arrow shows the location of the sample in the projection space of the second dataset (i.e. the metabolomics dataset). In this plot, most of the arrows are large, which indicates that there is not a good agreement between the latent components of the two datasets.\n\n8.6.2 Correlation circle plots\nThe correlation circle plots produced by the mixOmics::plotVar() function allows to visualise the correlation between each of the selected features and the different latent components:\n\nplotVar(\n  spls_final_run,\n  comp = 1:2,\n  var.names = TRUE,\n  overlap = FALSE,\n  cex = c(3, 3)\n)\n\n\n\n\nThe plotVar() function offers the option to show the label of the features rather than representing them as points. However, it can be more informative to use information from the feature metadata as labels, rather than using the feature IDs. For example in the transcriptomics dataset, it would be more interesting to use the name of the genes. This information is available in the datasets’ features metadata:\n\ntar_load(mo_set_de)\nget_features_metadata(mo_set_de)[[\"rnaseq\"]] |>\n  str()\n#> 'data.frame':    20335 obs. of  15 variables:\n#>  $ feature_id : chr  \"ENSBTAG00000000005\" \"ENSBTAG00000000008\" \"ENSBTAG00000000009\" \"ENSBTAG00000000010\" ...\n#>  $ chromosome : Factor w/ 178 levels \"1\",\"2\",\"3\",\"4\",..: 17 29 18 21 8 20 20 5 5 23 ...\n#>  $ start      : int  65389743 32214439 12338037 34209956 7950815 33708626 33674806 74883636 74905031 27720176 ...\n#>  $ end        : int  65505336 32244810 12342272 34223394 7971600 33732944 33703223 74893752 74919112 27721739 ...\n#>  $ width      : int  115594 30372 4236 13439 20786 24319 28418 10117 14082 1564 ...\n#>  $ strand     : Factor w/ 3 levels \"+\",\"-\",\"*\": 1 2 1 1 2 1 1 2 2 2 ...\n#>  $ Name       : chr  \"GRK3\" \"KCNJ1\" \"FOXF1\" \"UBL7\" ...\n#>  $ description: chr  \"G protein-coupled receptor kinase 3 [Source:VGNC Symbol;Acc:VGNC:53904]\" \"potassium inwardly rectifying channel subfamily J member 1 [Source:VGNC Symbol;Acc:VGNC:30453]\" \"forkhead box F1 [Source:VGNC Symbol;Acc:VGNC:29084]\" \"ubiquitin like 7 [Source:VGNC Symbol;Acc:VGNC:50128]\" ...\n#>  $ log_fc     : num  0.136 -0.1297 1.2714 0.3957 0.0777 ...\n#>  $ log_cpm    : num  5.905 -0.744 -2.563 6.256 -2.761 ...\n#>  $ f          : num  4.163 1.068 23.956 60.53 0.142 ...\n#>  $ p_value    : num  4.32e-02 3.03e-01 2.73e-06 1.58e-12 7.07e-01 ...\n#>  $ fdr        : num  7.17e-02 3.94e-01 9.67e-06 1.52e-11 7.77e-01 ...\n#>  $ de_signif  : chr  \"Not DE\" \"Not DE\" \"Not DE\" \"Not DE\" ...\n#>  $ de_status  : chr  \"Not DE\" \"Not DE\" \"Not DE\" \"Not DE\" ...\n\nThe spls_plot_var() function is a variant of plotVar(), which uses columns from the features metadata to label features plot. It takes as an input the sPLS result object as well as the MultiDataSet object, and a named list providing for each dataset the name of the column in the feature metadata data-frame to use as features label:\n\nspls_plot_var(\n  spls_final_run,\n  mo_set_de,\n  label_cols = list(\n    \"rnaseq\" = \"Name\",\n    \"metabolome\" = \"name\"\n  ),\n  overlap = FALSE,\n  cex = c(3, 3),\n  comp = 1:2\n)\n\n\n\n\nNote that if a dataset is not present in the list passed to label_cols, the feature IDs will be used as labels."
  },
  {
    "objectID": "spls.html#recap---targets-list",
    "href": "spls.html#recap---targets-list",
    "title": "8  Integration with sPLS",
    "section": "\n8.7 Recap - targets list",
    "text": "8.7 Recap - targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for sPLS analysis\n\nlist(\n  ## Creating sPLS input\n  tar_target(\n    spls_input,\n    get_input_spls(\n      mo_presel_supervised,\n      mode = \"canonical\",\n      datasets = c(\"rnaseq\", \"metabolome\")\n    )\n  ),\n  \n  ## Initial PLS run with no feature selection and large number of components\n  tar_target(\n    spls_novarsel,\n    spls_run(\n      spls_input,\n      ncomp = 4\n    )\n  ),\n  \n  ## Cross-validation for number of components\n  tar_target(\n    spls_perf_res,\n    mixOmics::perf(\n      spls_novarsel,\n      validation = \"Mfold\",\n      folds = 10,\n      nrepeat = 10,\n      cpus = 3\n    )\n  ),\n  \n  ## Plotting cross-validation results (for number of components)\n  ## Can try criterion = 'Q2.total', 'cor.tpred', 'cor.upred', 'RSS.tpred',\n  ## 'RSS.upred' (but avoid 'RSS' and 'PRESS')\n  tar_target(\n    spls_perf_plot,\n    plot(spls_perf_res, criterion = \"Q2.total\")\n  ),\n  \n  ## Selected value for ncomp\n  tar_target(\n    spls_optim_ncomp,\n    spls_get_optim_ncomp(spls_perf_res, min_ncomp = 2)\n  ),\n  \n  ## Cross-validation for number of features to retain\n  tar_target(\n    spls_tune_res,\n    spls_tune(\n      spls_input,\n      ncomp = spls_optim_ncomp,\n      keepX = seq(10, 100, 10),\n      keepY = seq(10, 100, 10),\n      validation = \"Mfold\",\n      folds = 10,\n      nrepeat = 5,\n      measure = \"cor\",\n      cpus = 3\n    )\n  ),\n  \n  ## Plotting cross-validation results (for number of features)\n  tar_target(\n    spls_tune_plot,\n    spls_plot_tune(spls_tune_res)\n  ),\n  \n  ## Final sPLS run\n  tar_target(\n    spls_final_run,\n    spls_run(\n      spls_input,\n      ncomp = spls_optim_ncomp,\n      keepX = spls_tune_res$choice.keepX,\n      keepY = spls_tune_res$choice.keepY\n    )\n  )\n)\n\n\n\n\n\nLiquet, Benoit, Kim-Anh Lê Cao, Hakim Hocini, and Rodolphe Thiébaut. 2012. “A Novel Approach for Biomarker Selection and the Integration of Repeated Measures Experiments from Two Assays.” BMC Bioinformatics 13 (1): 325. https://doi.org/10.1186/1471-2105-13-325."
  },
  {
    "objectID": "so2pls.html#what-is-so2pls",
    "href": "so2pls.html#what-is-so2pls",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.1 What is sO2PLS?",
    "text": "9.1 What is sO2PLS?\nsO2PLS (for Sparse Orthogonal two-way Partial Least Squares) is a method for the unsupervised integration of two omics datasets. It aims at decomposing each of the two datasets into a joint part, which reflects covariation shared by the datasets, an orthogonal or specific part, which represents variation unique to each dataset, and a residual part. To this end, sets of joint and specific latent components are constructed for each dataset; these components are constructed as a linear combination of the features from the corresponding dataset. Feature selection is performed on the joint components of each dataset.\nThe joint latent components are constructed by maximising the covariance between the projection of each dataset onto their respective joint part. Sparsity is obtained by applying \\(L_1\\) regularisation (LASSO) to the feature joint loadings (i.e. their contribution to the joint components). However, no sparsity constraint is used on the feature loadings for the specific components. Joint and specific components are iteratively updated, with the datasets being corrected by removing the specific part before re-calculating the joint components. While the number of joint components is the same for the two datasets, each dataset can have a different number of specific components, including no specific component at all.\nsO2PLS requires as input matrices of omics measurements obtained on the same samples. In addition, it assumes that the features have been centred. The method cannot handle missing values, so these should be imputed prior to using sO2PLS."
  },
  {
    "objectID": "so2pls.html#creating-the-so2pls-input",
    "href": "so2pls.html#creating-the-so2pls-input",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.2 Creating the sO2PLs input",
    "text": "9.2 Creating the sO2PLs input\nThe first step is to transform the MultiDataSet object into a suitable format for the OmicsPLS package. This is done through the get_input_omicspls() function. The names of the two datasets to integrate are passed to the function through the datasets argument (in our case, we want to integrate the transcriptomics and metabolomics datasets). In the OmicsPLS terminology, the first dataset will be the X dataset, and the second will be the Y dataset. The function filters out samples that are not present in both datasets, and centres the features in each dataset. There is also the option to scale the datasets, through the scale_data argument (default behaviour is to not scale the data).\n\ntar_target(\n  omicspls_input,\n  get_input_omicspls(\n    mo_presel_supervised,\n    datasets = c(\"rnaseq\", \"metabolome\")\n  )\n)\n\nThe function returns a named list, where each element of the list is a matrix of measurements corresponding to one of the datasets to integrate, with samples as rows and features as columns:\n\ntar_read(omicspls_input) |> str()\n#> List of 2\n#>  $ rnaseq    : num [1:139, 1:994] -0.39 -0.39 0.129 -0.39 0.376 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:139] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>   .. ..$ : chr [1:994] \"ENSBTAG00000000020\" \"ENSBTAG00000000046\" \"ENSBTAG00000000056\" \"ENSBTAG00000000061\" ...\n#>   ..- attr(*, \"scaled:center\")= Named num [1:994] 3.88 7.44 10.14 4.75 12.64 ...\n#>   .. ..- attr(*, \"names\")= chr [1:994] \"ENSBTAG00000000020\" \"ENSBTAG00000000046\" \"ENSBTAG00000000056\" \"ENSBTAG00000000061\" ...\n#>  $ metabolome: num [1:139, 1:55] -0.2552 -0.151 0.1016 -0.2171 0.0842 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:139] \"R9497\" \"R5969\" \"R5327\" \"R5979\" ...\n#>   .. ..$ : chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n#>   ..- attr(*, \"scaled:center\")= Named num [1:55] 3.542 4.681 8.468 6.998 0.873 ...\n#>   .. ..- attr(*, \"names\")= chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n\nNote that there are 139 samples in each matrix, which are the samples that are present in both datasets."
  },
  {
    "objectID": "so2pls.html#choosing-the-number-of-latent-components",
    "href": "so2pls.html#choosing-the-number-of-latent-components",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.3 Choosing the number of latent components",
    "text": "9.3 Choosing the number of latent components\nIn order to integrate the datasets with sO2PLS, it is necessary to set the number of joint and specific components that will be computed for each dataset. Within the sO2PLS framework, the number of joint components is denoted as \\(n\\) (it is the same value for both datasets), while \\(n_x\\) and \\(n_y\\) represent the number of specific components for the first and second dataset, respectively. \\(n\\) must be a positive integer (i.e. at least 1), but \\(n_x\\) and \\(n_y\\) can also be set to 0. Values for the number of joint and specific components can be estimated using cross-validation. The authors of the OmicsPLS package recommend a two-step approach, which is detailed below.\n\n9.3.1 Adjusted cross-validation\nIn a first step, an adjusted cross-validation procedure is used to compute the optimal values for \\(n\\), \\(n_x\\) and \\(n_y\\). This is done through the so2pls_crossval_o2m_adjR2() function, which is a wrapper around the OmicsPLS::crossval_o2m_adjR2() function. Briefly, the function estimates, for each possible value of \\(n\\), the values of \\(n_x\\) and \\(n_y\\) that minimise the prediction error for the joint part of each dataset1. Using these values, it then computes the prediction error for the full dataset decomposition and uses it to select the optimal value of \\(n\\). Compared with the traditional cross-validation approach where the “full decomposition” prediction error is calculated for each possible combination of the tested parameter values, this adjusted procedure can be much faster, while often giving a very similar result. The authors recommend using it as a first pass to reduce the number of values to test for \\(n\\), \\(n_x\\) and \\(n_y\\).\nIn this example, we will test 1 to 5 joint components, and 0 to 10 specific components for each dataset (testing only even values). We use a 10-fold cross validation (nr_folds parameter specifying the number of folds to use for the cross-validation), and distribute the computation over 6 cores to reduce the running time (through the nr_cores argument). We will also set the seed to ensure the reproducibility of the results, through the seed argument.\n\ntar_target(\n  so2pls_cv_adj,\n  so2pls_crossval_o2m_adjR2(\n    omicspls_input,\n    a = 1:5,\n    ax = seq(0, 10, by = 2),\n    ay = seq(0, 10, by = 2),\n    nr_folds = 10,\n    nr_cores = 6,\n    seed = 127\n  )\n)\n\n\n\n\n\n\n\nNote\n\n\n\nIn order to use the seed argument for the so2pls_crossval_o2m_adjR2, you need to be using the latest version of the OmicsPLS package from GitHub, which can be installed with devtools::install_github(\"selbouhaddani/OmicsPLS\").\n\n\nDespite using multiple cores, this function can take a while to run (this example took around 8 minutes to execute). The function returns a data-frame giving, for each tested value for \\(n\\), the values of \\(n_x\\) and \\(n_y\\) that yielded the lowest prediction error, as well as the prediction error obtained for these values of the parameters:\n\ntar_load(so2pls_cv_adj)\n\nso2pls_cv_adj\n#>        MSE n nx ny\n#> 1 1.553526 1  8  2\n#> 2 1.575951 2  0  6\n#> 3 1.566081 3  4  4\n#> 4 1.594803 4  0  8\n#> 5 1.610357 5  0 10\n\nThe prediction error obtained with each tested value of \\(n\\) can be visualised with the so2pls_plot_cv_adj() function, which takes as input the result from so2pls_crossval_o2m_adjR2():\n\nso2pls_plot_cv_adj(so2pls_cv_adj)\n\n\n\n\nIn the plot, The values of \\(n_x\\) and \\(n_y\\) chosen for each value of \\(n\\) are displayed next to each point. We can see that the smallest prediction error is obtained with one joint component.\nIn addition, the so2pls_get_optim_ncomp_adj() function returns the optimal values of \\(n\\), \\(n_x\\) and \\(n_y\\) from the output of the cross-validation step as a vector:\n\nso2pls_get_optim_ncomp_adj(so2pls_cv_adj)\n#>  n nx ny \n#>  1  8  2\n\n\n9.3.2 Standard cross-validation\nIn a second step, a standard cross-validation approach is used to refine the results from the adjusted cross-validation step, by testing values around the selected \\(n\\), \\(n_x\\) and \\(n_y\\) values. Standard cross-validation is performed within the OmicsPLS package by the OmicsPLS::crossval_o2m() function. The function so2pls_crossval_o2m() is a convenient wrapper that takes as input the result from the adjusted cross-validation step, in order to automatically select which values should be tested for \\(n\\), \\(n_x\\) and \\(n_y\\).\nOnce again, we perform a 10-fold cross-validation, distributed over 6 cores:\n\ntar_target(\n  so2pls_cv,\n  so2pls_crossval_o2m(\n    omicspls_input,\n    so2pls_cv_adj, ## result from the adjusted cross-validation\n    nr_folds = 10,\n    nr_cores = 6,\n    seed = 356\n  )\n)\n\nThis standard cross-validation step takes around 6 minutes to run, which makes sense as we are testing less values than in the previous step.\nThe result is a cvo2m object, which is a list containing the results of the cross-validation step as well as further information about the run. It can be visualised with the so2pls_plot_cv() function:\n\ntar_load(so2pls_cv)\n\nso2pls_plot_cv(so2pls_cv)\n\n\n\n\nThe plot displays, for each tested value of \\(n\\) (facets), the prediction error (MSE – colour) obtained for the tested values of \\(n_x\\) (x-axis) and \\(n_y\\) (y-axis). The combination of values yielding the smallest prediction error is highlighted in red. This optimal combination can be extracted from the results with the so2pls_get_optim_ncomp() function:\n\nso2pls_get_optim_ncomp(so2pls_cv)\n#>  n nx ny \n#>  1  7  2\n\nFor convenience for the next steps of the analysis, we can store these values as a target:\n\ntar_target(\n  so2pls_cv_res,\n  so2pls_get_optim_ncomp(so2pls_cv)\n)"
  },
  {
    "objectID": "so2pls.html#choosing-the-number-of-features-to-retain-for-the-joint-components",
    "href": "so2pls.html#choosing-the-number-of-features-to-retain-for-the-joint-components",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.4 Choosing the number of features to retain for the joint components",
    "text": "9.4 Choosing the number of features to retain for the joint components\nOnce the number of joint and specific components to compute has been decided, we need to estimate the optimal number of features to retain from each dataset for each of the joint components (no feature selection is performed for the specific components). This is done through cross-validation, where the aim is to maximise the covariance between the joint components of the two datasets. This step is performed via the so2pls_crossval_sparsity() function (which is identical to the OmicsPLS::crossval_sparsity() function, except that all results from the cross-validation are returned, which is necessary for plotting purposes). The function takes as input the number of joint and specific components (n, nx and ny arguments), the number of folds to use for the cross-validation (nr_folds), and the values to test for each dataset (keepx_seq and keepy_seq arguments). For this example, we will test values between 5 and 100 for the rnaseq dataset, and between 5 and 40 for the metabolomics dataset:\n\ntar_target(\n  so2pls_cv_sparsity,\n  so2pls_crossval_sparsity(\n    omicspls_input,\n    n = so2pls_cv_res['n'],\n    nx = so2pls_cv_res['nx'],\n    ny = so2pls_cv_res['ny'],\n    nr_folds = 10,\n    keepx_seq = c(seq(5, 30, 5), seq(40, 100, 10)),\n    keepy_seq = c(seq(5, 40, 5))\n  )\n)\n\nThe function returns a list containing the results of the cross-validation. They can be visualised with the so2pls_plot_cv_sparsity() function:\n\ntar_load(so2pls_cv_sparsity)\n\nso2pls_plot_cv_sparsity(so2pls_cv_sparsity)\n\n\n\n\nThe plot depicts, for each joint component (facets – in this case there is only one), the average (colour) and standard deviation (gray area) of the covariance between the datasets’ joint components obtained for different number of features retained from the first (x-axis) and second (y-axis) dataset. The OmicsPLS package offers two options to select the optimal values to select for each latent component:\n\nthe maximum rule: the values that maximise the covariance between the datasets’ joint components are selected;\nthe 1 standard deviation rule: the smallest values (i.e. number of features) yielding a covariance within 1SD of the maximum covariance obtained for this latent component are selected.\n\nIn the plot, the values selected with the maximum rule are highlighted in orange, and the ones selected with the 1SD rule in red.\nThese optimal values can be extracted from the results of the cross-validation step with the so2pls_get_optim_keep() function. By default, the function uses the 1SD rule, but this can be disabled by setting the use_1sd_rule parameter to FALSE:\n\nso2pls_get_optim_keep(so2pls_cv_sparsity)\n#> $keepx\n#> x_1sd \n#>   100 \n#> \n#> $keepy\n#> y_1sd \n#>    20 \n#> \n#> attr(,\"datasets_name\")\n#> [1] \"rnaseq\"     \"metabolome\"\n\nso2pls_get_optim_keep(so2pls_cv_sparsity, use_1sd_rule = FALSE)\n#> $keepx\n#>   x \n#> 100 \n#> \n#> $keepy\n#>  y \n#> 40 \n#> \n#> attr(,\"datasets_name\")\n#> [1] \"rnaseq\"     \"metabolome\"\n\nThe function returns a list of vectors, which are needed for the next step of the analysis. We will save these values as a target:\n\ntar_target(\n  so2pls_cv_sparsity_res,\n  so2pls_get_optim_keep(so2pls_cv_sparsity)\n)\n\nThis list can be printed in a more reader-friendly way through the so2pls_print_cv_sparsity() function (which has been created mainly for reporting purposes), which returns a tibble containing the number of features to retain from each dataset for the different joint components:\n\nso2pls_print_cv_sparsity(tar_read(so2pls_cv_sparsity_res))\n#> # A tibble: 2 × 3\n#>   dataset    `Joint component 1` Total\n#>   <chr>                    <dbl> <dbl>\n#> 1 rnaseq                     100   100\n#> 2 metabolome                  20    20"
  },
  {
    "objectID": "so2pls.html#final-so2pls-run",
    "href": "so2pls.html#final-so2pls-run",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.5 Final sO2PLS run",
    "text": "9.5 Final sO2PLS run\nOnce a value has been selected for all parameters, we can perform the final sO2PLS run. This can be done through the so2pls_o2m() function, which is a wrapper around the OmicsPLS::o2m() function. We’ll use the values selected from the different cross-validation steps to inform the parameter values:\n\ntar_target(\n  so2pls_final_run,\n  so2pls_o2m(\n    omicspls_input,\n    so2pls_cv_res,\n    so2pls_cv_sparsity_res\n  )\n)\n\nThis function returns an o2m object, which, when printed, provides a summary of the sO2PLS results:\n\ntar_load(so2pls_final_run)\n\nso2pls_final_run\n#> SO2PLS fit \n#> with 1 joint components  \n#> and  7 orthogonal components in X \n#> and  2 orthogonal components in Y \n#> Elapsed time: 1.553 sec"
  },
  {
    "objectID": "so2pls.html#results-interpretation",
    "href": "so2pls.html#results-interpretation",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.6 Results interpretation",
    "text": "9.6 Results interpretation\nIn Chapter 12, we show the different functionalities implemented in the moiraine package that facilitate the interpretation of the results from an integration tool. In this section, we show some of the sO2PLS-specific plots that can be generated to help interpret the results of an sO2PLS run.\n\n9.6.1 Variance explained\nThe summary() function, when applied to an sO2PLS output, provides an overview of the variance explained by the different latent components, as well as other summary statistics:\n\nsummary(so2pls_final_run)\n#> \n#> *** Summary of the SO2PLS fit *** \n#> \n#> -  Call: OmicsPLS::o2m(X = omicspls_input[[\"rnaseq\"]], Y = omicspls_input[[\"metabolome\"]],      n = 1, nx = 7, ny = 2, sparse = TRUE, keepx = c(100), keepy = c(20)) \n#> \n#> -  Modeled variation\n#> -- Total variation:\n#> in X: 72197.01 \n#> in Y: 7792.534 \n#> \n#> -- Joint, Orthogonal and Noise as proportions:\n#> \n#>            data X data Y\n#> Joint       0.266  0.108\n#> Orthogonal  0.349  0.183\n#> Noise       0.385  0.710\n#> \n#> -- Predictable variation in Y-joint part by X-joint part:\n#> Variation in T*B_T relative to U: 0.78 \n#> -- Predictable variation in X-joint part by Y-joint part:\n#> Variation in U*B_U relative to T: 0.78 \n#> \n#> -- Variances per component:\n#> \n#>            Comp 1\n#> X joint 19223.344\n#> Y joint   839.995\n#> \n#>          Comp 1   Comp 2  Comp 3   Comp 4   Comp 5   Comp 6  Comp 7\n#> X Orth 11092.62 9297.399 3763.24 1132.937 2173.405 1178.272 823.791\n#> \n#>         Comp 1  Comp 2\n#> Y Orth 884.754 640.694\n#> \n#> \n#> -  Coefficient in 'U = T B_T + H_U' model:\n#> -- Diagonal elements of B_T =\n#>  0.185\n\nThese statistics can be visualised with the so2pls_plot_summary() function:\n\nso2pls_plot_summary(so2pls_final_run)\n\n\n\n\nThe plot on the left shows the percentage of variation explained by the joint, orthogonal/specific and residual parts for each dataset. In our example, the joint parts explain around 27% and 11% of the variation in the transcriptomics and metabolomics datasets, respectively, while the specific components explain around 35% and 18% of the variation in each dataset. Taken together, the joint and specific parts explain 61.5% of the variation present in the transcriptomics dataset, but only 29% in the metabolomics dataset. The plot on the right depicts the percentage of joint variation of a dataset explained by the joint variation of the other dataset, i.e. it gives an indication of how correlated the joint parts of the two datasets are. In our case, the joint part of each dataset explains between 78% of the joint variation in the other dataset, which is indicative of a good covariance between the datasets.\nTo get more information about the percentage of variation explained by each individual joint or specific component, we can have a look at the screeplot generated by the so2pls_screeplot() function, which shows the percentage of variance explained by each individual joint or specific component for each dataset:\n\nso2pls_screeplot(so2pls_final_run)\n\n\n\n\n\n9.6.2 Relationship between the datasets’ joint components\nAs the joint components of the two datasets are related, we can compare the joint components’ sample scores obtained for each dataset. This can be visualised with the so2pls_compare_samples_joint_components() function. In the resulting plot, the samples can be coloured according to some information that we have about the samples. This is done by passing the MultiDataSet multi-omics object to the function through the mo_data argument, and choosing the column from the samples metadata information that will be used to colour the samples, through the colour_by argument. Note that if nothing is passed to these two arguments, all samples will have the same colour. In our case, we want to colour the samples according to their disease status, and show the feedlot with the shape of the points:\n\ntar_load(mo_set_de)\n\nso2pls_compare_samples_joint_components(\n  so2pls_final_run,\n  mo_data = mo_set_de,\n  colour_by = \"status\",\n  shape_by = \"feedlot\"\n)\n\n\n\n\nIt seems that joint component 1 is pretty consistent between the two datasets, except for one individual (an infected animal from feedlot 2 seen on the bottom-left of the plot). For both datasets, this joint component clearly separates the control and infected animals.\nNote that, since the function returns a patchwork of ggplots, it is possible to customise the colours used as follows:\n\nso2pls_compare_samples_joint_components(\n  so2pls_final_run,\n  mo_data = tar_read(mo_set),\n  colour_by = \"status\",\n  shape_by = \"feedlot\"\n) &\n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n\nWe can visualise precisely how the joint components from the two datasets are related. As it is possible to express each joint component of one dataset as a linear combination of the joint components of the other dataset, we can look at the coefficients of these linear combinations. This is the purpose of the so2pls_plot_joint_components_coefficients() function. In this case, since there is only one joint component, the visualisation is not particularly useful:\n\nso2pls_plot_joint_components_coefficients(so2pls_final_run)\n\n\n\n\n\n9.6.3 Samples plots\nIn Chapter 12, different visualisations implemented in moiraine are presented to show the samples in the space spanned by the latent components. However, since in sO2PLS there is a distinction between joint and specific latent components, some convenience functions have been implemented to generate such plots for all joint or all specific components at once.\nFirst, we will focus on the joint components. The so2pls_plot_samples_joint_components() function represents the sample scores for the joint components as either violin plots (if there is only one joint component) or as a matrix of two-by-two scatter plots. Rather than displaying the sample scores separately for each dataset, it instead represents for each joint component the average of the sample scores across the two datasets. This function is based on the plot_samples_score() function (see ?sec-interpretation-samples-scores), so information about the samples can be added to the plot by passing a MultiDataSet object to the function. Here, we want to visualise the samples’ health status as well as feedlot.\n\nso2pls_plot_samples_joint_components(\n  so2pls_final_run,\n  mo_data = mo_set_de,\n  colour_upper = \"status\",\n  scale_colour_upper = scale_colour_brewer(palette = \"Paired\"),\n  shape_upper = \"feedlot\"\n) +\n  theme(legend.box = \"vertical\")\n\n\n\n\nWe can clearly see that joint component 1 separates most of the control from the infected animals. Note that the function returns a ggplot, which can be further customised.\nSimilarly, the so2pls_plot_samples_specific_components() function displays the sample scores for the specific components of each dataset. Contrary to the joint components, these specific sample scores plots are independent between the two datasets, and therefore the function returns a list of plots, one for each dataset. Again, the sample scores are represented either as a violin plot (if there is only one specific component), or as scatterplots (if there are more than one specific component). Samples metadata available in the MultiDataSet object can be used to colour the samples.\n\nso2pls_plot_samples_specific_components(\n  so2pls_final_run,\n  mo_data = mo_set_de,\n  colour_upper = \"feedlot\",\n  scale_colour_upper = scale_colour_brewer(palette = \"Paired\"),\n  colour_lower = \"rnaseq_batch\",\n  shape_upper = \"gender\"\n) |> \n  ## customising legend for each plot\n  map(\\(x) x + theme(legend.box = \"vertical\"))\n#> $rnaseq\n\n\n\n#> \n#> $metabolome\n\n\n\n\nThe transcriptomics specific components plots are harder to read as there are 8 of them, but it looks like for both datasets the specific components are not representing variation due to covariates such as gender or feedlot."
  },
  {
    "objectID": "so2pls.html#recap-targets-list",
    "href": "so2pls.html#recap-targets-list",
    "title": "9  Integration with sO2PLS",
    "section": "\n9.7 Recap – targets list",
    "text": "9.7 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for sO2PLS analysis\n\nlist(\n  ## Creating sO2PLS input\n  tar_target(\n    omicspls_input,\n    get_input_omicspls(\n      mo_presel_supervised,\n      datasets = c(\"rnaseq\", \"metabolome\")\n    )\n  ),\n  \n  ## Adjusted cross-validation for number of components\n  tar_target(\n    so2pls_cv_adj,\n    so2pls_crossval_o2m_adjR2(\n      omicspls_input,\n      a = 1:5,\n      ax = seq(0, 10, by = 2),\n      ay = seq(0, 10, by = 2),\n      nr_folds = 10,\n      nr_cores = 6,\n      seed = 127\n    )\n  ),\n  tar_target(\n    so2pls_cv_adj_res,\n    so2pls_get_optim_ncomp_adj(so2pls_cv_adj)\n  ),\n  \n  ## Plotting adjusted cross-validation results\n  tar_target(\n    so2pls_cv_adj_plot,\n    so2pls_plot_cv_adj(so2pls_cv_adj)\n  ),\n  \n  ## Standard cross-validation for number of components\n  tar_target(\n    so2pls_cv,\n    so2pls_crossval_o2m(\n      omicspls_input,\n      so2pls_cv_adj,\n      nr_folds = 10,\n      nr_cores = 6,\n      seed = 356\n    )\n  ),\n  tar_target(\n    so2pls_cv_res,\n    so2pls_get_optim_ncomp(so2pls_cv)\n  ),\n  \n  ## Plotting standard cross-validation results\n  tar_target(\n    so2pls_cv_plot,\n    so2pls_plot_cv(so2pls_cv)\n  ),\n  \n  ## Cross-validation for sparsity parameters\n  tar_target(\n    so2pls_cv_sparsity,\n    so2pls_crossval_sparsity(\n      omicspls_input,\n      n = so2pls_cv_res[\"n\"],\n      nx = so2pls_cv_res[\"nx\"],\n      ny = so2pls_cv_res[\"ny\"],\n      nr_folds = 10,\n      keepx_seq = c(seq(5, 30, 5), seq(40, 100, 10)),\n      keepy_seq = c(seq(5, 40, 5))\n    )\n  ),\n  tar_target(\n    so2pls_cv_sparsity_res,\n    so2pls_get_optim_keep(so2pls_cv_sparsity)\n  ),\n  \n  ## Plotting the results of the cross-validation for the number of features\n  ## to retain from each dataset for the different joint components\n  tar_target(\n    so2pls_cv_sparsity_plot,\n    so2pls_plot_cv_sparsity(so2pls_cv_sparsity)\n  ),\n  \n  ## Extracting sparsity results in table format\n  tar_target(\n    so2pls_cv_sparsity_table,\n    so2pls_print_cv_sparsity(so2pls_cv_sparsity_res)\n  ),\n  \n  ## Final sO2PLS run\n  tar_target(\n    so2pls_final_run,\n    so2pls_o2m(\n      omicspls_input,\n      so2pls_cv_res,\n      so2pls_cv_sparsity_res\n    )\n  ),\n  \n  ## Summary plot of percentage of variance explained\n  tar_target(\n    so2pls_summary_plot,\n    so2pls_plot_summary(so2pls_final_run)\n  ),\n  \n  ## Screeplot\n  tar_target(\n    so2pls_screeplot,\n    so2pls_screeplot(so2pls_final_run)\n  ),\n  \n  ## Comparison of samples score for joint components\n  tar_target(\n    so2pls_joint_components_comparison_plot,\n    so2pls_compare_samples_joint_components(\n      so2pls_final_run,\n      mo_data = mo_set_de,\n      colour_by = \"status\",\n      shape_by = \"feedlot\"\n    )\n  ),\n  \n  ## Coefficient plot for joint components\n  tar_target(\n    so2pls_joint_components_coefficients_plot,\n    so2pls_plot_joint_components_coefficients(so2pls_final_run)\n  ),\n  \n  ## Joint component samples score plot\n  tar_target(\n    so2pls_joint_components_samples_score_plot,\n    so2pls_plot_samples_joint_components(\n      so2pls_final_run,\n      mo_data = mo_set_de,\n      colour_upper = \"status\",\n      scale_colour_upper = scale_colour_brewer(palette = \"Paired\"),\n      shape_upper = \"feedlot\"\n    ) +\n      theme(legend.box = \"vertical\")\n  ),\n  \n  ## Specific components samples score plot\n  tar_target(\n    so2pls_specific_components_samples_score_plot,\n    so2pls_plot_samples_specific_components(\n      so2pls_final_run,\n      mo_data = mo_set_de,\n      colour_upper = \"feedlot\",\n      scale_colour_upper = scale_colour_brewer(palette = \"Paired\"),\n      colour_lower = \"rnaseq_batch\",\n      shape_upper = \"gender\"\n    ) |> \n      map(\\(x) x + theme(legend.box = \"vertical\"))\n  )\n)\n\n\n\n\n\nBouhaddani, Said el, Hae-Won Uh, Geurt Jongbloed, Caroline Hayward, Lucija Klarić, Szymon M. Kiełbasa, and Jeanine Houwing-Duistermaat. 2018. “Integrating Omics Datasets with the OmicsPLS Package.” BMC Bioinformatics 19 (1): 371. https://doi.org/10.1186/s12859-018-2371-3.\n\n\nGu, Zhujie, Said el Bouhaddani, Jiayi Pei, Jeanine Houwing-Duistermaat, and Hae-Won Uh. 2021. “Statistical Integration of Two Omics Datasets Using GO2PLS.” BMC Bioinformatics 22 (1): 131. https://doi.org/10.1186/s12859-021-03958-3."
  },
  {
    "objectID": "mofa.html#what-is-mofa",
    "href": "mofa.html#what-is-mofa",
    "title": "10  Integration with MOFA",
    "section": "\n10.1 What is MOFA?",
    "text": "10.1 What is MOFA?\nMOFA (for Multi-Omics Factor Analysis) is a method for the unsupervised integration of two or more omics datasets. It aims at uncovering the main axes of variations shared by all or a subset of the datasets, through the construction of a small number of latent factors. It can be assimilated to a generalisation of the PCA (Principal Components Analysis) to multiple datasets. The latent factors are constructed as sparse linear combinations of the omics features, highlighting the features that contribute to each axis of variation.\nMOFA uses a Bayesian framework to decompose each dataset (referred to as “view” in the package documentation) into the product of a latent factor matrix (representing to the main axes of variation) and a matrix of feature weights (indicating the extent to which the features contribute to the different latent factors). MOFA applies two levels of sparsity. The first level of sparsity is used to detect whether a given latent factor is active (i.e. explains variation) in each dataset. This allows to assess which sources of variations are shared across the datasets. The second level of sparsity is applied to the features, allowing to assess which features contribute to each source of variation.\nMOFA requires as input matrices of omics measurements that must have at least some samples in common, but accepts some samples being absent from some of the datasets. It is recommended to have at least 15 samples in common across the datasets in order to obtain sensible results. The number of features in each dataset must not be too small (i.e. at least 15). It is critical that each dataset is properly normalised, with batch effects removed. In addition, it is strongly recommended that the datasets are pre-filtered to retain highly variable features; a similar number of features should be retained across the datasets. Missing values are not a problem with MOFA. MOFA can also accept samples and features metadata as data-frames. Lastly, there is a number of parameters that can be customised. The most important ones will be mentioned in this chapter; for a complete list see the MOFA tutorial.\n\n\n\n\n\n\nNote\n\n\n\nA characteristic of the MOFA2 package is that the model training part of the analysis is done in Python with the mofapy2 module. MOFA2 interacts with Python from R with the reticulate package. This is important to know as it might affect the installation and run depending on the environment in which it is used.\n\n\n\n10.1.1 A note on groups in MOFA\nMOFA offers the option to assign samples to groups (referred to as the multi-group framework within MOFA). It is important to understand what this does before using it. In particular, if the aim of the analysis is to find features that separate samples based on a grouping, you should not use this multi-group framework. By setting sample groups in MOFA, it will ignore sources of variations that discriminate the groups. Instead, this multi-group framework will seek sources of variations that are shared between the different groups, and sources of variations that are exclusive to a specific group. For example, in this analysis, the multi-group framework was used to split samples (cells) based on their developmental stage. The goal of the analysis was to find coordinated variation between the different datasets, and detect at which developmental stage (i.e. in which group) it occurred, rather than trying to differentiate the different developmental stages."
  },
  {
    "objectID": "mofa.html#creating-the-mofa-input",
    "href": "mofa.html#creating-the-mofa-input",
    "title": "10  Integration with MOFA",
    "section": "\n10.2 Creating the MOFA input",
    "text": "10.2 Creating the MOFA input\nThe first step is to transform the MultiDataSet object into a suitable format for the MOFA2 package. This is done through the get_input_mofa() function. In addition to the MultiDataSet object, this function accepts as arguments:\n\ndatasets: a vector of dataset names that dictates which datasets from the multi-omics set should be included in the analysis (by default, all datasets are included);\ngroups: the name of the column in the samples metadata to be used as group indicator if using the multi-group framework (if not set, the multi-group framework will not be used);\noptions_list: a list of options to customise the MOFA model – more information below;\nonly_common_samples: whether only the samples present in all datasets should be retained. The default value is FALSE; this argument should be set to TRUE when some datasets contain many more samples than others; when only a few samples are missing from some of the datasets, it is ok to leave it to FALSE.\n\nWe expand on the options that can be set with the options_list argument below.\n\n10.2.1 MOFA parameters\nMOFA has three type of parameters (referred to as options) that the user can control.\n\n10.2.1.1 Data options\nThese are options for data handling when creating the MOFA model. The two important options are:\n\nscale_views: logical, whether the datasets (views) should be scaled to unit variance. The default is FALSE, but it is recommended to set it to TRUE if the scale difference between the datasets is high.\nscale_groups: logical, whether to scale the groups to unit variance. Irrelevant unless using the multi-group framework. The default is FALSE, but it is recommended to set it to TRUE if the scale difference between the groups is high.\n\nSee the documentation for get_default_data_options() for a full list.\n\n10.2.1.2 Model options\nThese are the options defining different aspects of the MOFA model. The most important ones are:\n\nnum_factors: The maximum number of factors to construct (note that this is a maximum, i.e. MOFA can return a lower number of factors if there is not a lot of variation in the datasets). The default is set to 15, which is a good start. This can be adjusted after running MOFA with the default value.\nlikelihoods: Named character vector, the type of likelihood to use for each dataset. MOFA offers three options: Gaussian likelihood ('gaussian') for continuous data, Bernoulli likelihood ('bernoulli') for binary data, and Poisson likelihood ('poisson') for count data. It is highly recommended to transform the datasets in order to use a Gaussian likelihood: for example, applying a variance-stabilising transformation on RNAseq data rather than using the raw read counts with a Poisson likelihood. By default, a Gaussian likelihood is chosen for all datasets.\n\nSee the documentation for get_default_model_options() for a full list.\n\n10.2.1.3 Training options\nThese are the options that control how the model is trained. The most important one is:\n\n\nseed: setting the random seed. This is standard practice, to make sure that the results are reproducible.\n\nSee the documentation for get_default_training_options() for a full list.\n\n10.2.1.4 Passing the parameters to get_input_mofa\n\nWhen creating a MOFA input object with get_input_mofa(), all options will be set to their default values. It is possible to customise the values for some of the options through the options_list parameter. This should be a named list, with up to three elements (one per type of options that MOFA accepts): data_options, model_options, and training_options. Each element is itself a named list, where each element of the list corresponds to a specific option. All three elements do not have to be present; for example, if we want to only specify a value for the model option num_factors, we can set options_list to:\n\nlist(\n  model_options = list(num_factors = 10)\n)\n\nIf we also want to set the likelihoods and the random seed, then options_list becomes:\n\nlist(\n  model_options = list(num_factors = 10),\n  training_options = list(seed = 43)\n)\n\nFor our example, we’ll make sure that each dataset is scaled to unit variance, and that a Poisson likelihood is used for the genomics data, for which we have variants dosage. Since there are only 9 samples that are not present in all omics datasets, we can keep them in the analysis. We’ll set the random seed to ensure that the results are reproducible:\n\ntar_target(\n  mofa_input,\n  get_input_mofa(\n    mo_presel_supervised,\n    options_list = list(\n      data_options = list(scale_views = TRUE),\n      model_options = list(likelihoods = c(\n        \"snps\" = \"poisson\",\n        \"rnaseq\" = \"gaussian\",\n        \"metabolome\" = \"gaussian\")\n      ),\n      training_options = list(seed = 43)\n    ),\n    only_common_samples = FALSE\n  )\n)\n\nThis will produce a warning:\n\n#> Warning: Dataset snps is to be modelled with a poisson likelihood, but is not\n#> integer. Transforming to integer.\n\nThe warning informs us that we have chosen to use a Poisson likelihood for the genomics dataset, but the latter does not have integer data. This is because the missing values imputed with NIPALS-PCA (see Section 6.2.3) are continuous rather than discrete. This is taken care of by automatically rounding the dataset; but in other settings this warning can be an indication that the Poisson likelihood is not appropriate for the dataset.\n\n\n\n\n\n\nNote\n\n\n\nWhen constructing the MOFA object, if there exists a column named group in a samples metadata table in the MultiDataSet object, the column will be renamed as group_metadata in the resulting MOFA input object. This is because MOFA needs a group column its own version of the samples metadata (for the multi-group framework), so there cannot be another column named group. Note that will have no effect on the MultiDataSet object, but is important to remember if you want to use some of the plotting functionalities that MOFA2 offers.\n\n\nThe output of the get_input_mofa() function is a MOFA object, more specifically an untrained model:\n\ntar_load(mofa_input)\nmofa_input\n#> Untrained MOFA model with the following characteristics: \n#>  Number of views: 3 \n#>  Views names: snps rnaseq metabolome \n#>  Number of features (per view): 1000 994 55 \n#>  Number of groups: 1 \n#>  Groups names: group1 \n#>  Number of samples (per group): 144 \n#> \n\nWe did not use the multi-group framework, so there is only one samples group."
  },
  {
    "objectID": "mofa.html#visualising-the-mofa-input",
    "href": "mofa.html#visualising-the-mofa-input",
    "title": "10  Integration with MOFA",
    "section": "\n10.3 Visualising the MOFA input",
    "text": "10.3 Visualising the MOFA input\nIt is possible to represent the samples that are present or missing across the different datasets with the plot_data_overview() function implemented in MOFA2:\n\nplot_data_overview(mofa_input)\n\n\n\n\nThe plot generated shows the dimensions of the datasets that will be analysed.\nFor reporting purposes, it can also be useful to summarise the different options used to create the MOFA model. This can be done with the options_list_as_tibble() function, which turns a list of parameters into a tibble presenting the name and value of each parameter. For example, we can list the data options used:\n\noptions_list_as_tibble(mofa_input@data_options)\n#> # A tibble: 6 × 2\n#>   Parameter     Value                         \n#>   <chr>         <chr>                         \n#> 1 scale_views   TRUE                          \n#> 2 scale_groups  FALSE                         \n#> 3 center_groups TRUE                          \n#> 4 use_float32   TRUE                          \n#> 5 views         'snps', 'rnaseq', 'metabolome'\n#> 6 groups        group1"
  },
  {
    "objectID": "mofa.html#training-the-model",
    "href": "mofa.html#training-the-model",
    "title": "10  Integration with MOFA",
    "section": "\n10.4 Training the model",
    "text": "10.4 Training the model\nOnce we have prepared our MOFA input, we can train the model with the run_mofa() function (from the MOFA2 package). This is done through Python, so there might be some issues when trying to run the function for the first time after installing MOFA2 if the configuration is not correct. If that is the case, see the MOFA2 tutorial (e.g. their troubleshooting section) for tips on how to solve this.\nBy default, the function will save the resulting trained model in a temporary file, with the .hdf5 format. It can be preferable to save the result into the output folder of your project; here as we are using targets to save the results of each step of the analysis, we do not need to do so. In addition, is it strongly recommended to set the save_data parameter to TRUE, as otherwise without the data, some of the visualisation options might not be available. Lastly, the use_basilisk parameter might have to be switched to FALSE if there are any issues with calling the Python module:\n\ntar_target(\n  mofa_trained,\n  run_mofa(\n    mofa_input,\n    save_data = TRUE,\n    use_basilisk = TRUE\n  )\n)\n\nIn our case, the following warnings are returned:\n\n#> Warning: No output filename provided. Using tmpRtmpaM7XWKmofa_20240125102128.hdf5 to store the trained model.\n#> \n#> Factors 1 are strongly correlated with the total number of expressed features for at least one of your omics. Such factors appear when there are differences in the total levels between your samples, sometimes because of poor normalisation in the preprocessing steps.\n\nThe first one has to do with saving the model into a temporary file. We will explain the others in more detail when analysing the resulting model.\nThe output of the function is a trained MOFA model, with 15 latent factors:\n\ntar_load(mofa_trained)\nmofa_trained\n#> Trained MOFA with the following characteristics: \n#>  Number of views: 3 \n#>  Views names: snps rnaseq metabolome \n#>  Number of features (per view): 1000 994 55 \n#>  Number of groups: 1 \n#>  Groups names: group1 \n#>  Number of samples (per group): 144 \n#>  Number of factors: 15"
  },
  {
    "objectID": "mofa.html#results-interpretation",
    "href": "mofa.html#results-interpretation",
    "title": "10  Integration with MOFA",
    "section": "\n10.5 Results interpretation",
    "text": "10.5 Results interpretation\nIn Chapter 12, we show the different functionalities implemented in the moiraine package that facilitate the interpretation of the results from an integration tool. In this section, we show some of the MOFA-specific plots that can be generated to help interpret the results of a MOFA run.\n\n10.5.1 Variance explained\nWhen training the model, MOFA2 constructs latent factors, which are sparse combination of the features, and which represent main axes of variation shared across all or a subsets of the datasets. One of the first steps to take when analysing the trained model is to assess the amount of variance in the datasets explained by each factor; similarly to what we would do when running a PCA.\nThe function plot_variance_explained() from MOFA2 displays the percentage of variance in each dataset explained by each factor as a heatmap. The x and y arguments (and also split_by argument when using the multi-group framework) control whether the factors or the datasets should be represented as the rows or columns in the heatmap:\n\nplot_variance_explained(\n  mofa_trained,\n  x = \"view\",  ## datasets on the x-axis\n  y = \"factor\" ## factors on the y-axis\n) \n\n\n\n\nHere, we see that factor 1 explains a lot (almost 50%) of variation in the transcriptomics dataset, a little (around 10%) in the metabolomics dataset and almost none in the genomics dataset. Factor 2 explains around 20% of variation in the genomics dataset, and none in the other datasets. Factors 3 and 4 seem specificto the transcriptomics datasets, and factors 5 to 15 explain very little variation; seeing this, we could re-train the MOFA model by setting the number of factors to 4 or 5.\nIn addition, the plot_variance_explained() function can create a second plot displaying the total percentage of variance explained by all the factors for each dataset, if the argument plot_total is set to TRUE :\n\nplot_variance_explained(\n  mofa_trained,\n  x = \"view\",\n  y = \"factor\",\n  plot_total = TRUE\n)[[2]] ## show only the 2nd plot\n\n\n\n\nThe factors explain almost 85% of the variation in the transcriptomics dataset, but less than 30% in in the genomics and metabolomics datasets.\n\n10.5.2 Correlation between factors\nBefore moving to the interpretation, it is always good practice to check the correlation between the different computed factors. All factors should be mostly uncorrelated; if a large correlation is observed between some factors, it could be an indication of poor model fit, either because too many factors were computed (in this case, try reducing the number of factors to compute or increasing the number of iterations for the model training via the convergence_mode training option) or maybe because of an improper normalisation of the datasets. The function plot_factor_cor() from MOFA2 displays the correlation between all factors via the corrplot::corrplot() function:\n\nplot_factor_cor(\n  mofa_trained,\n  type = \"upper\",\n  diag = FALSE,\n  tl.cex = 0.8\n)\n\n\n\n\nThere is a moderate positive correlation between factors 2 and 5, but nothing to be concerned about.\n\n10.5.3 Correlation between factors and samples covariates\nTo assess which sources of variation each factor is representing, we can check whether the factors are correlated with some known covariates that were recorded in the samples metadata. The mofa_plot_cor_covariates() function displays the correlation between each factor and the samples covariates. Note that factor covariates are transformed to numerical group indicators in order to compute the correlations. This function is similar to the MOFA2 function correlate_factors_with_covariates() except that it returns a ggplot (rather than creating a base R plot), and offers a few convenient features through the optional arguments. By default, the function uses all covariates recorded in the samples metadata; however we can focus on a subset of them by passing the column names to the covariates argument.\n\nmofa_plot_cor_covariates(\n  mofa_trained\n)\n\n\n\n\nFrom this plot, we can spot some interesting trends. For example, factor 1, which explains variation in the transcriptomics and metabolomics datasets, is strongly correlated with disease status. It makes sense, since the transcriptomics dataset has been filtered to retain genes most associated with differences between healthly and infected animals, so we expect it to be the strongest source of variation in the dataset. Factor 2, which explains variation only in the genomics dataset, is strongly correlated with the genomics composition of the animals, which makes a lot of sense as more related individuals share similar genomics profiles. It also makes sense that this variation is less present in the other two datasets. Factor 5, which explains variation only in the transcriptomics dataset, seems to be modestly correlated with the RNASeq batches.\nIn general, it is a good idea to interpret this correlation plot together with the plot representing the percentage of variation explained in the datasets by each factor. However, it is necessary to investigate more before making claims about what the factors are representing.\n\n10.5.4 Visualising the top features\nThe MOFA2 package offers a number of very useful visualisations to further interpret the results. For example, for a given factor and dataset of interest, we can visualise how the top contributing features vary with the factor values (i.e. the sample scores). For example, let’s have a look at the top contributing features from the transcriptomics dataset for factor 1:\n\nplot_data_scatter(\n  mofa_trained,\n  factor = 1,\n  view = \"rnaseq\",\n  features = 6,\n  color_by = \"status\",\n  shape_by = \"feedlot\"\n)\n\n\n\n\nFor all six genes, their expression is higher in infected animals compared with healthy ones.\nWe can also visualise the measurements of the top contributing features across the samples as a heatmap, for a given dataset and factor. That the annotation_samples argument allows us to add annotations on top of the heatmap to represent certain characteristics of the samples. For example here we’ll add information about the phenotype group and chromosome:\n\nMOFA2::plot_data_heatmap(\n  mofa_trained,\n  factor = 1,\n  view = \"rnaseq\",\n  features = 20,\n  annotation_samples = \"status\",\n  fontsize_col = 5\n)\n\n\n\n\nNote that moiraine implements similar functions to represent the top contributing features, as we will see in Chapter 12."
  },
  {
    "objectID": "mofa.html#recap-targets-list",
    "href": "mofa.html#recap-targets-list",
    "title": "10  Integration with MOFA",
    "section": "\n10.6 Recap – targets list",
    "text": "10.6 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for MOFA analysis\n\nlist(\n  ## Creating MOFA input\n  tar_target(\n    mofa_input,\n    get_input_mofa(\n      mo_presel_supervised,\n      options_list = list(\n        data_options = list(scale_views = TRUE),\n        model_options = list(likelihoods = c(\n          \"snps\" = \"poisson\",\n          \"rnaseq\" = \"gaussian\",\n          \"metabolome\" = \"gaussian\")\n        ),\n        training_options = list(seed = 43)\n      ),\n      only_common_samples = FALSE\n    )\n  ),\n  \n  ## Overview plot of the samples in each dataset\n  tar_target(\n    mofa_input_plot,\n    plot_data_overview(mofa_input)\n  ),\n  \n  ## Training MOFA model\n  tar_target(\n    mofa_trained,\n    run_mofa(\n      mofa_input,\n      save_data = TRUE,\n      use_basilisk = TRUE\n    )\n  ),\n  \n  ## Formatting MOFA output\n  tar_target(\n    mofa_output,\n    get_output(mofa_trained)\n  ),\n  \n  ## Plots of variance explained\n  tar_target(\n    mofa_var_explained_plot,\n    plot_variance_explained(\n      mofa_trained,\n      x = \"view\",  ## datasets on the x-axis\n      y = \"factor\" ## factors on the y-axis\n    )\n  ),\n  tar_target(\n    mofa_total_var_explained_plot,\n    plot_variance_explained(\n      mofa_trained,\n      x = \"view\",\n      y = \"factor\",\n      plot_total = TRUE\n    )[[2]]\n  ),\n  \n  ## Plot of factors correlation with covariates\n  tar_target(\n    mofa_factors_covariates_cor_plot,\n    mofa_plot_cor_covariates(mofa_trained)\n  )\n)"
  },
  {
    "objectID": "diablo.html#what-is-diablo",
    "href": "diablo.html#what-is-diablo",
    "title": "11  Integration with DIABLO",
    "section": "\n11.1 What is DIABLO?",
    "text": "11.1 What is DIABLO?\nDIABLO (for Data Integration Analysis for Biomarker discovery using Latent Components) is a multivariate approach to perform supervised data integration. Given two or more omics datasets for which measurements are taken on the same samples, DIABLO aims at selecting correlated features across the datasets that best discriminate between the different sample groups for a categorical outcome of interest.\nDIABLO works by iteratively constructing linear combinations of the features, called latent components, which maximise the correlation between the datasets and with the categorical outcome. In order to perform feature selection, the latent components are subjected to \\(L1\\)-regularisation (or LASSO), i.e. the number of features included in the linear combination is constrained by the user. Moreover, the optimisation problem is weighted to allow the user to control the balance between maximising the correlation between omics datasets and discriminating between the outcome groups of interest.\nDIABLO requires as input the matrices of omics measurements, all with the same samples, as well as a factor variable indicating the outcome group for each sample. While the omics datasets are automatically centred and scaled by DIABLO, proper preprocessing and normalisation is assumed to be carried out by the user. Although the DIABLO algorithm can handle the presence of missing features, it will prohibit the use of cross-validation. It is thus recommended to perform data imputation prior to running a DIABLO analysis. Importantly, DIABLO tends to perform better when the number of features in the datasets is not too large. Therefore, it is highly recommended to perform some prefiltering prior to using DIABLO."
  },
  {
    "objectID": "diablo.html#creating-the-diablo-input",
    "href": "diablo.html#creating-the-diablo-input",
    "title": "11  Integration with DIABLO",
    "section": "\n11.2 Creating the DIABLO input",
    "text": "11.2 Creating the DIABLO input\nThe first step is to transform the MultiDataSet object into a suitable format for the mixOmics package. This is done with the get_input_mixomics_supervised() function, which takes as input:\n\na MultiDataSet object,\nthe name of the column in the samples metadata table that corresponds to the categorical outcome of interest,\noptionally, the names of the datasets to include in the analysis. This is useful if we want to exclude one of more datasets from the analysis.\n\nIn our case, we want to find differences between the healthy and diseased animals. We will use the multi-omics datasets that have gone through supervised prefiltering (i.e. we discarded the features least related with the disease status, as seen in Chapter 7).\n\ntar_target(\n  diablo_input,\n  get_input_mixomics_supervised(\n    mo_presel_supervised,\n    group = \"status\"\n  )\n)\n\nImportantly, the get_input_mixomics_supervised() function only retains samples that are present in all omics datasets to be analysed. It also makes sure that the column provided as categorical outcome does not contain numerical values, as DIABLO can only handle categorical outcome. If the column contains integers, they will be considered as levels of a factor.\nThe result of the function is a named list with one element per dataset to integrate, plus a Y element that contains the categorical outcome. The omics datasets are stored as matrices with samples as rows and features as columns; the categorical outcome is a named factor vector.\n\ntar_load(diablo_input)\nstr(diablo_input)\n#> List of 4\n#>  $ snps      : num [1:135, 1:1000] 2 1 2 2 2 2 1 2 2 2 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:135] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>   .. ..$ : chr [1:1000] \"21-25977541-C-T-rs41974686\" \"22-51403583-A-C-rs210306176\" \"24-12959068-G-T-rs381471286\" \"8-85224224-T-C-rs43565287\" ...\n#>   ..- attr(*, \"datatype\")= chr \"SnpSet\"\n#>   .. ..- attr(*, \"package\")= chr \"Biobase\"\n#>  $ rnaseq    : num [1:135, 1:994] 3.87 4.97 4.08 3.49 3.49 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:135] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>   .. ..$ : chr [1:994] \"ENSBTAG00000000020\" \"ENSBTAG00000000046\" \"ENSBTAG00000000056\" \"ENSBTAG00000000061\" ...\n#>   ..- attr(*, \"datatype\")= chr \"ExpressionSet\"\n#>   .. ..- attr(*, \"package\")= chr \"Biobase\"\n#>  $ metabolome: num [1:135, 1:55] 3.19 3.28 3.22 3.77 3.63 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:135] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ...\n#>   .. ..$ : chr [1:55] \"HMDB00001\" \"HMDB00008\" \"HMDB00042\" \"HMDB00043\" ...\n#>   ..- attr(*, \"datatype\")= chr \"MetabolomeSet\"\n#>   .. ..- attr(*, \"package\")= chr \"moiraine\"\n#>  $ Y         : Factor w/ 2 levels \"BRD\",\"Control\": 2 2 2 2 1 2 2 2 2 2 ...\n#>   ..- attr(*, \"names\")= chr [1:135] \"R21\" \"Y3660\" \"Y3243\" \"R5764\" ..."
  },
  {
    "objectID": "diablo.html#constructing-the-design-matrix",
    "href": "diablo.html#constructing-the-design-matrix",
    "title": "11  Integration with DIABLO",
    "section": "\n11.3 Constructing the design matrix",
    "text": "11.3 Constructing the design matrix\nDIABLO relies on a design matrix to balance its two optimisation objectives: maximising the covariance between the omics datasets, and maximising the discrimination between the outcome categories. The design matrix is a matrix with one row and one column per dataset, plus a row and a column for the \"Y\" dataset, i.e. the categorical outcome. The values within each cell of the matrix indicate the ratio between the two objectives for this combination of dataset. A value of 0 means that we want to prioritise outcome discrimination, while a value of 1 indicates that we want to prioritise maximising the covariance between the two corresponding datasets. All values must be between 0 and 1.\nThere are two options for constructing the design matrix, which we present below.\n\n11.3.1 Predefined design matrices\nA first option is to choose a strategy based on what we are trying to obtain from the integration:\n\nif we want to strike a balance between the two objectives (recommended option), we’ll constructed a “weighted full” design matrix that looks like this:\n\n\ndiablo_predefined_design_matrix(names(diablo_input), \"weighted_full\")\n#>            snps rnaseq metabolome Y\n#> snps        0.0    0.1        0.1 1\n#> rnaseq      0.1    0.0        0.1 1\n#> metabolome  0.1    0.1        0.0 1\n#> Y           1.0    1.0        1.0 0\n\n\nif we want to maximise the discrimination between the outcome categories, we’ll construct a “null” design matrix that looks like this:\n\n\ndiablo_predefined_design_matrix(names(diablo_input), \"null\")\n#>            snps rnaseq metabolome Y\n#> snps          0      0          0 1\n#> rnaseq        0      0          0 1\n#> metabolome    0      0          0 1\n#> Y             1      1          1 0\n\n\nif we want only to maximise the covariance between the datasets, we’ll construct a “full” design matrix that looks like this:\n\n\ndiablo_predefined_design_matrix(names(diablo_input), \"full\")\n#>            snps rnaseq metabolome Y\n#> snps          0      1          1 1\n#> rnaseq        1      0          1 1\n#> metabolome    1      1          0 1\n#> Y             1      1          1 0\n\nWe will show how to use these pre-defined design matrices when running DIABLO.\n\n11.3.2 Estimating the design matrix through pairwise PLS\nAlternatively, we can let the data guide the construction of the design matrix. This is achieved by assessing the correlation between each pair of datasets, through a PLS (Projection to Latent Structures) run. More specifically, the correlation between the datasets is computed as the correlation coefficient between the first component constructed for each dataset during the PLS run. Then, based on the correlation obtained between a pair of dataset, we can decide on a value to use for the design matrix. Typically, the following thresholds are recommended by the authors of the mixOmics package:\n\ncorrelation coefficient of 0.8 or above between two datasets: assign a value of 1 in the corresponding cell of the design matrix;\ncorrelation coefficient below 0.8: assign a value of 0.1 in the corresponding cell of the design matrix.\n\nThe diablo_pairwise_pls_factory() function automates this process. It takes as input the DIABLO input object that we constructed previously:\n\ndiablo_pairwise_pls_factory(diablo_input)\n\nThe function works as follows:\n\nIt creates a list of all possible pairs of datasets, which is stored in the diablo_pairs_datasets target:\n\n\ntar_read(diablo_pairs_datasets)\n#> [[1]]\n#> [1] \"snps\"   \"rnaseq\"\n#> \n#> [[2]]\n#> [1] \"snps\"       \"metabolome\"\n#> \n#> [[3]]\n#> [1] \"rnaseq\"     \"metabolome\"\n\n\nIt uses dynamic branching to perform a PLS run on each pair of datasets, via the run_pairwise_pls() function. The results are stored as a list in the diablo_pls_runs_list target. Each element of the list has a datasets_name attribute to indicate which datasets were analysed:\n\n\nmap(tar_read(diablo_pls_runs_list), attr, \"datasets_name\")\n#> $diablo_pls_runs_list_05fb1980\n#> [1] \"snps\"   \"rnaseq\"\n#> \n#> $diablo_pls_runs_list_1b4ec539\n#> [1] \"snps\"       \"metabolome\"\n#> \n#> $diablo_pls_runs_list_4f48d450\n#> [1] \"rnaseq\"     \"metabolome\"\n\n\nIt constructs the estimated correlation matrix between the datasets, based on the results of the PLS runs, via the diablo_get_pairwise_pls_corr() function. The resulting matrix is available through the diablo_pls_correlation_matrix target:\n\n\ntar_read(diablo_pls_correlation_matrix)\n#>                 snps    rnaseq metabolome\n#> snps       1.0000000 0.6474490  0.6294428\n#> rnaseq     0.6474490 1.0000000  0.8513394\n#> metabolome 0.6294428 0.8513394  1.0000000\n\n\nIt constructs the design matrix according to the datasets correlation matrix, through the diablo_generate_design_matrix() function. This function has parameters to customise how the correlation matrix should be translated into a design matrix, notably by setting the threshold to use on the correlation coefficients (default is 0.8, as recommended). These arguments can be customised in the diablo_pairwise_pls_factory() function. The resulting design function is stored in the target diablo_design_matrix:\n\n\ntar_read(diablo_design_matrix)\n#>            snps rnaseq metabolome Y\n#> snps        0.0    0.1        0.1 1\n#> rnaseq      0.1    0.0        1.0 1\n#> metabolome  0.1    1.0        0.0 1\n#> Y           1.0    1.0        1.0 0"
  },
  {
    "objectID": "diablo.html#choosing-the-number-of-latent-components",
    "href": "diablo.html#choosing-the-number-of-latent-components",
    "title": "11  Integration with DIABLO",
    "section": "\n11.4 Choosing the number of latent components",
    "text": "11.4 Choosing the number of latent components\nOne important parameter that must be set when performing a DIABLO analysis is the number of latent components to construct for each dataset. The optimal number of components can be estimated by cross-validation, implemented in the mixOmics::perf() function. This function assesses the classification performance (i.e. how well the different outcome groups are separated) achieved by DIABLO for different numbers of latent components.\nChoosing the optimal number of latent components to construct is a multi-step process. The first step is to run DIABLO without feature selection, setting the number of latent components to the maximum value we wish to test. We recommend to set this to the number of groups in the categorical outcome + 2, which in our case equals 4; however this can be further refined after checking the results. For this example, we will set the maximum to 7. This is done through the diablo_run() function, which is a wrapper for the mixOmics::block.splsda() function. The function also requires as input the design matrix to be used; here we will use the one constructed from the PLS runs:\n\ntar_target(\n  diablo_novarsel,\n  diablo_run(\n    diablo_input, \n    diablo_design_matrix, \n    ncomp = 7\n  )\n)\n\nAlternatively, if we want to use one of the predefined design matrices, we can pass on one of 'null', 'weighted_full' or 'full' instead of the computed diablo_design_matrix, e.g.:\n\ntar_target(\n  diablo_novarsel,\n  diablo_run(\n    diablo_input, \n    \"weighted_full\", \n    ncomp = 7\n  )\n)\n\nThen, we call the mixOmics::perf() function on the result of this first DIABLO run. There are a number of parameters to set:\n\nvalidation: the type of cross-validation to perform, M-fold (\"Mfold\") or leave-one-out (\"loo\"). We recommend to use M-fold validation, except when the number of samples is very small.\nfolds: for M-fold cross-validation, the number of folds to construct, i.e. the number of groups in which to split the samples. Each group in turn will be considered as test set while the remaining groups will be considered the training set. The value to use depends on the number of samples in the datasets. By default, 10 is a reasonable number. For leave-one-out cross-validation, this parameter is set to the number of samples (that is the principle of leave-one-out cross-validation).\nnrepeat: the number of times the cross-validation will be repeated. This is important for M-fold cross-validation, as the way the samples are split into groups affects the results. Therefore, by repeating the cross-validation scheme we’re averaging the results over different splits, thus reducing the impact of samples splitting. We recommend at least 10 repeats. Irrelevant for leave-one-out cross-validation, so can be left to 1.\ncpus: number of CPUs to use for the computation. Useful if folds \\(\\times\\) repeats is large, as this can be computationally intensive.\n\nHere we’ll perform a 10-fold cross validation with 10 repeats.\n\ntar_target(\n  diablo_perf_res,\n  mixOmics::perf(\n    diablo_novarsel,\n    validation = \"Mfold\",\n    folds = 10,\n    nrepeat = 10,\n    cpus = 3\n  )\n)\n\nWe can visualise the results of the cross-validation with the diablo_plot_perf() function:\n\ntar_load(diablo_perf_res)\ndiablo_plot_perf(diablo_perf_res)\n\n\n\n\nThe plot displays the cross-validation results computed with several different distances and error rates:\n\nDistance: this refers to the prediction distance that is used to predict the samples group in the test set, based on the samples grouping in the training set. DIABLO tests the maximum, centroids and Mahalanobis distance. The authors of the package recommend using either the centroids or the Mahalanobis distance over the maximum distance when choosing the optimal number of components.\nError rate: this refers to the method by which the performance of the produced model is computed. DIABLO uses both the overall misclassification error rate and the balanced error rate. The authors recommend the latter, as it is is less biased towards the majority group when there is an unbalanced number of samples per group.\n\nThe function diablo_get_optim_ncomp() extracts from the cross-validation results the optimal number of components to compute, given a chosen distance and error rate. The authors of the package recommend to use the results obtained with the centroids distance and the balanced error rate; these are used by default by the diablo_get_optim_ncomp() function. In our example, the optimal number of components is:\n\ndiablo_get_optim_ncomp(diablo_perf_res)\n#> [1] 4\n\nFor ease of reuse we will save this value as a target in our analysis pipeline:\n\ntar_target(\n  diablo_optim_ncomp,\n  diablo_get_optim_ncomp(diablo_perf_res)\n)"
  },
  {
    "objectID": "diablo.html#choosing-the-number-of-features-to-retain",
    "href": "diablo.html#choosing-the-number-of-features-to-retain",
    "title": "11  Integration with DIABLO",
    "section": "\n11.5 Choosing the number of features to retain",
    "text": "11.5 Choosing the number of features to retain\nThe next parameter to set is the number of features to retain from the different datasets for each latent component. This is usually chosen by performing cross-validation on a grid of possible values. The range of values to test depends on the type of question we are trying to answer: selecting a larger number of features might lead to a better discrimination of the outcome groups, but will be hard to manually inspect for further interpretation.\nThe function diablo_tune() provides a wrapper around the mixOmics::tune() function that performs this cross-validation. Some of the arguments are similar to the mixOmics::perf() function, e.g. validation, folds, nrepeats or cpus. In addition, we recommend setting the dist argument, which corresponds to the prediction distance metric used for performance assessment, to \"centroids.dist\" (or \"mahalanobis.dist\").\nThe keepX_list argument controls the grid of values to be tested as possible number of features to retain from each dataset. It should be in the form of a named list, with one element per dataset, and where each element is a vector of integers corresponding to the values to test. The names of the list should correspond to the names of the datasets in the MultiDataSet object. If no value is provided for keepX_list, six values ranging from 5 to 30 (by increments of 5) are tested for each dataset.\n\ntar_target(\n  diablo_tune_res,\n  diablo_tune(\n    diablo_input,\n    diablo_design_matrix,\n    ncomp = diablo_optim_ncomp,\n    validation = \"Mfold\",\n    folds = 10,\n    nrepeat = 5,\n    dist = \"centroids.dist\",\n    cpus = 3\n  )\n)\n\nThis step can be very time-consuming, especially if the grid of values to test is very large. For this example, it takes around 50 minutes to run.\nThe cross-validation results can be inspected with the diablo_plot_tune() function:\n\ntar_load(diablo_tune_res)\ndiablo_plot_tune(diablo_tune_res)\n\n\n\n\nThe visualisation shows the performance of DIABLO runs with different number of features retained from each dataset. The different runs are ordered according to their performance. Here, we can see for example that it seems preferable to retain more genes and less metabolites for component 1.\nThe optimal number of features to retain from each dataset for the different latent components is stored in the cross-validation results object, and can be accessed with:\n\ndiablo_tune_res$choice.keepX\n#> $snps\n#> [1] 25 25 30 20\n#> \n#> $rnaseq\n#> [1] 30 30 20 25\n#> \n#> $metabolome\n#> [1] 10  5 30 10\n\nFor reporting purposes, the diablo_table_optim_keepX() function displays the optimal keepX values in a table format:\n\ndiablo_table_optim_keepX(diablo_tune_res)\n#> # A tibble: 3 × 6\n#>   Dataset    `Component 1` `Component 2` `Component 3` `Component 4` Total\n#>   <chr>              <dbl>         <dbl>         <dbl>         <dbl> <dbl>\n#> 1 snps                  25            25            30            20   100\n#> 2 rnaseq                30            30            20            25   105\n#> 3 metabolome            10             5            30            10    55"
  },
  {
    "objectID": "diablo.html#final-diablo-run",
    "href": "diablo.html#final-diablo-run",
    "title": "11  Integration with DIABLO",
    "section": "\n11.6 Final DIABLO run",
    "text": "11.6 Final DIABLO run\nOnce a value has been selected for all parameters, it is time to perform the final DIABLO run:\n\ntar_target(\n  diablo_final_run,\n  diablo_run(\n    diablo_input,\n    diablo_design_matrix,\n    ncomp = diablo_optim_ncomp,\n    keepX = diablo_tune_res$choice.keepX\n  )\n)\n\n\ntar_load(diablo_final_run)\n\nTo facilitate reporting, the diablo_get_params() function extracts from the DIABLO result the parameters used (i.e. number of latent components computed and number of features retained from each dataset for each latent component), with HTML formatting:\n\ndiablo_get_params(diablo_final_run)\n#> # A tibble: 2 × 3\n#>   Parameter Description                                                    Value\n#>   <chr>     <chr>                                                          <chr>\n#> 1 ncomp     Number of latent component                                     4    \n#> 2 keepX     Number of features retained in each X for each latent compone… snps…"
  },
  {
    "objectID": "diablo.html#results-interpretation",
    "href": "diablo.html#results-interpretation",
    "title": "11  Integration with DIABLO",
    "section": "\n11.7 Results interpretation",
    "text": "11.7 Results interpretation\nIn Chapter 12, we show the different functionalities implemented in the moiraine package that facilitate the interpretation of the results from an integration tool. In this section, we show some of the DIABLO-specific plots that can be generated to help interpret the results of a DIABLO run.\n\n11.7.1 Correlation between datasets\nFirst, we can assess how well the latent components correlate across the datasets. The diablo_plot() function is adapted from the mixOmics::plotDiablo() function, and displays, for a given latent component (specified with the ncomp argument), the correlation between the samples coordinates for this latent component across the datasets. Additionally, it allows to assess how well the latent components discriminate the outcome groups in each dataset.\n\nn_comp <- diablo_get_optim_ncomp(diablo_perf_res)\nwalk(\n  seq_len(n_comp), \n  \\(x) {\n    diablo_plot(diablo_final_run, ncomp = x)\n    title(paste(\"Latent component\", x))\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the first three latent components, the strongest correlation is observed between the transcriptomics and metabolomics components, while the lowest correlation is observed between the genomics and metabolomics components. Across all three datasets, the first latent component alone is able to separate quite clearly the control and diseased animals. Note that as each latent component maximises the correlation between the datasets, these plots inform us about co-variation across the datasets.\n\n11.7.2 Samples projection to the latent component space\nWe can also represent the samples in the subspace spanned by the latent components for each dataset, using the mixOmics::plotIndiv() function. For example, we can have a look at the samples coordinates for the first two latent components:\n\nplotIndiv(\n  diablo_final_run,\n  comp = 1:2,\n  ind.names = FALSE,\n  legend = TRUE,\n  legend.title = \"Disease status\"\n)\n\n\n\n\nAs noted above, based on the first two latent components, there is a clear separation of the control and BRD animals across all three datasets.\nIdeally, we would look at all possible combinations of latent components, as follows:\n\nwalk(\n  combn(seq_len(n_comp), 2, simplify = FALSE),\n  \\(x) {\n    plotIndiv(\n      diablo_final_run,\n      comp = x,\n      ind.names = FALSE,\n      legend = TRUE,\n      legend.title = \"Phenotype group\"\n    )\n  } \n)\n\n\n11.7.3 Correlation circle plots\nThe correlation circle plots produced by the mixOmics::plotVar() function displays the contribution of the selected features to the different latent components. We will focus here on the first two latent components:\n\nplotVar(\n  diablo_final_run,\n  comp = 1:2,\n  var.names = FALSE,\n  ## If overlap = TRUE, features from the\n  ## different datasets are shown in one plot\n  overlap = FALSE,\n  pch = rep(16, 3),\n  cex = rep(2, 3)\n)\n\n\n\n\nAcross all three datasets, it seems that most selected features contribute to either one or the other latent component, but not both.\nThe plotVar() function offers the option to show the label of the features rather than representing them as points. However, it can be more informative to use information from the feature metadata as labels, rather than using the feature IDs. For example in the transcriptomics dataset, it would be more interesting to use the name of the genes. This information is available in the datasets’ features metadata:\n\ntar_load(mo_set_de)\nget_features_metadata(mo_set_de)[[\"rnaseq\"]] |>\n  str()\n#> 'data.frame':    20335 obs. of  15 variables:\n#>  $ feature_id : chr  \"ENSBTAG00000000005\" \"ENSBTAG00000000008\" \"ENSBTAG00000000009\" \"ENSBTAG00000000010\" ...\n#>  $ chromosome : Factor w/ 178 levels \"1\",\"2\",\"3\",\"4\",..: 17 29 18 21 8 20 20 5 5 23 ...\n#>  $ start      : int  65389743 32214439 12338037 34209956 7950815 33708626 33674806 74883636 74905031 27720176 ...\n#>  $ end        : int  65505336 32244810 12342272 34223394 7971600 33732944 33703223 74893752 74919112 27721739 ...\n#>  $ width      : int  115594 30372 4236 13439 20786 24319 28418 10117 14082 1564 ...\n#>  $ strand     : Factor w/ 3 levels \"+\",\"-\",\"*\": 1 2 1 1 2 1 1 2 2 2 ...\n#>  $ Name       : chr  \"GRK3\" \"KCNJ1\" \"FOXF1\" \"UBL7\" ...\n#>  $ description: chr  \"G protein-coupled receptor kinase 3 [Source:VGNC Symbol;Acc:VGNC:53904]\" \"potassium inwardly rectifying channel subfamily J member 1 [Source:VGNC Symbol;Acc:VGNC:30453]\" \"forkhead box F1 [Source:VGNC Symbol;Acc:VGNC:29084]\" \"ubiquitin like 7 [Source:VGNC Symbol;Acc:VGNC:50128]\" ...\n#>  $ log_fc     : num  0.136 -0.1297 1.2714 0.3957 0.0777 ...\n#>  $ log_cpm    : num  5.905 -0.744 -2.563 6.256 -2.761 ...\n#>  $ f          : num  4.163 1.068 23.956 60.53 0.142 ...\n#>  $ p_value    : num  4.32e-02 3.03e-01 2.73e-06 1.58e-12 7.07e-01 ...\n#>  $ fdr        : num  7.17e-02 3.94e-01 9.67e-06 1.52e-11 7.77e-01 ...\n#>  $ de_signif  : chr  \"Not DE\" \"Not DE\" \"Not DE\" \"Not DE\" ...\n#>  $ de_status  : chr  \"Not DE\" \"Not DE\" \"Not DE\" \"Not DE\" ...\n\nThe diablo_plot_var() function is a variant of plotVar(), which uses columns from the features metadata to label features plot. It takes as an input the DIABLO result object as well as the MultiDataSet object, and a named list providing for each dataset the name of the column in the feature metadata data-frame to use as features label:\n\ndiablo_plot_var(\n  diablo_final_run,\n  mo_set_de,\n  label_cols = list(\n    \"rnaseq\" = \"Name\",\n    \"metabolome\" = \"name\"\n  ),\n  overlap = FALSE,\n  cex = rep(2, 3),\n  comp = 1:2\n)\n\n\n\n\nNote that if a dataset is not present in the list passed to label_cols (here, that is the case of the genomics dataset), the feature IDs will be used as labels.\n\n11.7.4 Circos plot\nLastly, it is possible to represent the correlation between features selected from different datasets, with the mixOmics::circosPlot() function. For ease of visualisation, it only displays correlations above a certain threshold (specified via the cutoff argument). By default, it displays the features selected for all latent components, but this can be controlled via the comp argument:\n\ncircosPlot(\n  diablo_final_run,\n  cutoff = 0.7,\n  size.variables = 0.5,\n  comp = 1:2\n)\n\n\n\n\nAs for the correlation circle plot function, the diablo_plot_circos() function generates the same plot, but allows us to use columns in the feature metadata of each dataset as feature labels:\n\ndiablo_plot_circos(\n  diablo_final_run,\n  tar_read(mo_set),\n  label_cols = list(\n    \"rnaseq\" = \"Name\",\n    \"metabolome\" = \"name\"\n  ),\n  cutoff = 0.7,\n  size.variables = 0.5,\n  comp = 1:2\n)\n\n\n\n\nThis plot is useful to identify features across the datasets with high correlations."
  },
  {
    "objectID": "diablo.html#recap-targets-list",
    "href": "diablo.html#recap-targets-list",
    "title": "11  Integration with DIABLO",
    "section": "\n11.8 Recap – targets list",
    "text": "11.8 Recap – targets list\nFor convenience, here is the list of targets that we created in this section:\n\nTargets list for DIABLO analysis\n\nlist(\n  ## Creating the DIABLO input\n  tar_target(\n    diablo_input,\n    get_input_mixomics_supervised(\n      mo_presel_supervised,\n      group = \"status\"\n    )\n  ),\n  \n  ## Running sPLS on each dataset to construct the design matrix\n  diablo_pairwise_pls_factory(diablo_input),\n  \n  ## Initial DIABLO run with no feature selection and large number of components\n  tar_target(\n    diablo_novarsel,\n    diablo_run(\n      diablo_input,\n      diablo_design_matrix,\n      ncomp = 7\n    )\n  ),\n  \n  ## Cross-validation for number of components\n  tar_target(\n    diablo_perf_res,\n    mixOmics::perf(\n      diablo_novarsel,\n      validation = \"Mfold\",\n      folds = 10,\n      nrepeat = 10,\n      cpus = 3\n    )\n  ),\n  \n  ## Plotting cross-validation results (for number of components)\n  tar_target(\n    diablo_perf_plot,\n    diablo_plot_perf(diablo_perf_res)\n  ),\n  \n  ## Selected value for ncomp\n  tar_target(\n    diablo_optim_ncomp,\n    diablo_get_optim_ncomp(diablo_perf_res)\n  ),\n  \n  ## Cross-validation for number of features to retain\n  tar_target(\n    diablo_tune_res,\n    diablo_tune(\n      diablo_input,\n      diablo_design_matrix,\n      ncomp = diablo_optim_ncomp,\n      validation = \"Mfold\",\n      folds = 10,\n      nrepeat = 5,\n      dist = \"centroids.dist\",\n      cpus = 3\n    )\n  ),\n  \n  ## Plotting cross-validation results (for number of features)\n  tar_target(\n    diablo_tune_plot,\n    diablo_plot_tune(diablo_tune_res)\n  ),\n  \n  ## Final DIABLO run\n  tar_target(\n    diablo_final_run,\n    diablo_run(\n      diablo_input,\n      diablo_design_matrix,\n      ncomp = diablo_optim_ncomp,\n      keepX = diablo_tune_res$choice.keepX\n    )\n  )\n)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bouhaddani, Said el, Hae-Won Uh, Geurt Jongbloed, Caroline Hayward,\nLucija Klarić, Szymon M. Kiełbasa, and Jeanine Houwing-Duistermaat.\n2018. “Integrating Omics Datasets with the OmicsPLS\nPackage.” BMC Bioinformatics 19 (1): 371. https://doi.org/10.1186/s12859-018-2371-3.\n\n\nGu, Zhujie, Said el Bouhaddani, Jiayi Pei, Jeanine Houwing-Duistermaat,\nand Hae-Won Uh. 2021. “Statistical Integration of Two Omics\nDatasets Using GO2PLS.” BMC Bioinformatics\n22 (1): 131. https://doi.org/10.1186/s12859-021-03958-3.\n\n\nHernandez-Ferrer, Carles, Carlos Ruiz-Arenas, Alba Beltran-Gomila, and\nJuan R. González. 2017. “MultiDataSet: An r Package for\nEncapsulating Multiple Data Sets with Application to Omic Data\nIntegration.” BMC Bioinformatics 18 (1): 36. https://doi.org/10.1186/s12859-016-1455-1.\n\n\nLê Cao, Kim-Anh, Simon Boitard, and Philippe Besse. 2011. “Sparse\nPLS Discriminant Analysis: Biologically Relevant Feature Selection and\nGraphical Displays for Multiclass Problems.” BMC\nBioinformatics 12 (1): 253. https://doi.org/10.1186/1471-2105-12-253.\n\n\nLi, Jiyuan, Robert Mukiibi, Janelle Jiminez, Zhiquan Wang, Everestus C.\nAkanno, Edouard Timsit, and Graham S. Plastow. 2022. “Applying\nMulti-Omics Data to Study the Genetic Background of Bovine Respiratory\nDisease Infection in Feedlot Crossbred Cattle.” Frontiers in\nGenetics 13. https://www.frontiersin.org/articles/10.3389/fgene.2022.1046192.\n\n\nLiquet, Benoit, Kim-Anh Lê Cao, Hakim Hocini, and Rodolphe Thiébaut.\n2012. “A Novel Approach for Biomarker Selection and the\nIntegration of Repeated Measures Experiments from Two Assays.”\nBMC Bioinformatics 13 (1): 325. https://doi.org/10.1186/1471-2105-13-325."
  },
  {
    "objectID": "interpretation.html#generating-a-standardised-output",
    "href": "interpretation.html#generating-a-standardised-output",
    "title": "12  Interpreting the integration results",
    "section": "\n12.1 Generating a standardised output",
    "text": "12.1 Generating a standardised output\n\n12.1.1 get_output function\nDespite relying on very different statistical approaches, the different integration methods included in the pipeline all perform dimension reduction of the omics datasets through feature extraction. That is, they construct a small number of latent components/variables/dimensions (that we refer to as latent dimensions in the moiraine package) that capture as much information from the original datasets as possible. A dimension reduction approach typically returns, for each latent dimension constructed, two sets of values:\n\nFeatures weight: the contribution of the features from the different omics dataset to the latent dimension. All methods included in the pipeline construct latent dimensions as linear combinations of the original features, and therefore the features contribution is quantified by their weight in the linear combination.\nSamples score: the projection of the samples onto the latent dimension.\n\nIn addition, the fraction or percentage of variance that each latent dimension explains in the different omics datasets is usually calculated.\nIn the moiraine package, the output of the different integration methods can be converted to a standardised output containing these three pieces of information (features weight, samples score and percentage of variance explained) stored in a consistent format. This enables us to construct functions for visualisation or analysis which can be applied to the results of any integration method, rather than having to implement one for each object type returned by the different integration packages.\nThe get_output() function transforms the output from any integration package included in moiraine into an output_dimension_reduction object, which is a list with three tibbles: features_weight, samples_score and variance_explained:\n\n\nsPLS\nsO2PLS\nMOFA\nDIABLO\n\n\n\n\ntar_target(\n  spls_output,\n  get_output(spls_final_run)\n)\n\n\ntar_load(spls_output)\nspls_output\n#> $features_weight\n#> # A tibble: 2,098 × 5\n#>    feature_id         dataset latent_dimension weight importance\n#>    <chr>              <fct>   <fct>             <dbl>      <dbl>\n#>  1 ENSBTAG00000000020 rnaseq  Component 1           0          0\n#>  2 ENSBTAG00000000046 rnaseq  Component 1           0          0\n#>  3 ENSBTAG00000000056 rnaseq  Component 1           0          0\n#>  4 ENSBTAG00000000061 rnaseq  Component 1           0          0\n#>  5 ENSBTAG00000000113 rnaseq  Component 1           0          0\n#>  6 ENSBTAG00000000149 rnaseq  Component 1           0          0\n#>  7 ENSBTAG00000000164 rnaseq  Component 1           0          0\n#>  8 ENSBTAG00000000205 rnaseq  Component 1           0          0\n#>  9 ENSBTAG00000000212 rnaseq  Component 1           0          0\n#> 10 ENSBTAG00000000289 rnaseq  Component 1           0          0\n#> # ℹ 2,088 more rows\n#> \n#> $samples_score\n#> # A tibble: 278 × 3\n#>    sample_id latent_dimension score\n#>    <chr>     <fct>            <dbl>\n#>  1 G1979     Component 1       3.54\n#>  2 G2500     Component 1       2.37\n#>  3 G3030     Component 1       6.47\n#>  4 G3068     Component 1       6.24\n#>  5 G3121     Component 1       3.51\n#>  6 G3315     Component 1      -4.75\n#>  7 G3473     Component 1      -5.03\n#>  8 G3474     Component 1      -4.85\n#>  9 G3550     Component 1       3.69\n#> 10 G3594     Component 1      -4.74\n#> # ℹ 268 more rows\n#> \n#> $variance_explained\n#> # A tibble: 4 × 3\n#>   latent_dimension dataset    prop_var_expl\n#>   <fct>            <fct>              <dbl>\n#> 1 Component 1      rnaseq            0.204 \n#> 2 Component 1      metabolome        0.176 \n#> 3 Component 2      rnaseq            0.147 \n#> 4 Component 2      metabolome        0.0686\n#> \n#> attr(,\"class\")\n#> [1] \"output_dimension_reduction\"\n#> attr(,\"method\")\n#> [1] \"sPLS\"\n\n\n\n\ntar_target(\n  so2pls_output,\n  get_output(so2pls_final_run)\n)\n\n\ntar_load(so2pls_output)\nso2pls_output\n#> $features_weight\n#> # A tibble: 8,117 × 5\n#>    feature_id         dataset latent_dimension  weight importance\n#>    <chr>              <fct>   <fct>              <dbl>      <dbl>\n#>  1 ENSBTAG00000000020 rnaseq  joint component 1      0          0\n#>  2 ENSBTAG00000000046 rnaseq  joint component 1      0          0\n#>  3 ENSBTAG00000000056 rnaseq  joint component 1      0          0\n#>  4 ENSBTAG00000000061 rnaseq  joint component 1      0          0\n#>  5 ENSBTAG00000000113 rnaseq  joint component 1      0          0\n#>  6 ENSBTAG00000000149 rnaseq  joint component 1      0          0\n#>  7 ENSBTAG00000000164 rnaseq  joint component 1      0          0\n#>  8 ENSBTAG00000000205 rnaseq  joint component 1      0          0\n#>  9 ENSBTAG00000000212 rnaseq  joint component 1      0          0\n#> 10 ENSBTAG00000000289 rnaseq  joint component 1      0          0\n#> # ℹ 8,107 more rows\n#> \n#> $samples_score\n#> # A tibble: 1,390 × 3\n#>    sample_id latent_dimension  score\n#>    <chr>     <fct>             <dbl>\n#>  1 G1979     joint component 1 -3.86\n#>  2 G2500     joint component 1 -4.29\n#>  3 G3030     joint component 1 -8.02\n#>  4 G3068     joint component 1 -6.00\n#>  5 G3121     joint component 1 -5.82\n#>  6 G3315     joint component 1  8.51\n#>  7 G3473     joint component 1  7.08\n#>  8 G3474     joint component 1 10.7 \n#>  9 G3550     joint component 1 -3.54\n#> 10 G3594     joint component 1 10.5 \n#> # ℹ 1,380 more rows\n#> \n#> $variance_explained\n#> # A tibble: 11 × 3\n#>    latent_dimension                dataset    prop_var_expl\n#>    <fct>                           <fct>              <dbl>\n#>  1 joint component 1               rnaseq            0.266 \n#>  2 joint component 1               metabolome        0.108 \n#>  3 rnaseq specific component 1     rnaseq            0.154 \n#>  4 rnaseq specific component 2     rnaseq            0.129 \n#>  5 rnaseq specific component 3     rnaseq            0.0521\n#>  6 rnaseq specific component 4     rnaseq            0.0157\n#>  7 rnaseq specific component 5     rnaseq            0.0301\n#>  8 rnaseq specific component 6     rnaseq            0.0163\n#>  9 rnaseq specific component 7     rnaseq            0.0114\n#> 10 metabolome specific component 1 metabolome        0.114 \n#> 11 metabolome specific component 2 metabolome        0.0822\n#> \n#> attr(,\"class\")\n#> [1] \"output_dimension_reduction\"\n#> attr(,\"method\")\n#> [1] \"sO2PLS\"\n\n\n\n\ntar_target(\n  mofa_output,\n  get_output(mofa_trained)\n)\n\n\ntar_load(mofa_output)\nmofa_output\n#> $features_weight\n#> # A tibble: 30,735 × 5\n#>    feature_id                  dataset latent_dimension     weight importance\n#>    <chr>                       <fct>   <fct>                 <dbl>      <dbl>\n#>  1 21-25977541-C-T-rs41974686  snps    Factor 1          0.000390      0.140 \n#>  2 22-51403583-A-C-rs210306176 snps    Factor 1          0.0000554     0.0199\n#>  3 24-12959068-G-T-rs381471286 snps    Factor 1          0.000274      0.0987\n#>  4 8-85224224-T-C-rs43565287   snps    Factor 1         -0.000510      0.184 \n#>  5 ARS-BFGL-BAC-16973          snps    Factor 1         -0.000913      0.328 \n#>  6 ARS-BFGL-BAC-19403          snps    Factor 1         -0.000516      0.185 \n#>  7 ARS-BFGL-BAC-2450           snps    Factor 1          0.000567      0.204 \n#>  8 ARS-BFGL-BAC-2600           snps    Factor 1         -0.0000356     0.0128\n#>  9 ARS-BFGL-BAC-27911          snps    Factor 1         -0.000588      0.212 \n#> 10 ARS-BFGL-BAC-35925          snps    Factor 1          0.000836      0.301 \n#> # ℹ 30,725 more rows\n#> \n#> $samples_score\n#> # A tibble: 2,160 × 3\n#>    sample_id latent_dimension score\n#>    <chr>     <fct>            <dbl>\n#>  1 G1979     Factor 1          1.94\n#>  2 G2500     Factor 1          2.02\n#>  3 G3030     Factor 1          3.38\n#>  4 G3068     Factor 1          3.68\n#>  5 G3121     Factor 1          1.56\n#>  6 G3315     Factor 1         -1.82\n#>  7 G3473     Factor 1         -2.65\n#>  8 G3474     Factor 1         -2.32\n#>  9 G3550     Factor 1          2.99\n#> 10 G3594     Factor 1         -1.89\n#> # ℹ 2,150 more rows\n#> \n#> $variance_explained\n#> # A tibble: 45 × 3\n#>    latent_dimension dataset    prop_var_expl\n#>    <fct>            <fct>              <dbl>\n#>  1 Factor 1         snps           0.000325 \n#>  2 Factor 1         rnaseq         0.496    \n#>  3 Factor 1         metabolome     0.0846   \n#>  4 Factor 2         snps           0.239    \n#>  5 Factor 2         rnaseq         0.000207 \n#>  6 Factor 2         metabolome     0.00671  \n#>  7 Factor 3         snps           0.000113 \n#>  8 Factor 3         rnaseq         0.199    \n#>  9 Factor 3         metabolome     0.0000529\n#> 10 Factor 4         snps           0.0000964\n#> # ℹ 35 more rows\n#> \n#> attr(,\"class\")\n#> [1] \"output_dimension_reduction\"\n#> attr(,\"method\")\n#> [1] \"MOFA\"\n\n\n\n\ntar_target(\n  diablo_output,\n  get_output(diablo_final_run)\n)\n\n\ntar_load(diablo_output)\ndiablo_output\n#> $features_weight\n#> # A tibble: 8,196 × 5\n#>    feature_id                  dataset latent_dimension weight importance\n#>    <chr>                       <fct>   <fct>             <dbl>      <dbl>\n#>  1 21-25977541-C-T-rs41974686  snps    Component 1           0          0\n#>  2 22-51403583-A-C-rs210306176 snps    Component 1           0          0\n#>  3 24-12959068-G-T-rs381471286 snps    Component 1           0          0\n#>  4 8-85224224-T-C-rs43565287   snps    Component 1           0          0\n#>  5 ARS-BFGL-BAC-16973          snps    Component 1           0          0\n#>  6 ARS-BFGL-BAC-19403          snps    Component 1           0          0\n#>  7 ARS-BFGL-BAC-2450           snps    Component 1           0          0\n#>  8 ARS-BFGL-BAC-2600           snps    Component 1           0          0\n#>  9 ARS-BFGL-BAC-27911          snps    Component 1           0          0\n#> 10 ARS-BFGL-BAC-35925          snps    Component 1           0          0\n#> # ℹ 8,186 more rows\n#> \n#> $samples_score\n#> # A tibble: 540 × 3\n#>    sample_id latent_dimension score\n#>    <chr>     <fct>            <dbl>\n#>  1 G1979     Component 1       1.95\n#>  2 G2500     Component 1       1.43\n#>  3 G3030     Component 1       3.72\n#>  4 G3068     Component 1       3.70\n#>  5 G3121     Component 1       2.03\n#>  6 G3315     Component 1      -3.45\n#>  7 G3473     Component 1      -4.02\n#>  8 G3474     Component 1      -3.24\n#>  9 G3550     Component 1       2.43\n#> 10 G3594     Component 1      -2.49\n#> # ℹ 530 more rows\n#> \n#> $variance_explained\n#> # A tibble: 12 × 3\n#>    latent_dimension dataset    prop_var_expl\n#>    <fct>            <fct>              <dbl>\n#>  1 Component 1      snps              0.0778\n#>  2 Component 1      rnaseq            0.201 \n#>  3 Component 1      metabolome        0.166 \n#>  4 Component 2      snps              0.0188\n#>  5 Component 2      rnaseq            0.0830\n#>  6 Component 2      metabolome        0.0433\n#>  7 Component 3      snps              0.0177\n#>  8 Component 3      rnaseq            0.147 \n#>  9 Component 3      metabolome        0.0965\n#> 10 Component 4      snps              0.0147\n#> 11 Component 4      rnaseq            0.0603\n#> 12 Component 4      metabolome        0.0457\n#> \n#> attr(,\"class\")\n#> [1] \"output_dimension_reduction\"\n#> attr(,\"method\")\n#> [1] \"DIABLO\"\n\n\n\n\nThe features_weight tibble contains one row per combination of feature and latent dimension. The ID of the features and the name of the dataset from which they originate are stored in the feature_id and dataset columns, respectively. The latent_dimension column gives the name of the latent dimension; this is a factor column. For each feature and latent dimension, the weight column shows the weight that was attributed to the feature for the corresponding latent dimension. In addition, the importance column contains the features importance score, which is computed as the absolute value of the features weight, divided by the maximum absolute weight across all features from the same omics dataset for the corresponding latent dimension. This importance score allows us to compare the contribution of the features across latent dimensions or integration methods, as the weight can be on different scales and thus cannot be directly compared. The importance scores range from 0 to 1. For any method performing feature selection (e.g. sPLS or DIABLO), features that were not selected for a given latent dimension are assigned a weight and importance score of 0.\nThe samples score tibble contains for each sample (sample_id) and latent dimension (latent_dimension) the sample’s coordinate for the corresponding latent dimension.\nThe variance_explained tibble gives for each latent dimension (latent_dimension) the proportion of variance explained (prop_var_expl) for each dataset (dataset). The values in prop_var_expl are between 0 and 1.\nFor convenience, the get_latent_dimensions() function can be used on an output_dimension_reduction object to see the names of the latent dimensions (the levels used for the latent_dimension column in each tibble):\n\n\nsPLS\nsO2PLS\nMOFA\nDIABLO\n\n\n\n\nget_latent_dimensions(spls_output)\n#> [1] \"Component 1\" \"Component 2\"\n\n\n\n\nget_latent_dimensions(so2pls_output)\n#>  [1] \"joint component 1\"               \"rnaseq specific component 1\"    \n#>  [3] \"rnaseq specific component 2\"     \"rnaseq specific component 3\"    \n#>  [5] \"rnaseq specific component 4\"     \"rnaseq specific component 5\"    \n#>  [7] \"rnaseq specific component 6\"     \"rnaseq specific component 7\"    \n#>  [9] \"metabolome specific component 1\" \"metabolome specific component 2\"\n\n\n\n\nget_latent_dimensions(mofa_output)\n#>  [1] \"Factor 1\"  \"Factor 2\"  \"Factor 3\"  \"Factor 4\"  \"Factor 5\"  \"Factor 6\" \n#>  [7] \"Factor 7\"  \"Factor 8\"  \"Factor 9\"  \"Factor 10\" \"Factor 11\" \"Factor 12\"\n#> [13] \"Factor 13\" \"Factor 14\" \"Factor 15\"\n\n\n\n\nget_latent_dimensions(diablo_output)\n#> [1] \"Component 1\" \"Component 2\" \"Component 3\" \"Component 4\"\n\n\n\n\n\n\n\n\n\n\nOther methods covered by get_output\n\n\n\nNote that both PCA and sPLS-DA (the method used for supervised features preselection in Section 7.3.2) are also both dimension reduction methods. Therefore, the get_output function also converts pcaRes objects (from run_pca() or pcaMethods::pca()) and mixo_splsda objects (from run_splsda() or mixOmics::splsda()).\n\n\n\n12.1.2 Averaging latent dimensions over datasets\nWhile MOFA computes one score per sample for each latent dimension created, sPLS, DIABLO and sO2PLS all compute one score per dataset for each sample and latent dimension. For each latent dimension. the samples score obtained for the different datasets are then compared, to assess the agreement or covariation between datasets. Ideally, these scores should be highly correlated across datasets, since the methods aim at maximising the variation between datasets, but it is not always the case. However, when they are highly correlated, it becomes redundant to interpret the latent dimensions for each dataset.\nInstead, the mixOmics authors proposed a solution for DIABLO, which is to construct a weighted average space: for each latent component, the samples score are averaged over the different datasets. The weight is given per dataset and determined by how well the corresponding dataset discriminate between the samples group of interest. This way, rather than looking at samples score for each dataset for any given latent component, we can look at an average of them.\nThe get_output() function uses this idea to construct, for the output of sPLS, sO2PLS and DIABLO a set of average samples score for each latent dimension, rather than returning a set of samples score per dataset. For DIABLO, the average is weighted as explained above, while for sPLS and sO2PLS each dataset is given equal weight in the average. This calculation can be disabled in the get_output() function to extract the dataset-specific samples score, by setting the use_average_dimensions parameter to FALSE. Note that this only affects the samples_score tibble in terms of dimensions, but the name of the latent dimensions will change to reflect the dataset to which they refer.\n\n\nsPLS\nsO2PLS\nDIABLO\n\n\n\n\ntar_target(\n  spls_output_no_average,\n  get_output(spls_final_run, use_average_dimensions = FALSE)\n)\n\n\ntar_load(spls_output_no_average)\n\nget_latent_dimensions(spls_output_no_average)\n#> [1] \"rnaseq Component 1\"     \"metabolome Component 1\" \"rnaseq Component 2\"    \n#> [4] \"metabolome Component 2\"\n\nnrow(spls_output$samples_score)\n#> [1] 278\nnrow(spls_output_no_average$samples_score)\n#> [1] 556\n\n\n\n\ntar_target(\n  so2pls_output_no_average,\n  get_output(so2pls_final_run, use_average_dimensions = FALSE)\n)\n\n\ntar_load(so2pls_output_no_average)\n\nget_latent_dimensions(so2pls_output_no_average)\n#>  [1] \"rnaseq joint component 1\"        \"metabolome joint component 1\"   \n#>  [3] \"rnaseq specific component 1\"     \"rnaseq specific component 2\"    \n#>  [5] \"rnaseq specific component 3\"     \"rnaseq specific component 4\"    \n#>  [7] \"rnaseq specific component 5\"     \"rnaseq specific component 6\"    \n#>  [9] \"rnaseq specific component 7\"     \"metabolome specific component 1\"\n#> [11] \"metabolome specific component 2\"\n\nnrow(so2pls_output$samples_score)\n#> [1] 1390\nnrow(so2pls_output_no_average$samples_score)\n#> [1] 1529\n\n\n\n\ntar_target(\n  diablo_output_no_average,\n  get_output(diablo_final_run, use_average_dimensions = FALSE)\n)\n\n\ntar_load(diablo_output_no_average)\n  \nget_latent_dimensions(diablo_output_no_average)\n#>  [1] \"snps Component 1\"       \"rnaseq Component 1\"     \"metabolome Component 1\"\n#>  [4] \"snps Component 2\"       \"rnaseq Component 2\"     \"metabolome Component 2\"\n#>  [7] \"snps Component 3\"       \"rnaseq Component 3\"     \"metabolome Component 3\"\n#> [10] \"snps Component 4\"       \"rnaseq Component 4\"     \"metabolome Component 4\"\n\nnrow(diablo_output$samples_score)\n#> [1] 540\nnrow(diablo_output_no_average$samples_score)\n#> [1] 1620"
  },
  {
    "objectID": "interpretation.html#interpretation",
    "href": "interpretation.html#interpretation",
    "title": "12  Interpreting the integration results",
    "section": "\n12.2 Interpretation",
    "text": "12.2 Interpretation\nInterpreting the results of a dimension reduction method involves:\n\nUnderstanding the source of the variation captured by each latent dimension: is a given latent dimension representing an important source of biological variation, such as effect of a treatment, or age of the samples? Or do they show a source of technical variation, for example highlighting a group of outlier samples with different omics profiles from the rest of the observations? Answering these questions allows us to identify which latent dimensions capture the biological phenomenon investigated, or whether there are some sources of noise that should be accounted for in follow-up experiments.\nInvestigating which omics features are driving the latent dimensions: once we have identified some latent dimensions of interest, we can look at the features that contribute the most to understand the molecular mechanisms or pathways involved. This is typically done after looking into the phenomenon captured by the latent dimensions, but can also help to identify it."
  }
]